#!/usr/bin/env python3
"""
ê°œì„ ëœ S3 + Hive Metastore + Iceberg ì—°ê²° ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
í•„ìš”í•œ JAR íŒŒì¼ë“¤ì„ í¬í•¨í•˜ì—¬ S3/Hive/Iceberg ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

from pyspark.sql import SparkSession
import time

def test_s3_with_packages():
    print("ğŸ” S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘ (í•„ìˆ˜ íŒ¨í‚¤ì§€ í¬í•¨)...")
    
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì—¬ SparkSession ìƒì„±
    spark = SparkSession.builder \
        .appName("S3ConnectionTest") \
        .config("spark.jars.packages", 
               "org.apache.hadoop:hadoop-aws:3.3.4," \
               "com.amazonaws:aws-java-sdk-bundle:1.12.262," \
               "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.0") \
        .config("spark.hadoop.fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", 
               "com.amazonaws.auth.profile.ProfileCredentialsProvider") \
        .config("spark.hadoop.fs.s3a.connection.timeout", "30000") \
        .config("spark.hadoop.fs.s3a.connection.establish.timeout", "30000") \
        .config("spark.hadoop.fs.s3a.attempts.maximum", "3") \
        .config("spark.hadoop.fs.s3a.retry.limit", "5") \
        .config("spark.hadoop.fs.s3a.retry.interval", "1000") \
        .config("spark.hadoop.fs.s3a.fast.upload", "true") \
        .getOrCreate()
    
    try:
        s3_path = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
        print(f"ğŸ“‚ í…ŒìŠ¤íŠ¸ ê²½ë¡œ: {s3_path}")
        
        # 1. Sparkì˜ FileSystem APIë¥¼ ì‚¬ìš©í•œ ê²½ë¡œ í™•ì¸
        print("1ï¸âƒ£ Spark FileSystem APIë¥¼ í†µí•œ ê²½ë¡œ í™•ì¸...")
        start_time = time.time()
        
        try:
            # ì‹¤ì œ ë°ì´í„°ëŠ” jsonl í™•ì¥ìë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ íŒ¨í„´ ìˆ˜ì •
            files_df = spark.read.option("multiline", "false").text(s3_path + "*.jsonl")
            
            # lazy evaluationì´ë¯€ë¡œ ì‹¤ì œë¡œ actionì„ ìˆ˜í–‰í•´ì•¼ í•¨
            file_count = files_df.count()
            elapsed = time.time() - start_time
            
            print(f"   âœ… S3 ì ‘ê·¼ ì„±ê³µ! íŒŒì¼ ìˆ˜: {file_count} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
            
            if file_count > 0:
                print("2ï¸âƒ£ JSONL íŒŒì¼ ì½ê¸° í…ŒìŠ¤íŠ¸...")
                start_time = time.time()
                
                # JSONLìœ¼ë¡œ ì½ì–´ë³´ê¸°
                json_df = spark.read.option("multiline", "false").json(s3_path + "*.jsonl").limit(5)
                row_count = json_df.count()
                elapsed = time.time() - start_time
                
                print(f"   âœ… JSONL ì½ê¸° ì„±ê³µ! í–‰ ìˆ˜: {row_count} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
                
                print("3ï¸âƒ£ ë°ì´í„° êµ¬ì¡° í™•ì¸...")
                json_df.printSchema()
                
                print("\nğŸ“„ ìƒ˜í”Œ ë°ì´í„° (ì²« ë²ˆì§¸ í–‰):")
                json_df.show(1, truncate=False)
                
                return True
            else:
                print("   âš ï¸  íŒŒì¼ì€ ì ‘ê·¼ ê°€ëŠ¥í•˜ì§€ë§Œ ì½ì„ ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as inner_e:
            print(f"   âŒ S3 ì ‘ê·¼ ì‹¤íŒ¨: {str(inner_e)}")
            return False
            
    except Exception as e:
        print(f"âŒ S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return False
        
    finally:
        spark.stop()

def test_aws_credentials():
    print("\nğŸ” AWS ìê²©ì¦ëª… í…ŒìŠ¤íŠ¸...")
    
    import subprocess
    import os
    
    try:
        # AWS CLIê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        result = subprocess.run(['aws', '--version'], capture_output=True, text=True)
        print(f"   AWS CLI ë²„ì „: {result.stdout.strip()}")
        
        # AWS ìê²©ì¦ëª… í™•ì¸
        result = subprocess.run(['aws', 'sts', 'get-caller-identity'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"   âœ… AWS ìê²©ì¦ëª… ì •ìƒ: {result.stdout.strip()}")
        else:
            print(f"   âŒ AWS ìê²©ì¦ëª… ë¬¸ì œ: {result.stderr.strip()}")
            return False
        
        # S3 ë²„í‚· ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        result = subprocess.run([
            'aws', 's3', 'ls', 
            's3://reciping-user-event-logs/bronze/landing-zone/events/',
            '--region', 'ap-northeast-2'
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            file_count = len([line for line in lines if line.strip()])
            print(f"   âœ… S3 ë²„í‚· ì ‘ê·¼ ì„±ê³µ! íŒŒì¼ ìˆ˜: {file_count}")
            
            # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ í‘œì‹œ
            for line in lines[:5]:
                if line.strip():
                    print(f"      {line.strip()}")
            
            return True
        else:
            print(f"   âŒ S3 ë²„í‚· ì ‘ê·¼ ì‹¤íŒ¨: {result.stderr.strip()}")
            return False
            
    except FileNotFoundError:
        print("   âŒ AWS CLIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"   âŒ AWS ìê²©ì¦ëª… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_hive_metastore_improved():
    print("\nğŸ—„ï¸ Hive Metastore ì—°ê²° í…ŒìŠ¤íŠ¸ (ê°œì„ ë¨)...")
    
    spark = SparkSession.builder \
        .appName("HiveMetastoreTest") \
        .config("spark.jars.packages", 
                "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.0") \
        .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
        .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://10.0.11.86:9083") \
        .getOrCreate()
    
    try:
        # 1. ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ í™•ì¸
        print("   1ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ í™•ì¸...")
        databases = spark.sql("SHOW DATABASES").collect()
        print(f"      ë°ì´í„°ë² ì´ìŠ¤ ìˆ˜: {len(databases)}")
        
        for db in databases:
            db_name = db[0] if isinstance(db, (list, tuple)) else str(db)
            print(f"      - {db_name}")
        
        # 2. Iceberg ì¹´íƒˆë¡œê·¸ í…ŒìŠ¤íŠ¸
        print("   2ï¸âƒ£ Iceberg ì¹´íƒˆë¡œê·¸ í…ŒìŠ¤íŠ¸...")
        try:
            iceberg_dbs = spark.sql("SHOW DATABASES IN iceberg_catalog").collect()
            print(f"      Iceberg ë°ì´í„°ë² ì´ìŠ¤ ìˆ˜: {len(iceberg_dbs)}")
            for db in iceberg_dbs[:3]:
                db_name = db[0] if isinstance(db, (list, tuple)) else str(db)
                print(f"      - iceberg_catalog.{db_name}")
        except Exception as iceberg_e:
            print(f"      âš ï¸  Iceberg ì¹´íƒˆë¡œê·¸ ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜: {str(iceberg_e)}")
        
        # 3. í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ìƒì„±í•´ë³´ê¸°
        print("   3ï¸âƒ£ í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ìƒì„± ì‹œë„...")
        try:
            spark.sql("""
                CREATE DATABASE IF NOT EXISTS iceberg_catalog.test_db
                COMMENT 'Test database for connection verification'
            """)
            print("      âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì„±ê³µ!")
            return True
        except Exception as table_e:
            print(f"      âŒ í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(table_e)}")
            return False
    except Exception as e:
        print(f"   âŒ Hive Metastore ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return False
    finally:
        spark.stop()

def check_network_connectivity():
    print("\nğŸŒ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸...")
    
    import subprocess
    
    endpoints = [
        ("S3 ì—”ë“œí¬ì¸íŠ¸", "s3.ap-northeast-2.amazonaws.com", 443),
        ("Hive Metastore", "10.0.11.86", 9083)
    ]
    
    for name, host, port in endpoints:
        try:
            result = subprocess.run([
                'timeout', '10', 'bash', '-c', f'echo > /dev/tcp/{host}/{port}'
            ], capture_output=True)
            if result.returncode == 0:
                print(f"   âœ… {name} ({host}:{port}) ì—°ê²° ê°€ëŠ¥")
            else:
                print(f"   âŒ {name} ({host}:{port}) ì—°ê²° ë¶ˆê°€")
        except Exception as e:
            print(f"   âŒ {name} ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    print("ğŸ”§ ê°œì„ ëœ ETL íŒŒì´í”„ë¼ì¸ ì‚¬ì „ ì§„ë‹¨ ì‹œì‘")
    print("=" * 60)
    
    aws_ok = test_aws_credentials()
    network_ok = check_network_connectivity()
    s3_ok = test_s3_with_packages()
    hive_ok = test_hive_metastore_improved()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ ì§„ë‹¨ ê²°ê³¼ ìš”ì•½:")
    print(f"   AWS ìê²©ì¦ëª…: {'âœ… ì •ìƒ' if aws_ok else 'âŒ ë¬¸ì œ'}")
    print(f"   S3 ì—°ê²°: {'âœ… ì •ìƒ' if s3_ok else 'âŒ ë¬¸ì œ'}")
    print(f"   Hive Metastore: {'âœ… ì •ìƒ' if hive_ok else 'âŒ ë¬¸ì œ'}")
    
    if aws_ok and s3_ok and hive_ok:
        print("\nğŸ‰ ëª¨ë“  ì—°ê²°ì´ ì •ìƒì…ë‹ˆë‹¤! ETL íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if not aws_ok:
            print("   ğŸ”§ AWS ìê²©ì¦ëª…ì„ í™•ì¸í•˜ì„¸ìš”: aws configure ë˜ëŠ” IAM ì—­í•  ì„¤ì •")
        if not s3_ok:
            print("   ğŸ”§ S3 ì ‘ê·¼ ê¶Œí•œê³¼ ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        if not hive_ok:
            print("   ğŸ”§ Hive Metastore ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”")
