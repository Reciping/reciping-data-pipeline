#!/usr/bin/env python3
"""
ğŸ§Š Iceberg + Hive Metastore ê¸°ë°˜ Bronze to Silver ETL Pipeline
=============================================================

Apache Iceberg í…Œì´ë¸” í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ ë°ì´í„° ë ˆì´í¬ ê¸°ëŠ¥ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
- ACID íŠ¸ëœì­ì…˜
- ìŠ¤í‚¤ë§ˆ ì§„í™” (Schema Evolution)
- íƒ€ì„ íŠ¸ë˜ë¸” (Time Travel)
- ìŠ¤ëƒ…ìƒ· ê´€ë¦¬
- Hive Metastoreë¥¼ í†µí•œ í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ê´€ë¦¬

Author: Data Engineering Team
Date: 2025-08-08
"""

import os
import logging
from datetime import datetime
from typing import Optional

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, when, isnan, isnull, regexp_replace, 
    to_timestamp, date_format, year, month, dayofmonth, hour,
    expr, from_json, lit, current_timestamp
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, BooleanType, TimestampType, DateType, LongType, ArrayType
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class IcebergETLPipeline:
    """Iceberg ê¸°ë°˜ ETL íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        self.database_name = "recipe_analytics"
        
        # S3 ê²½ë¡œ ì„¤ì • - ìš´ì˜ í™˜ê²½ìš© S3 ì‚¬ìš©
        self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/warehouse/"
        self.s3_landing_zone = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
        
        # Hive Metastore ì„¤ì •
        self.hive_metastore_uri = "thrift://metastore:9083"
        
    def create_spark_session(self) -> SparkSession:
        """Icebergì™€ Hive Metastoreë¥¼ ì§€ì›í•˜ëŠ” SparkSession ìƒì„±"""
        
        print("ğŸ§Š Iceberg + Hive Metastore SparkSession ìƒì„± ì¤‘...")
        
        try:
            spark = SparkSession.builder \
                .appName("IcebergETLPipeline") \
                .config("spark.sql.session.timeZone", "Asia/Seoul") \
                .config("spark.driver.memory", "3g") \
                .config("spark.executor.memory", "3g") \
                .config("spark.driver.maxResultSize", "2g") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.maxBatchSize", "128MB") \
                .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "64MB") \
                .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
                .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
                .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
                .config("spark.sql.catalog.iceberg_catalog.uri", self.hive_metastore_uri) \
                .config("spark.sql.catalog.iceberg_catalog.warehouse", self.s3_warehouse_path) \
                .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
                .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
                .config("spark.hadoop.fs.s3a.path.style.access", "true") \
                .config("spark.hadoop.fs.s3a.block.size", "134217728") \
                .config("spark.hadoop.fs.s3a.buffer.dir", "/tmp") \
                .config("spark.hadoop.fs.s3a.fast.upload", "true") \
                .config("spark.hadoop.fs.s3a.fast.upload.buffer", "bytebuffer") \
                .config("spark.hadoop.fs.s3a.multipart.size", "67108864") \
                .config("spark.hadoop.fs.s3a.multipart.threshold", "134217728") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .getOrCreate()
            
            # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
            spark.sparkContext.setLogLevel("WARN")
            
            print("âœ… Iceberg SparkSession ìƒì„± ì™„ë£Œ!")
            print(f"ğŸ“ Warehouse ê²½ë¡œ: {self.s3_warehouse_path}")
            print(f"ğŸ—„ï¸  Hive Metastore URI: {self.hive_metastore_uri}")
            
            self.spark = spark
            return spark
            
        except Exception as e:
            print(f"âŒ SparkSession ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def create_database_if_not_exists(self):
        """Iceberg ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
        try:
            print(f"ğŸ—ƒï¸  ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±: {self.catalog_name}.{self.database_name}")
            
            self.spark.sql(f"""
                CREATE DATABASE IF NOT EXISTS {self.catalog_name}.{self.database_name}
                COMMENT 'Recipe Analytics Database for Iceberg Tables'
                LOCATION '{self.s3_warehouse_path}{self.database_name}.db/'
            """)
            
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def define_event_schema(self) -> StructType:
        """ì´ë²¤íŠ¸ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜ - ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ì¶¤"""
        return StructType([
            StructField("anonymous_id", StringType(), True),
            StructField("context", StringType(), True),
            StructField("date", StringType(), True),
            StructField("event_id", StringType(), True),
            StructField("event_name", StringType(), True),
            StructField("event_properties", StringType(), True),
            StructField("session_id", StringType(), True),
            StructField("timestamp", StringType(), True),
            StructField("user_id", StringType(), True)
        ])
    
    def read_from_landing_zone(self) -> DataFrame:
        """ëœë”© ì¡´ì—ì„œ ì›ì‹œ ë°ì´í„° ì½ê¸°"""
        try:
            print("ğŸ“‚ ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸° ì‹œì‘...")
            print(f"ğŸ“ ê²½ë¡œ: {self.s3_landing_zone}")
            
            # JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
            schema = self.define_event_schema()
            
            # ëœë”© ì¡´ì—ì„œ JSON íŒŒì¼ ì½ê¸°
            df = self.spark.read \
                .option("multiline", "false") \
                .option("mode", "PERMISSIVE") \
                .option("columnNameOfCorruptRecord", "_corrupt_record") \
                .schema(schema) \
                .json(self.s3_landing_zone)
            
            row_count = df.count()
            print(f"âœ… ëœë”© ì¡´ ë°ì´í„° ë¡œë“œ ì„±ê³µ! í–‰ ìˆ˜: {row_count:,}")
            
            return df
            
        except Exception as e:
            print(f"âŒ ëœë”© ì¡´ ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            raise
    
    def clean_and_transform_data(self, df: DataFrame) -> DataFrame:
        """ë°ì´í„° ì •ì œ ë° ë³€í™˜ - archive/old_versions/bronze_to_silver.py ë¡œì§ ì™„ì „ ì ìš©"""
        try:
            print("ğŸ§¹ ë°ì´í„° ì •ì œ ë° ë³€í™˜ ì‹œì‘...")
            
            # --- 4.1. ì¤‘ì²©ëœ JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ê¸° ìœ„í•œ ìŠ¤í‚¤ë§ˆë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ ---
            context_schema = StructType([
                StructField("page", StructType([
                    StructField("name", StringType(), True),
                    StructField("url", StringType(), True),
                    StructField("path", StringType(), True)
                ]), True),
                StructField("user_segment", StringType(), True),
                StructField("activity_level", StringType(), True),
                StructField("cooking_style", StringType(), True),
                StructField("ab_test", StructType([
                    StructField("scenario", StringType(), True),
                    StructField("group", StringType(), True),
                    StructField("start_date", StringType(), True),
                    StructField("end_date", StringType(), True)
                ]), True)
            ])

            event_properties_schema = StructType([
                StructField("page_name", StringType(), True),
                StructField("referrer", StringType(), True),
                StructField("path", StringType(), True),
                StructField("method", StringType(), True),
                StructField("type", StringType(), True),
                StructField("search_type", StringType(), True),
                StructField("search_keyword", StringType(), True),
                StructField("selected_filters", ArrayType(StringType()), True),
                StructField("result_count", IntegerType(), True),
                StructField("list_type", StringType(), True),
                StructField("displayed_recipe_ids", ArrayType(StringType()), True),
                StructField("recipe_id", StringType(), True),
                StructField("rank", IntegerType(), True),
                StructField("action", StringType(), True),
                StructField("comment_length", IntegerType(), True),
                StructField("category", StringType(), True),
                StructField("ingredient_count", IntegerType(), True),
                StructField("ad_id", StringType(), True),
                StructField("ad_type", StringType(), True),
                StructField("position", StringType(), True),
                StructField("target_url", StringType(), True)
            ])

            # --- 4.2. JSON íŒŒì‹± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ìë£Œí˜• ë³€í™˜ ---
            # Bronze timestampëŠ” KSTë¡œ ë˜ì–´ ìˆìŒ: "2025-07-07T08:40:12.782565795+09:00"
            df_transformed = df \
                .withColumn("parsed_context", from_json(col("context"), context_schema)) \
                .withColumn("parsed_properties", from_json(col("event_properties"), event_properties_schema)) \
                .withColumn("kst_timestamp", col("timestamp").cast(TimestampType())) \
                .withColumn("utc_timestamp", expr("kst_timestamp - INTERVAL 9 HOURS")) \
                .withColumn("date", col("date").cast(DateType())) \
                .drop("context", "event_properties", "timestamp")

            print("âœ… JSON íŒŒì‹± ë° KST/UTC íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ì™„ë£Œ.")

            # --- 4.3. Silver Layer ì €ì¥ì„ ìœ„í•œ íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„± (KST ê¸°ì¤€) ---
            # kst_timestampë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„±
            df_with_partitions = df_transformed \
                .withColumn("year", year(col("kst_timestamp"))) \
                .withColumn("month", month(col("kst_timestamp"))) \
                .withColumn("day", dayofmonth(col("kst_timestamp"))) \
                .withColumn("hour", hour(col("kst_timestamp"))) \
                .withColumn("day_of_week", date_format(col("kst_timestamp"), "E")) # 'Mon', 'Tue' ë“± ìš”ì¼ ì¶”ì¶œ

            print("âœ… KST ê¸°ì¤€ íŒŒí‹°ì…˜ ì»¬ëŸ¼(year, month, day, hour) ìƒì„± ì™„ë£Œ.")

            # --- 4.4. ìµœì¢… ì»¬ëŸ¼ ì„ íƒ ë° ì •ë¦¬ (í‰íƒ„í™”) - ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•´ í•µì‹¬ ì»¬ëŸ¼ë§Œ ì„ íƒ ---
            df_silver = df_with_partitions.select(
                # ê¸°ë³¸ ì´ë²¤íŠ¸ ì •ë³´
                "event_id", 
                "event_name", 
                "user_id", 
                "anonymous_id", 
                "session_id", 
                
                # ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ (KSTì™€ UTC ëª¨ë‘ í¬í•¨)
                "kst_timestamp",  # í•œêµ­ ì‹œê°„ (ì›ë³¸)
                "utc_timestamp",  # UTC ì‹œê°„ (ë³€í™˜ë¨)
                "date",
                
                # KST ê¸°ì¤€ íŒŒìƒ ì»¬ëŸ¼ë“¤
                "year", 
                "month", 
                "day", 
                "hour",
                "day_of_week",
                
                # Contextì—ì„œ ì£¼ìš” ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ
                col("parsed_context.page.name").alias("page_name"),
                col("parsed_context.page.url").alias("page_url"),
                col("parsed_context.user_segment").alias("user_segment"),
                col("parsed_context.cooking_style").alias("cooking_style"),
                col("parsed_context.ab_test.group").alias("ab_test_group"),
                
                # Event Propertiesì—ì„œ ì£¼ìš” ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ
                col("parsed_properties.recipe_id").cast(LongType()).alias("prop_recipe_id"),
                col("parsed_properties.list_type").alias("prop_list_type"),
                col("parsed_properties.action").alias("prop_action"),
                col("parsed_properties.search_keyword").alias("prop_search_keyword"),
                col("parsed_properties.result_count").alias("prop_result_count")
            )
            
            # --- 4.5. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ---
            df_silver = df_silver.filter(col("event_id").isNotNull()).dropDuplicates(["event_id"])
            print("âœ… ì»¬ëŸ¼ í‰íƒ„í™” ë° ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì™„ë£Œ.")

            # --- 4.6. ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ---
            df_final = df_silver \
                .withColumn("processed_at", current_timestamp()) \
                .withColumn("data_source", lit("landing_zone")) \
                .withColumn("pipeline_version", lit("iceberg_v2.0"))

            # --- 4.7. ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì • ---
            df_final = df_final.coalesce(2)  # 2ê°œ íŒŒí‹°ì…˜ìœ¼ë¡œ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë”ìš± ê°ì†Œ

            clean_count = df_final.count()
            print(f"âœ… ë°ì´í„° ì •ì œ ì™„ë£Œ! ì •ì œëœ í–‰ ìˆ˜: {clean_count:,}")
            
            return df_final
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì •ì œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def create_iceberg_table_if_not_exists(self):
        """Iceberg í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°) - archive/old_versions/bronze_to_silver.py ìŠ¤í‚¤ë§ˆ ì ìš©"""
        try:
            table_name = f"{self.catalog_name}.{self.database_name}.user_events_silver"
            
            print(f"ğŸ§Š Iceberg í…Œì´ë¸” ìƒì„±: {table_name}")
            
            create_table_sql = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                event_id STRING,
                event_name STRING,
                user_id STRING,
                anonymous_id STRING,
                session_id STRING,
                kst_timestamp TIMESTAMP,
                utc_timestamp TIMESTAMP,
                date DATE,
                year INT,
                month INT,
                day INT,
                hour INT,
                day_of_week STRING,
                page_name STRING,
                page_url STRING,
                user_segment STRING,
                cooking_style STRING,
                ab_test_group STRING,
                prop_recipe_id BIGINT,
                prop_list_type STRING,
                prop_action STRING,
                prop_search_keyword STRING,
                prop_result_count INT,
                processed_at TIMESTAMP,
                data_source STRING,
                pipeline_version STRING
            )
            USING ICEBERG
            PARTITIONED BY (year, month, day, hour)
            TBLPROPERTIES (
                'write.distribution-mode' = 'hash',
                'write.upsert.enabled' = 'true',
                'format-version' = '2'
            )
            """
            
            self.spark.sql(create_table_sql)
            print(f"âœ… Iceberg í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ Iceberg í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def write_to_iceberg_table(self, df: DataFrame):
        """Iceberg í…Œì´ë¸”ì— ë°ì´í„° ì“°ê¸°"""
        try:
            table_name = f"{self.catalog_name}.{self.database_name}.user_events_silver"
            
            print(f"ğŸ§Š Iceberg í…Œì´ë¸”ì— ë°ì´í„° ì“°ê¸°: {table_name}")
            
            # Iceberg í…Œì´ë¸”ì— append ëª¨ë“œë¡œ ì“°ê¸°
            df.writeTo(table_name) \
                .option("write-audit-publish", "true") \
                .append()
            
            print(f"âœ… Iceberg í…Œì´ë¸” ì“°ê¸° ì™„ë£Œ!")
            
            # í…Œì´ë¸” ì •ë³´ í™•ì¸
            self.show_table_info(table_name)
            
        except Exception as e:
            print(f"âŒ Iceberg í…Œì´ë¸” ì“°ê¸° ì‹¤íŒ¨: {str(e)}")
            raise
    
    def show_table_info(self, table_name: str):
        """Iceberg í…Œì´ë¸” ì •ë³´ í‘œì‹œ"""
        try:
            print(f"\nğŸ“Š Iceberg í…Œì´ë¸” ì •ë³´: {table_name}")
            
            # í…Œì´ë¸” í–‰ ìˆ˜ í™•ì¸
            count_df = self.spark.sql(f"SELECT COUNT(*) as total_rows FROM {table_name}")
            total_rows = count_df.collect()[0]['total_rows']
            print(f"ğŸ“ˆ ì´ í–‰ ìˆ˜: {total_rows:,}")
            
            # ìŠ¤ëƒ…ìƒ· ì •ë³´ í™•ì¸
            snapshots_df = self.spark.sql(f"SELECT * FROM {table_name}.snapshots ORDER BY committed_at DESC LIMIT 5")
            print(f"\nğŸ“¸ ìµœê·¼ ìŠ¤ëƒ…ìƒ· (ìµœëŒ€ 5ê°œ):")
            snapshots_df.show(truncate=False)
            
            # íŒŒí‹°ì…˜ ì •ë³´ í™•ì¸
            partitions_df = self.spark.sql(f"SELECT year, month, day, hour, COUNT(*) as row_count FROM {table_name} GROUP BY year, month, day, hour ORDER BY year DESC, month DESC, day DESC, hour DESC")
            print(f"\nğŸ“… íŒŒí‹°ì…˜ë³„ ë°ì´í„° ë¶„í¬:")
            partitions_df.show()
            
        except Exception as e:
            print(f"âš ï¸  í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def run_pipeline(self):
        """ì „ì²´ ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            print("ğŸš€ Iceberg ETL íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
            print("=" * 60)
            
            # 1. SparkSession ìƒì„±
            self.create_spark_session()
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            self.create_database_if_not_exists()
            
            # 3. ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°
            raw_df = self.read_from_landing_zone()
            
            # 4. ë°ì´í„° ì •ì œ ë° ë³€í™˜
            clean_df = self.clean_and_transform_data(raw_df)
            
            # 5. Iceberg í…Œì´ë¸” ìƒì„±
            self.create_iceberg_table_if_not_exists()
            
            # 6. Iceberg í…Œì´ë¸”ì— ë°ì´í„° ì“°ê¸°
            self.write_to_iceberg_table(clean_df)
            
            print("\n" + "=" * 60)
            print("ğŸ‰ Iceberg ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            raise
        finally:
            if self.spark:
                print("ğŸ”š SparkSession ì¢…ë£Œ")
                self.spark.stop()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    pipeline = IcebergETLPipeline()
    pipeline.run_pipeline()

if __name__ == "__main__":
    main()
