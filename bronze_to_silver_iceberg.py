# bronze_to_silver_iceberg.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, year, month, dayofmonth, hour, date_format, to_timestamp, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType, DateType, TimestampType

def main():
    """
    S3 ëœë”© ì¡´ì˜ ì›ë³¸ íŒŒì¼ì„ ì½ì–´ Bronze, Silver ì•„ì´ìŠ¤ë²„ê·¸ í…Œì´ë¸”ì„ êµ¬ì¶•í•˜ëŠ”
    ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ ETL íŒŒì´í”„ë¼ì¸ (Iceberg + Hive Metastore).
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì • (ì„±ê³µí•œ ì„¤ì • ì ìš©)
        import os
        import subprocess
        
        # ì„±ê³µí•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['HADOOP_USER_NAME'] = 'root'
        os.environ['USER'] = 'root'
        os.environ['HOME'] = '/tmp'
        os.environ['JAVA_OPTS'] = '-Duser.name=root'
        # Ivy ì„¤ì •
        os.environ['IVY_HOME'] = '/tmp/.ivy2'
        os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.jars.ivy=/tmp/.ivy2 pyspark-shell'
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('/tmp/.ivy2', exist_ok=True)

        # -----------------------------------------------------------------------------
        # 1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„± (Iceberg + Hive Metastore ì„¤ì •)
        # -----------------------------------------------------------------------------
        print("ğŸ”§ SparkSession with Iceberg ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        
        spark = SparkSession.builder \
            .appName("Bronze_to_Silver_Iceberg_Pipeline") \
            .master("local[*]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.jars.ivy", "/tmp/.ivy2") \
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.3") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalogImplementation", "hive") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
            .config("spark.sql.catalog.spark_catalog.type", "hive") \
            .config("spark.sql.catalog.spark_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.spark_catalog.warehouse", "s3a://reciping-user-event-logs/warehouse") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "")) \
            .config("spark.hadoop.fs.s3a.region", "ap-northeast-2") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.path.style.access", "false") \
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true") \
            .config("spark.hadoop.fs.s3a.fast.upload", "true") \
            .config("spark.hadoop.fs.s3a.block.size", "134217728") \
            .enableHiveSupport() \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSession with Icebergê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # -----------------------------------------------------------------------------
        # 2. ğŸ—ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° ê´€ë¦¬
        # -----------------------------------------------------------------------------
        print("\nğŸ—ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ êµ¬ì„±...")
        
        # Bronze, Silver, Gold ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        spark.sql("CREATE DATABASE IF NOT EXISTS bronze_db COMMENT 'Raw data from landing zone'")
        spark.sql("CREATE DATABASE IF NOT EXISTS silver_db COMMENT 'Cleaned and transformed data'")
        spark.sql("CREATE DATABASE IF NOT EXISTS gold_db COMMENT 'Business aggregated data'")
        
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ êµ¬ì„± ì™„ë£Œ")
        
        # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ í™•ì¸
        print("\nğŸ“‹ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:")
        spark.sql("SHOW DATABASES").show()

        # -----------------------------------------------------------------------------
        # 3. ğŸ¥‰ Bronze Layer - Iceberg í…Œì´ë¸” êµ¬ì¶•
        # -----------------------------------------------------------------------------
        print("\nğŸ¥‰ Bronze Layer (Iceberg) êµ¬ì¶• ì‹œì‘...")
        
        # S3 ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°
        landing_zone_path = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
        print(f"ğŸ“‚ ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°: {landing_zone_path}")
        
        try:
            df_raw = spark.read.json(landing_zone_path)
            row_count = df_raw.count()
            print(f"âœ… ëœë”© ì¡´ ë°ì´í„° ë¡œë“œ ì„±ê³µ! í–‰ ìˆ˜: {row_count:,}")
            
            # ì²˜ë¦¬ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            df_raw_with_metadata = df_raw.withColumn("ingestion_timestamp", current_timestamp())
            
            # Bronze Iceberg í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            try:
                existing_bronze = spark.table("bronze_db.raw_events")
                existing_count = existing_bronze.count()
                print(f"ğŸ“Š ê¸°ì¡´ Bronze í…Œì´ë¸” í–‰ ìˆ˜: {existing_count:,}")
                
                # ê¸°ì¡´ ë°ì´í„°ì™€ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì¦ë¶„ ë¡œë“œ
                max_timestamp = spark.sql("SELECT MAX(ingestion_timestamp) as max_ts FROM bronze_db.raw_events").collect()[0]["max_ts"]
                if max_timestamp:
                    print(f"ğŸ”„ ì¦ë¶„ ë°ì´í„° ë¡œë“œ ëª¨ë“œ (ë§ˆì§€ë§‰ ì ì¬: {max_timestamp})")
                    # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ì´ë²¤íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§
                    insert_mode = "append"
                else:
                    insert_mode = "overwrite"
            except:
                print("ğŸ“‹ Bronze Iceberg í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                insert_mode = "overwrite"
            
            # Bronze Iceberg í…Œì´ë¸” ìƒì„±/ì—…ë°ì´íŠ¸ (ê¸°ì¡´ í…Œì´ë¸” í™•ì¸ í›„ ì²˜ë¦¬)
            try:
                # ê¸°ì¡´ í…Œì´ë¸”ì´ ìˆìœ¼ë©´ append, ì—†ìœ¼ë©´ create
                spark.sql("DESCRIBE TABLE bronze_db.raw_events")
                print("ğŸ“‹ ê¸°ì¡´ Bronze í…Œì´ë¸” ë°œê²¬. ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                df_raw_with_metadata.writeTo("bronze_db.raw_events").append()
            except:
                print("ğŸ“‹ ìƒˆë¡œìš´ Bronze í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                df_raw_with_metadata.writeTo("bronze_db.raw_events").tableProperty("format-version", "2").create()
            
            print("âœ… Bronze Iceberg í…Œì´ë¸” 'bronze_db.raw_events' ìƒì„±/ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëœë”© ì¡´ì—ì„œ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("ğŸ’¡ upload_to_landing_zone.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            spark.stop()
            return

        # -----------------------------------------------------------------------------
        # 4. ğŸ¥ˆ Silver Layer - Iceberg í…Œì´ë¸” êµ¬ì¶• (ê³ ê¸‰ ë³€í™˜)
        # -----------------------------------------------------------------------------
        print("\nğŸ¥ˆ Silver Layer (Iceberg) êµ¬ì¶• ì‹œì‘...")
        
        # Bronze Iceberg í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì½ê¸°
        df_bronze = spark.table("bronze_db.raw_events")
        bronze_count = df_bronze.count()
        print(f"ğŸ“Š Bronzeì—ì„œ {bronze_count:,}í–‰ì˜ ë°ì´í„°ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.")

        # --- 4.1. ìŠ¤í‚¤ë§ˆ ì •ì˜ (ê°œì„ ëœ ë²„ì „) ---
        context_schema = StructType([
            StructField("page", StructType([
                StructField("name", StringType(), True),
                StructField("url", StringType(), True),
                StructField("path", StringType(), True)
            ]), True),
            StructField("user_segment", StringType(), True),
            StructField("activity_level", StringType(), True),
            StructField("cooking_style", StringType(), True),
            StructField("ab_test", StructType([
                StructField("scenario", StringType(), True),
                StructField("group", StringType(), True),
                StructField("start_date", StringType(), True),
                StructField("end_date", StringType(), True)
            ]), True)
        ])

        event_properties_schema = StructType([
            StructField("page_name", StringType(), True), StructField("referrer", StringType(), True),
            StructField("path", StringType(), True), StructField("method", StringType(), True),
            StructField("type", StringType(), True), StructField("search_type", StringType(), True),
            StructField("search_keyword", StringType(), True), StructField("selected_filters", ArrayType(StringType()), True),
            StructField("result_count", IntegerType(), True), StructField("list_type", StringType(), True),
            StructField("displayed_recipe_ids", ArrayType(StringType()), True), StructField("recipe_id", StringType(), True),
            StructField("rank", IntegerType(), True), StructField("action", StringType(), True),
            StructField("comment_length", IntegerType(), True), StructField("category", StringType(), True),
            StructField("ingredient_count", IntegerType(), True), StructField("ad_id", StringType(), True),
            StructField("ad_type", StringType(), True), StructField("position", StringType(), True),
            StructField("target_url", StringType(), True)
        ])

        # --- 4.2. JSON íŒŒì‹± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ---
        print("ğŸ”§ JSON íŒŒì‹± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ì¤‘...")
        df_transformed = df_bronze \
            .withColumn("parsed_context", from_json(col("context"), context_schema)) \
            .withColumn("parsed_properties", from_json(col("event_properties"), event_properties_schema)) \
            .withColumn("timestamp_parsed", to_timestamp(col("timestamp"))) \
            .withColumn("date_parsed", col("date").cast(DateType())) \
            .withColumn("processing_timestamp", current_timestamp()) \
            .drop("context", "event_properties")

        # --- 4.3. íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„± (KST ê¸°ì¤€) ---
        print("ğŸ“… íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„± ì¤‘...")
        df_with_partitions = df_transformed \
            .withColumn("year", year(col("timestamp_parsed"))) \
            .withColumn("month", month(col("timestamp_parsed"))) \
            .withColumn("day", dayofmonth(col("timestamp_parsed"))) \
            .withColumn("hour", hour(col("timestamp_parsed")))

        # --- 4.4. ì»¬ëŸ¼ í‰íƒ„í™” (í–¥ìƒëœ ë²„ì „) ---
        print("ğŸ—‚ï¸ ì»¬ëŸ¼ í‰íƒ„í™” ì¤‘...")
        df_silver_flat = df_with_partitions.select(
            # ê¸°ë³¸ ì´ë²¤íŠ¸ ì •ë³´
            "event_id", "event_name", "user_id", "anonymous_id", "session_id",
            col("timestamp_parsed").alias("event_timestamp"), 
            col("date_parsed").alias("event_date"),
            col("ingestion_timestamp"),
            col("processing_timestamp"),
            
            # íŒŒí‹°ì…˜ ì»¬ëŸ¼
            "year", "month", "day", "hour",
            
            # Context ì •ë³´
            col("parsed_context.page.name").alias("page_name"),
            col("parsed_context.page.url").alias("page_url"),
            col("parsed_context.page.path").alias("page_path"),
            col("parsed_context.user_segment").alias("user_segment"),
            col("parsed_context.activity_level").alias("activity_level"),
            col("parsed_context.cooking_style").alias("cooking_style"),
            col("parsed_context.ab_test.group").alias("ab_test_group"),
            col("parsed_context.ab_test.scenario").alias("ab_test_scenario"),
            
            # Event Properties
            col("parsed_properties.page_name").alias("prop_page_name"),
            col("parsed_properties.referrer").alias("prop_referrer"),
            col("parsed_properties.path").alias("prop_path"),
            col("parsed_properties.method").alias("prop_method"),
            col("parsed_properties.type").alias("prop_type"),
            col("parsed_properties.search_type").alias("prop_search_type"),
            col("parsed_properties.search_keyword").alias("prop_search_keyword"),
            col("parsed_properties.selected_filters").alias("prop_selected_filters"),
            col("parsed_properties.result_count").alias("prop_result_count"),
            col("parsed_properties.list_type").alias("prop_list_type"),
            col("parsed_properties.displayed_recipe_ids").alias("prop_displayed_recipe_ids"),
            col("parsed_properties.recipe_id").cast(LongType()).alias("prop_recipe_id"),
            col("parsed_properties.rank").alias("prop_rank"),
            col("parsed_properties.action").alias("prop_action"),
            col("parsed_properties.comment_length").alias("prop_comment_length"),
            col("parsed_properties.category").alias("prop_category"),
            col("parsed_properties.ingredient_count").alias("prop_ingredient_count"),
            col("parsed_properties.ad_id").alias("prop_ad_id"),
            col("parsed_properties.ad_type").alias("prop_ad_type"),
            col("parsed_properties.position").alias("prop_position"),
            col("parsed_properties.target_url").alias("prop_target_url")
        )
        
        # --- 4.5. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ (í–¥ìƒëœ ë²„ì „) ---
        print("ğŸ” ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì¤‘...")
        df_silver_clean = df_silver_flat \
            .filter(col("event_id").isNotNull()) \
            .filter(col("event_timestamp").isNotNull()) \
            .dropDuplicates(["event_id"])
        
        final_count = df_silver_clean.count()
        print(f"âœ… ì»¬ëŸ¼ í‰íƒ„í™” ë° ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì™„ë£Œ. ìµœì¢… í–‰ ìˆ˜: {final_count:,}")
        
        # Silver ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        print("\nğŸ“Š Silver Layer ìƒ˜í”Œ ë°ì´í„° (ìƒìœ„ 3í–‰):")
        df_silver_clean.show(3, truncate=True)
        
        # ì´ë²¤íŠ¸ë³„ ë¶„í¬ í™•ì¸
        print("\nğŸ“Š ì´ë²¤íŠ¸ë³„ ë¶„í¬:")
        df_silver_clean.groupBy('event_name').count().orderBy('count', ascending=False).show(10)

        # --- 4.6. Silver Iceberg í…Œì´ë¸” ìƒì„± (íŒŒí‹°ì…˜ ì ìš©) ---
        print("\nğŸ’¾ Silver Iceberg í…Œì´ë¸”ë¡œ ì €ì¥ ì¤‘...")
        
        # ì„±ëŠ¥ ìµœì í™”: íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •
        df_silver_optimized = df_silver_clean.coalesce(4)
        
        # Iceberg í…Œì´ë¸” ìƒì„± (íŒŒí‹°ì…˜ ì ìš©)
        try:
            # ê¸°ì¡´ í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            spark.sql("DROP TABLE IF EXISTS silver_db.cleaned_events")
            df_silver_optimized.writeTo("silver_db.cleaned_events") \
                .partitionedBy("year", "month", "day") \
                .tableProperty("format-version", "2") \
                .tableProperty("write.target-file-size-bytes", "134217728") \
                .create()
        except Exception as e:
            print(f"âš ï¸ Silver í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            # Fallback: ê¸°ì¡´ í…Œì´ë¸”ì´ ìˆë‹¤ë©´ append
            try:
                df_silver_optimized.writeTo("silver_db.cleaned_events").append()
                print("âœ… ê¸°ì¡´ Silver í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
            except Exception as e2:
                print(f"âŒ Silver í…Œì´ë¸” ì²˜ë¦¬ ì‹¤íŒ¨: {e2}")
                return
        
        print("âœ… Silver Iceberg í…Œì´ë¸” 'silver_db.cleaned_events' ìƒì„± ì™„ë£Œ")

        # -----------------------------------------------------------------------------
        # 5. ğŸ“Š í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ë° í†µê³„ ì •ë³´
        # -----------------------------------------------------------------------------
        print("\nğŸ“Š í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ì •ë³´:")
        
        # í…Œì´ë¸” ì •ë³´ ì¡°íšŒ
        print("\nğŸ—ƒï¸ Bronze í…Œì´ë¸” ì •ë³´:")
        spark.sql("DESCRIBE TABLE EXTENDED bronze_db.raw_events").show(truncate=False)
        
        print("\nğŸ—ƒï¸ Silver í…Œì´ë¸” ì •ë³´:")
        spark.sql("DESCRIBE TABLE EXTENDED silver_db.cleaned_events").show(truncate=False)
        
        # íŒŒí‹°ì…˜ ì •ë³´ í™•ì¸
        print("\nğŸ“ Silver í…Œì´ë¸” íŒŒí‹°ì…˜ ì •ë³´:")
        spark.sql("SHOW PARTITIONS silver_db.cleaned_events").show()

        # -----------------------------------------------------------------------------
        # 6. ğŸ§ª Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        # -----------------------------------------------------------------------------
        print("\nğŸ§ª Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        # ìŠ¤ëƒ…ìƒ· ì´ë ¥ ì¡°íšŒ
        print("\nğŸ“¸ Bronze í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì´ë ¥:")
        spark.sql("SELECT * FROM bronze_db.raw_events.snapshots").show(truncate=False)
        
        print("\nğŸ“¸ Silver í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì´ë ¥:")
        spark.sql("SELECT * FROM silver_db.cleaned_events.snapshots").show(truncate=False)
        
        # í…Œì´ë¸” ì†ì„± í™•ì¸
        print("\nâš™ï¸ Silver í…Œì´ë¸” ì†ì„±:")
        spark.sql("SHOW TBLPROPERTIES silver_db.cleaned_events").show(truncate=False)

        # -----------------------------------------------------------------------------
        # 7. ê²€ì¦ ë° ìš”ì•½
        # -----------------------------------------------------------------------------
        print("\nğŸ“ˆ Iceberg ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ìš”ì•½:")
        print(f"ğŸ¥‰ Bronze í–‰ ìˆ˜: {bronze_count:,}")
        print(f"ğŸ¥ˆ Silver í–‰ ìˆ˜: {final_count:,}")
        print(f"ğŸ“Š ìƒì„±ëœ ë°ì´í„°ë² ì´ìŠ¤: bronze_db, silver_db, gold_db")
        print(f"ğŸ—ƒï¸ Bronze í…Œì´ë¸”: bronze_db.raw_events (Iceberg)")
        print(f"ğŸ—ƒï¸ Silver í…Œì´ë¸”: silver_db.cleaned_events (Iceberg, íŒŒí‹°ì…˜: year/month/day)")
        print(f"ğŸ—ï¸ ë©”íƒ€ìŠ¤í† ì–´: Hive Metastore (thrift://metastore:9083)")
        print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ì†Œ: S3 (s3a://reciping-user-event-logs/warehouse)")

        # -----------------------------------------------------------------------------
        # 8. ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        spark.stop()
        print("âœ… SparkSessionì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # ìŠ¤íŒŒí¬ ì„¸ì…˜ì´ ìˆë‹¤ë©´ ì¢…ë£Œ
        try:
            spark.stop()
        except:
            pass

# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
if __name__ == "__main__":
    main()
