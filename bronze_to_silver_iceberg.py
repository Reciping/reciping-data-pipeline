#!/usr/bin/env python3
"""
ğŸ§Š Iceberg + Hive Metastore ê¸°ë°˜ Bronze to Silver ETL Pipeline
=============================================================

Apache Iceberg í…Œì´ë¸” í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ ë°ì´í„° ë ˆì´í¬ ê¸°ëŠ¥ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
- ACID íŠ¸ëœì­ì…˜
- ìŠ¤í‚¤ë§ˆ ì§„í™” (Schema Evolution)
- íƒ€ì„ íŠ¸ë˜ë¸” (Time Travel)
- ìŠ¤ëƒ…ìƒ· ê´€ë¦¬
- Hive Metastoreë¥¼ í†µí•œ í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ê´€ë¦¬

Author: Data Engineering Team
Date: 2025-08-31

ì„ì‹œ ìŠ¤í…Œì´ì§•ì„ í™œìš©í•œ ì²­í¬ ì²˜ë¦¬ Iceberg ETL Pipeline
ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ íŒŒì¼ ìµœì í™”ë¥¼ ë™ì‹œì— ë‹¬ì„±
"""

import os
import logging
from datetime import datetime
from typing import Optional, List
import time

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, when, isnan, isnull, regexp_replace, 
    to_timestamp, date_format, year, month, dayofmonth, hour,
    expr, from_json, lit, current_timestamp, monotonically_increasing_id
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, BooleanType, TimestampType, DateType, LongType, ArrayType
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedChunkedETL:
    """ì„ì‹œ ìŠ¤í…Œì´ì§• í™œìš© ì²­í¬ ì²˜ë¦¬ ETL"""
    
    def __init__(self, chunk_size: int = 100000, test_mode: bool = True):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        self.hive_metastore_uri = "thrift://10.0.11.86:9083"
        
        # ì²­í¬ ì²˜ë¦¬ ì„¤ì •
        self.chunk_size = chunk_size
        self.processed_chunks = 0
        self.failed_chunks = []
        
        # í…ŒìŠ¤íŠ¸/ìš´ì˜ í™˜ê²½ ë¶„ë¦¬
        if test_mode:
            print("=== í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
            self.database_name = "recipe_analytics_test"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/test_warehouse/"
            self.s3_landing_zone = "s3a://reciping-user-event-logs/bronze/landing-zone/events/test-sample/"
            self.s3_temp_path = "s3a://reciping-user-event-logs/temp/test_chunked_processing/"
            self.table_suffix = "_test"
        else:
            print("=== ìš´ì˜ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
            self.database_name = "recipe_analytics"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/warehouse/"
            self.s3_landing_zone = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
            self.s3_temp_path = "s3a://reciping-user-event-logs/temp/chunked_processing/"
            self.table_suffix = ""
        
        print(f"ë°ì´í„°ë² ì´ìŠ¤: {self.database_name}")
        print(f"ì›¨ì–´í•˜ìš°ìŠ¤ ê²½ë¡œ: {self.s3_warehouse_path}")
        print(f"ì„ì‹œ ì²˜ë¦¬ ê²½ë¡œ: {self.s3_temp_path}")
        
    def create_spark_session(self) -> SparkSession:
        """SparkSession ìƒì„±"""
        print("SparkSession ìƒì„± ì¤‘...")
        
        try:
            spark = SparkSession.builder \
                .appName("OptimizedChunkedETL") \
                .config("spark.sql.session.timeZone", "Asia/Seoul") \
                .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
                .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
                .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
                .config("spark.sql.catalog.iceberg_catalog.uri", self.hive_metastore_uri) \
                .config("spark.sql.catalog.iceberg_catalog.warehouse", self.s3_warehouse_path) \
                .getOrCreate()
            
            spark.sparkContext.setLogLevel("WARN")
            
            print("SparkSession ìƒì„± ì™„ë£Œ")
            self.spark = spark
            return spark
            
        except Exception as e:
            print(f"SparkSession ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def create_database_if_not_exists(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
        try:
            print(f"ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±: {self.catalog_name}.{self.database_name}")
            
            self.spark.sql(f"""
                CREATE DATABASE IF NOT EXISTS {self.catalog_name}.{self.database_name}
                COMMENT 'Recipe Analytics Database - Optimized Chunked Processing'
                LOCATION '{self.s3_warehouse_path}{self.database_name}.db/'
            """)
            
            print("ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def define_event_schema(self) -> StructType:
        """JSONL ìŠ¤í‚¤ë§ˆ ì •ì˜"""
        return StructType([
            StructField("anonymous_id", StringType(), True),
            StructField("context", StringType(), True),
            StructField("date", StringType(), True),
            StructField("event_id", StringType(), True),
            StructField("event_name", StringType(), True),
            StructField("event_properties", StringType(), True),
            StructField("session_id", StringType(), True),
            StructField("timestamp", StringType(), True),
            StructField("user_id", StringType(), True)
        ])
    
    def cleanup_temp_directory(self):
        """S3 ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬"""
        try:
            print(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬: {self.s3_temp_path}")
            
            try:
                test_df = self.spark.read.option("multiline", "false").text(self.s3_temp_path)
                print("ê¸°ì¡´ ì„ì‹œ íŒŒì¼ì´ ì¡´ì¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë®ì–´ì“°ê¸°ë©ë‹ˆë‹¤)")
            except:
                print("ì„ì‹œ ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìŒ")
                
        except Exception as e:
            print(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {str(e)}")
    
    def optimized_chunked_processing(self):
        """S3 íŒŒì¼ ë‹¨ìœ„ë¡œ ETLì„ ìˆ˜í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ ë³‘ëª©ì„ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°"""
        import json
        
        try:
            print("=== S3 íŒŒì¼ ë‹¨ìœ„ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ===")
            pipeline_start = time.time()
            
            # 1. S3 ëœë”© ì¡´ì˜ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            # Hadoop Path ê°ì²´ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ sparkContextë¥¼ í™œìš©í•©ë‹ˆë‹¤.
            URI = self.spark.sparkContext._gateway.jvm.java.net.URI
            Path = self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path
            fs = Path(self.s3_landing_zone).getFileSystem(self.spark.sparkContext._jsc.hadoopConfiguration())
            
            # listStatusë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼(ë° ë””ë ‰í„°ë¦¬) ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            file_statuses = fs.listStatus(Path(self.s3_landing_zone))
            
            # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤. (ë””ë ‰í„°ë¦¬ ì œì™¸)
            file_paths = [str(status.getPath()) for status in file_statuses if status.isFile()]
            
            if not file_paths:
                print("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
            
            print(f"ì´ {len(file_paths)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

            # 2. ê° íŒŒì¼ì„ ìˆœíšŒí•˜ë©° ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            for i, file_path in enumerate(file_paths):
                print(f"\n--- íŒŒì¼ {i+1}/{len(file_paths)} ì²˜ë¦¬ ì‹œì‘: {file_path.split('/')[-1]} ---")
                
                # 2.1. íŒŒì¼ í•˜ë‚˜ë§Œ í…ìŠ¤íŠ¸ë¡œ ì½ê¸°
                lines_df = self.spark.read.text(file_path)
                
                # 2.2. RDDë¡œ ë³€í™˜í•˜ì—¬ ë³‘ë ¬ JSON íŒŒì‹±
                def safe_json_loads(line_str):
                    try:
                        return json.loads(line_str.value)
                    except (json.JSONDecodeError, AttributeError):
                        return None

                rdd = lines_df.rdd.map(safe_json_loads).filter(lambda x: x is not None)
                
                # 2.3. DataFrameìœ¼ë¡œ ë³€í™˜
                schema = self.define_event_schema()
                df = self.spark.createDataFrame(rdd, schema)
                
                # 2.4. ë°ì´í„° ì •ì œ ë° ë³€í™˜ (ì¤‘ë³µ ì œê±° í¬í•¨)
                print("ë°ì´í„° ì •ì œ ë° ë³€í™˜ ì¤‘...")
                transformed_df = self.transform_chunk_data(df, i + 1)
                transformed_df = transformed_df.dropDuplicates(["event_id"]) # íŒŒì¼ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°
                
                # 2.5. Iceberg í…Œì´ë¸”ì— ë°”ë¡œ ì €ì¥
                print("Iceberg í…Œì´ë¸”ì— ì €ì¥ ì¤‘...")
                self.write_to_iceberg_table(transformed_df)
                
                print(f"--- íŒŒì¼ {i+1} ì²˜ë¦¬ ì™„ë£Œ ---")

            pipeline_elapsed = time.time() - pipeline_start
            print(f"\nğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {pipeline_elapsed:.1f}ì´ˆ)")

        except Exception as e:
            print(f"íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
            import traceback
            print(traceback.format_exc())
            raise
    
    def process_chunk_and_save_temp(self, chunk_df: DataFrame, chunk_id: int) -> bool:
        """DataFrame ì²­í¬ ì²˜ë¦¬ ë° ì„ì‹œ ì €ì¥"""
        try:
            print(f"ì²­í¬ {chunk_id} ì²˜ë¦¬ ì¤‘...")
            start_time = time.time()
            
            # ë°ì´í„° ë³€í™˜
            transformed_df = self.transform_chunk_data(chunk_df, chunk_id)
            
            # ì„ì‹œ ê²½ë¡œì— Parquetìœ¼ë¡œ ì €ì¥
            temp_chunk_path = f"{self.s3_temp_path}/chunk_{chunk_id:03d}"
            
            transformed_df.coalesce(1) \
                .write \
                .mode("overwrite") \
                .parquet(temp_chunk_path)
            
            elapsed = time.time() - start_time
            print(f"ì²­í¬ {chunk_id} ì„ì‹œ ì €ì¥ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
            
            self.processed_chunks += 1
            return True
            
        except Exception as e:
            print(f"ì²­í¬ {chunk_id} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            self.failed_chunks.append(chunk_id)
            return False
    
    def transform_chunk_data(self, chunk_df: DataFrame, chunk_id: int) -> DataFrame:
        """ì²­í¬ ë°ì´í„° ë³€í™˜ (ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ)"""
        try:
            # ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ì •ì˜
            context_schema = StructType([
                StructField("page", StructType([
                    StructField("name", StringType(), True),
                    StructField("url", StringType(), True),
                    StructField("path", StringType(), True)
                ]), True),
                StructField("user_segment", StringType(), True),
                StructField("activity_level", StringType(), True),
                StructField("cooking_style", StringType(), True),
                StructField("ab_test", StructType([
                    StructField("scenario", StringType(), True),
                    StructField("group", StringType(), True),
                    StructField("start_date", StringType(), True),
                    StructField("end_date", StringType(), True)
                ]), True)
            ])

            event_properties_schema = StructType([
                StructField("page_name", StringType(), True),
                StructField("referrer", StringType(), True),
                StructField("recipe_id", StringType(), True),
                StructField("list_type", StringType(), True),
                StructField("action", StringType(), True),
                StructField("search_keyword", StringType(), True),
                StructField("result_count", IntegerType(), True)
            ])

            # ë³€í™˜ ë¡œì§
            df_transformed = chunk_df \
                .withColumn("parsed_context", from_json(col("context"), context_schema)) \
                .withColumn("parsed_properties", from_json(col("event_properties"), event_properties_schema)) \
                .withColumn("kst_timestamp", col("timestamp").cast(TimestampType())) \
                .withColumn("utc_timestamp", expr("kst_timestamp - INTERVAL 9 HOURS")) \
                .withColumn("date", col("date").cast(DateType())) \
                .withColumn("year", year(col("kst_timestamp"))) \
                .withColumn("month", month(col("kst_timestamp"))) \
                .withColumn("day", dayofmonth(col("kst_timestamp"))) \
                .withColumn("hour", hour(col("kst_timestamp"))) \
                .withColumn("day_of_week", date_format(col("kst_timestamp"), "E")) \
                .drop("context", "event_properties", "timestamp")

            # ìµœì¢… ì»¬ëŸ¼ ì„ íƒ
            df_final = df_transformed.select(
                "event_id", "event_name", "user_id", "anonymous_id", "session_id",
                "kst_timestamp", "utc_timestamp", "date",
                "year", "month", "day", "hour", "day_of_week",
                col("parsed_context.page.name").alias("page_name"),
                col("parsed_context.page.url").alias("page_url"),
                col("parsed_context.user_segment").alias("user_segment"),
                col("parsed_context.cooking_style").alias("cooking_style"),
                col("parsed_context.ab_test.group").alias("ab_test_group"),
                col("parsed_properties.recipe_id").cast(LongType()).alias("prop_recipe_id"),
                col("parsed_properties.list_type").alias("prop_list_type"),
                col("parsed_properties.action").alias("prop_action"),
                col("parsed_properties.search_keyword").alias("prop_search_keyword"),
                col("parsed_properties.result_count").alias("prop_result_count")
            ) \
            .withColumn("processed_at", current_timestamp()) \
            .withColumn("data_source", lit("landing_zone")) \
            .withColumn("pipeline_version", lit("chunked_v1.0")) \
            .withColumn("chunk_id", lit(chunk_id))

            return df_final
            
        except Exception as e:
            print(f"ì²­í¬ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def write_to_iceberg_table(self, df: DataFrame):
        """Iceberg í…Œì´ë¸”ì— ì§ì ‘ ì €ì¥ - count() ì—†ì´"""
        try:
            table_name = f"{self.catalog_name}.{self.database_name}.user_events_silver{self.table_suffix}"
            
            print("Iceberg í…Œì´ë¸”ì— ì €ì¥ ì¤‘...")
            df.writeTo(table_name).append()
            print("ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"Iceberg ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def create_iceberg_table_if_not_exists(self):
        """Iceberg í…Œì´ë¸” ìƒì„±"""
        try:
            table_name = f"{self.catalog_name}.{self.database_name}.user_events_silver{self.table_suffix}"
            
            print(f"Iceberg í…Œì´ë¸” ìƒì„±: {table_name}")
            
            create_table_sql = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                event_id STRING,
                event_name STRING,
                user_id STRING,
                anonymous_id STRING,
                session_id STRING,
                kst_timestamp TIMESTAMP,
                utc_timestamp TIMESTAMP,
                date DATE,
                year INT,
                month INT,
                day INT,
                hour INT,
                day_of_week STRING,
                page_name STRING,
                page_url STRING,
                user_segment STRING,
                cooking_style STRING,
                ab_test_group STRING,
                prop_recipe_id BIGINT,
                prop_list_type STRING,
                prop_action STRING,
                prop_search_keyword STRING,
                prop_result_count INT,
                processed_at TIMESTAMP,
                data_source STRING,
                pipeline_version STRING,
                chunk_id INT
            )
            USING ICEBERG
            PARTITIONED BY (year, month, day)
            TBLPROPERTIES (
                'write.distribution-mode' = 'hash',
                'write.upsert.enabled' = 'true',
                'format-version' = '2'
            )
            """
            
            self.spark.sql(create_table_sql)
            print("Iceberg í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"Iceberg í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def print_pipeline_summary(self, total_rows: int, num_chunks: int, elapsed_time: float):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½"""
        print(f"\n{'='*60}")
        print("ì²­í¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼")
        print(f"{'='*60}")
        print(f"ì´ ì²˜ë¦¬ í–‰ ìˆ˜: {total_rows:,}")
        print(f"ì´ ì²­í¬ ìˆ˜: {num_chunks}")
        print(f"ì„±ê³µí•œ ì²­í¬: {self.processed_chunks}")
        print(f"ì‹¤íŒ¨í•œ ì²­í¬: {len(self.failed_chunks)}")
        print(f"ì „ì²´ ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        if num_chunks > 0:
            print(f"í‰ê·  ì²­í¬ ì²˜ë¦¬ì‹œê°„: {elapsed_time/num_chunks:.1f}ì´ˆ")
        print(f"ì‹œê°„ë‹¹ ì²˜ë¦¬ëŸ‰: {total_rows / elapsed_time * 3600:.0f} í–‰/ì‹œê°„")
        
        if self.failed_chunks:
            print(f"ì‹¤íŒ¨í•œ ì²­í¬ ID: {self.failed_chunks}")
        
        print(f"{'='*60}")
    
    def run_pipeline(self):
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            print("ì²­í¬ ì²˜ë¦¬ ê¸°ë°˜ ETL íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            print("=" * 50)
            
            # 1. SparkSession ìƒì„±
            self.create_spark_session()
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            self.create_database_if_not_exists()
            
            # 3. Iceberg í…Œì´ë¸” ìƒì„±
            self.create_iceberg_table_if_not_exists()
            
            # 4. ìµœì í™”ëœ ì²­í¬ ì²˜ë¦¬ ì‹¤í–‰
            self.optimized_chunked_processing()
            
            print("=" * 50)
            print("ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
            import traceback
            print(traceback.format_exc())
            raise
        finally:
            if self.spark:
                print("SparkSession ì¢…ë£Œ")
                self.spark.stop()

def main():
    # í™˜ê²½ë³€ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì œì–´
    import os
    import sys  # <--- sys ëª¨ë“ˆì„ import í•©ë‹ˆë‹¤.

    # [ìˆ˜ì • ì „]
    # test_mode = os.getenv('TEST_MODE', 'true').lower() == 'true'
    # chunk_size = int(os.getenv('CHUNK_SIZE', '100000'))

    # [ìˆ˜ì • í›„]
    # sys.argv[0]ì€ ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ ìì²´ì´ë¯€ë¡œ, ì¸ìëŠ” 1ë²ˆë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
    test_mode = sys.argv[1].lower() == 'true'
    chunk_size = int(sys.argv[2])
    
    print(f"ì‹¤í–‰ ëª¨ë“œ: {'í…ŒìŠ¤íŠ¸' if test_mode else 'ìš´ì˜'}")
    print(f"ì²­í¬ í¬ê¸°: {chunk_size:,}")
    
    pipeline = OptimizedChunkedETL(chunk_size=chunk_size, test_mode=test_mode)
    pipeline.run_pipeline()


if __name__ == "__main__":
    main()