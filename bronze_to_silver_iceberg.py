#!/usr/bin/env python3
"""
ğŸ§Š Iceberg + Hive Metastore ê¸°ë°˜ Bronze to Silver ETL Pipeline
=============================================================

Apache Iceberg í…Œì´ë¸” í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ ë°ì´í„° ë ˆì´í¬ ê¸°ëŠ¥ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
- ACID íŠ¸ëœì­ì…˜
- ìŠ¤í‚¤ë§ˆ ì§„í™” (Schema Evolution)
- íƒ€ì„ íŠ¸ë˜ë¸” (Time Travel)
- ìŠ¤ëƒ…ìƒ· ê´€ë¦¬
- Hive Metastoreë¥¼ í†µí•œ í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ê´€ë¦¬

Author: Data Engineering Team
Date: 2025-08-08
"""

import os
import logging
from datetime import datetime
from typing import Optional

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, when, isnan, isnull, regexp_replace, 
    to_timestamp, date_format, year, month, dayofmonth,
    expr, from_json, lit, current_timestamp
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, BooleanType, TimestampType
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class IcebergETLPipeline:
    """Iceberg ê¸°ë°˜ ETL íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        self.database_name = "recipe_analytics"
        
        # S3 ê²½ë¡œ ì„¤ì • - ìš´ì˜ í™˜ê²½ìš© S3 ì‚¬ìš©
        self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/warehouse/"
        self.s3_landing_zone = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
        
        # Hive Metastore ì„¤ì •
        self.hive_metastore_uri = "thrift://metastore:9083"
        
    def create_spark_session(self) -> SparkSession:
        """Icebergì™€ Hive Metastoreë¥¼ ì§€ì›í•˜ëŠ” SparkSession ìƒì„±"""
        
        print("ğŸ§Š Iceberg + Hive Metastore SparkSession ìƒì„± ì¤‘...")
        
        try:
            spark = SparkSession.builder \
                .appName("IcebergETLPipeline") \
                .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
                .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
                .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
                .config("spark.sql.catalog.iceberg_catalog.uri", self.hive_metastore_uri) \
                .config("spark.sql.catalog.iceberg_catalog.warehouse", self.s3_warehouse_path) \
                .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
                .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
                .config("spark.hadoop.fs.s3a.path.style.access", "true") \
                .config("spark.hadoop.fs.s3a.block.size", "134217728") \
                .config("spark.hadoop.fs.s3a.buffer.dir", "/tmp") \
                .config("spark.hadoop.fs.s3a.fast.upload", "true") \
                .config("spark.hadoop.fs.s3a.fast.upload.buffer", "bytebuffer") \
                .config("spark.hadoop.fs.s3a.multipart.size", "67108864") \
                .config("spark.hadoop.fs.s3a.multipart.threshold", "134217728") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .getOrCreate()
            
            # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
            spark.sparkContext.setLogLevel("WARN")
            
            print("âœ… Iceberg SparkSession ìƒì„± ì™„ë£Œ!")
            print(f"ğŸ“ Warehouse ê²½ë¡œ: {self.s3_warehouse_path}")
            print(f"ğŸ—„ï¸  Hive Metastore URI: {self.hive_metastore_uri}")
            
            self.spark = spark
            return spark
            
        except Exception as e:
            print(f"âŒ SparkSession ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def create_database_if_not_exists(self):
        """Iceberg ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
        try:
            print(f"ğŸ—ƒï¸  ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±: {self.catalog_name}.{self.database_name}")
            
            self.spark.sql(f"""
                CREATE DATABASE IF NOT EXISTS {self.catalog_name}.{self.database_name}
                COMMENT 'Recipe Analytics Database for Iceberg Tables'
                LOCATION '{self.s3_warehouse_path}{self.database_name}.db/'
            """)
            
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def define_event_schema(self) -> StructType:
        """ì´ë²¤íŠ¸ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜ - ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ì¶¤"""
        return StructType([
            StructField("anonymous_id", StringType(), True),
            StructField("context", StringType(), True),
            StructField("date", StringType(), True),
            StructField("event_id", StringType(), True),
            StructField("event_name", StringType(), True),
            StructField("event_properties", StringType(), True),
            StructField("session_id", StringType(), True),
            StructField("timestamp", StringType(), True),
            StructField("user_id", StringType(), True)
        ])
    
    def read_from_landing_zone(self) -> DataFrame:
        """ëœë”© ì¡´ì—ì„œ ì›ì‹œ ë°ì´í„° ì½ê¸°"""
        try:
            print("ğŸ“‚ ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸° ì‹œì‘...")
            print(f"ğŸ“ ê²½ë¡œ: {self.s3_landing_zone}")
            
            # JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
            schema = self.define_event_schema()
            
            # ëœë”© ì¡´ì—ì„œ JSON íŒŒì¼ ì½ê¸°
            df = self.spark.read \
                .option("multiline", "false") \
                .option("mode", "PERMISSIVE") \
                .option("columnNameOfCorruptRecord", "_corrupt_record") \
                .schema(schema) \
                .json(self.s3_landing_zone)
            
            row_count = df.count()
            print(f"âœ… ëœë”© ì¡´ ë°ì´í„° ë¡œë“œ ì„±ê³µ! í–‰ ìˆ˜: {row_count:,}")
            
            return df
            
        except Exception as e:
            print(f"âŒ ëœë”© ì¡´ ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            raise
    
    def clean_and_transform_data(self, df: DataFrame) -> DataFrame:
        """ë°ì´í„° ì •ì œ ë° ë³€í™˜"""
        try:
            print("ğŸ§¹ ë°ì´í„° ì •ì œ ë° ë³€í™˜ ì‹œì‘...")
            
            # 1. event_nameì„ event_typeìœ¼ë¡œ ë¦¬ë„¤ì„
            df_cleaned = df.withColumnRenamed("event_name", "event_type")
            
            # 2. íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ (ISO 8601 í˜•ì‹ ì²˜ë¦¬)
            df_cleaned = df_cleaned.withColumn(
                "event_timestamp",
                to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSSSSXXX")
            )
            
            # 3. íŒŒí‹°ì…”ë‹ì„ ìœ„í•œ ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
            df_cleaned = df_cleaned \
                .withColumn("event_year", year(col("event_timestamp"))) \
                .withColumn("event_month", month(col("event_timestamp"))) \
                .withColumn("event_day", dayofmonth(col("event_timestamp")))
            
            # 4. ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì •ì œ (ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
            df_cleaned = df_cleaned \
                .filter(col("user_id").isNotNull()) \
                .filter(col("event_type").isNotNull()) \
                .filter(col("event_timestamp").isNotNull())
            
            # 5. JSON ë¬¸ìì—´ì—ì„œ ì£¼ìš” ì •ë³´ ì¶”ì¶œ (event_propertiesì—ì„œ)
            df_cleaned = df_cleaned \
                .withColumn("recipe_id", expr("get_json_object(event_properties, '$.recipe_id')")) \
                .withColumn("list_type", expr("get_json_object(event_properties, '$.list_type')")) \
                .withColumn("comment_length", expr("get_json_object(event_properties, '$.comment_length')")) \
                .withColumn("rank", expr("get_json_object(event_properties, '$.rank')"))
            
            # 6. context JSONì—ì„œ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
            df_cleaned = df_cleaned \
                .withColumn("page_name", expr("get_json_object(context, '$.page.name')")) \
                .withColumn("page_url", expr("get_json_object(context, '$.page.url')")) \
                .withColumn("user_segment", expr("get_json_object(context, '$.user_segment')")) \
                .withColumn("cooking_style", expr("get_json_object(context, '$.cooking_style')"))
            
            # 7. ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            df_cleaned = df_cleaned \
                .withColumn("processed_at", current_timestamp()) \
                .withColumn("data_source", lit("landing_zone")) \
                .withColumn("pipeline_version", lit("iceberg_v1.0"))
            
            # 8. ìµœì¢… ì»¬ëŸ¼ ì„ íƒ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë“¤ë¡œ)
            final_columns = [
                "user_id", "session_id", "event_type", "event_timestamp", "event_id",
                "anonymous_id", "date", "recipe_id", "list_type", "comment_length", "rank",
                "page_name", "page_url", "user_segment", "cooking_style",
                "event_properties", "context",
                "event_year", "event_month", "event_day",
                "processed_at", "data_source", "pipeline_version"
            ]
            
            df_final = df_cleaned.select(*final_columns)
            
            clean_count = df_final.count()
            print(f"âœ… ë°ì´í„° ì •ì œ ì™„ë£Œ! ì •ì œëœ í–‰ ìˆ˜: {clean_count:,}")
            
            return df_final
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì •ì œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def create_iceberg_table_if_not_exists(self):
        """Iceberg í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)"""
        try:
            table_name = f"{self.catalog_name}.{self.database_name}.user_events_silver"
            
            print(f"ğŸ§Š Iceberg í…Œì´ë¸” ìƒì„±: {table_name}")
            
            create_table_sql = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                user_id STRING,
                session_id STRING,
                event_type STRING,
                event_timestamp TIMESTAMP,
                event_id STRING,
                anonymous_id STRING,
                date STRING,
                recipe_id STRING,
                list_type STRING,
                comment_length STRING,
                rank STRING,
                page_name STRING,
                page_url STRING,
                user_segment STRING,
                cooking_style STRING,
                event_properties STRING,
                context STRING,
                event_year INT,
                event_month INT,
                event_day INT,
                processed_at TIMESTAMP,
                data_source STRING,
                pipeline_version STRING
            )
            USING ICEBERG
            PARTITIONED BY (event_year, event_month, event_day)
            TBLPROPERTIES (
                'write.distribution-mode' = 'hash',
                'write.upsert.enabled' = 'true',
                'format-version' = '2'
            )
            """
            
            self.spark.sql(create_table_sql)
            print(f"âœ… Iceberg í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ Iceberg í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def write_to_iceberg_table(self, df: DataFrame):
        """Iceberg í…Œì´ë¸”ì— ë°ì´í„° ì“°ê¸°"""
        try:
            table_name = f"{self.catalog_name}.{self.database_name}.user_events_silver"
            
            print(f"ğŸ§Š Iceberg í…Œì´ë¸”ì— ë°ì´í„° ì“°ê¸°: {table_name}")
            
            # Iceberg í…Œì´ë¸”ì— append ëª¨ë“œë¡œ ì“°ê¸°
            df.writeTo(table_name) \
                .option("write-audit-publish", "true") \
                .append()
            
            print(f"âœ… Iceberg í…Œì´ë¸” ì“°ê¸° ì™„ë£Œ!")
            
            # í…Œì´ë¸” ì •ë³´ í™•ì¸
            self.show_table_info(table_name)
            
        except Exception as e:
            print(f"âŒ Iceberg í…Œì´ë¸” ì“°ê¸° ì‹¤íŒ¨: {str(e)}")
            raise
    
    def show_table_info(self, table_name: str):
        """Iceberg í…Œì´ë¸” ì •ë³´ í‘œì‹œ"""
        try:
            print(f"\nğŸ“Š Iceberg í…Œì´ë¸” ì •ë³´: {table_name}")
            
            # í…Œì´ë¸” í–‰ ìˆ˜ í™•ì¸
            count_df = self.spark.sql(f"SELECT COUNT(*) as total_rows FROM {table_name}")
            total_rows = count_df.collect()[0]['total_rows']
            print(f"ğŸ“ˆ ì´ í–‰ ìˆ˜: {total_rows:,}")
            
            # ìŠ¤ëƒ…ìƒ· ì •ë³´ í™•ì¸
            snapshots_df = self.spark.sql(f"SELECT * FROM {table_name}.snapshots ORDER BY committed_at DESC LIMIT 5")
            print(f"\nğŸ“¸ ìµœê·¼ ìŠ¤ëƒ…ìƒ· (ìµœëŒ€ 5ê°œ):")
            snapshots_df.show(truncate=False)
            
            # íŒŒí‹°ì…˜ ì •ë³´ í™•ì¸
            partitions_df = self.spark.sql(f"SELECT event_year, event_month, event_day, COUNT(*) as row_count FROM {table_name} GROUP BY event_year, event_month, event_day ORDER BY event_year DESC, event_month DESC, event_day DESC")
            print(f"\nğŸ“… íŒŒí‹°ì…˜ë³„ ë°ì´í„° ë¶„í¬:")
            partitions_df.show()
            
        except Exception as e:
            print(f"âš ï¸  í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def run_pipeline(self):
        """ì „ì²´ ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            print("ğŸš€ Iceberg ETL íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
            print("=" * 60)
            
            # 1. SparkSession ìƒì„±
            self.create_spark_session()
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            self.create_database_if_not_exists()
            
            # 3. ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°
            raw_df = self.read_from_landing_zone()
            
            # 4. ë°ì´í„° ì •ì œ ë° ë³€í™˜
            clean_df = self.clean_and_transform_data(raw_df)
            
            # 5. Iceberg í…Œì´ë¸” ìƒì„±
            self.create_iceberg_table_if_not_exists()
            
            # 6. Iceberg í…Œì´ë¸”ì— ë°ì´í„° ì“°ê¸°
            self.write_to_iceberg_table(clean_df)
            
            print("\n" + "=" * 60)
            print("ğŸ‰ Iceberg ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            raise
        finally:
            if self.spark:
                print("ğŸ”š SparkSession ì¢…ë£Œ")
                self.spark.stop()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    pipeline = IcebergETLPipeline()
    pipeline.run_pipeline()

if __name__ == "__main__":
    main()
