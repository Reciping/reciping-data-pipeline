#!/usr/bin/env python3
"""
ğŸ§Š Iceberg + Hive Metastore ê¸°ë°˜ Bronze to Silver ETL Pipeline (Stateless & Idempotent)
=======================================================================================
Bronze Iceberg í…Œì´ë¸”ì—ì„œ ì‹ ê·œ ë°ì´í„°ë¥¼ ì½ì–´ Silver Iceberg í…Œì´ë¸”ë¡œ ë³€í™˜/ì •ì œí•©ë‹ˆë‹¤.
Airflowë¡œë¶€í„° data_interval_startë¥¼ ë°›ì•„ ì²˜ë¦¬í•  íŒŒí‹°ì…˜ì„ ë™ì ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.
"""
import logging
import argparse
from datetime import datetime
import pytz
from dateutil.parser import isoparse
from typing import Optional 

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, from_json, current_timestamp, lit,
    year, month, dayofmonth, hour, date_format, expr, to_date, coalesce, to_timestamp
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType,
    DoubleType, BooleanType, TimestampType, DateType, LongType, ArrayType
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BronzeToSilverETL:
    """Bronze Iceberg Tableì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ Silver Tableë¡œ ë³€í™˜í•˜ëŠ” ETL íŒŒì´í”„ë¼ì¸"""

    def __init__(self, test_mode: bool = True):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        self.hive_metastore_uri = "thrift://10.0.11.86:9083" # ìì‹ ì˜ Hive Metastore URIë¡œ ë³€ê²½

        if test_mode:
            print("=== í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
            self.database_name = "recipe_analytics_test"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/test_warehouse/"
            self.table_suffix = "_test"
        else:
            print("=== ìš´ì˜ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
            self.database_name = "recipe_analytics"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/warehouse/"
            self.table_suffix = ""

        self.bronze_table_name = f"{self.catalog_name}.{self.database_name}.bronze_events_iceberg{self.table_suffix}"
        self.silver_table_name = f"{self.catalog_name}.{self.database_name}.user_events_silver{self.table_suffix}"

        print(f"ë°ì´í„°ë² ì´ìŠ¤: {self.database_name}")
        print(f"ì…ë ¥(Bronze) í…Œì´ë¸”: {self.bronze_table_name}")
        print(f"ì¶œë ¥(Silver) í…Œì´ë¸”: {self.silver_table_name}")

    def create_spark_session(self):
        """SparkSession ìƒì„±"""
        print("SparkSession ìƒì„± ì¤‘...")
        self.spark = SparkSession.builder \
            .appName("BronzeToSilverETL") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", self.hive_metastore_uri) \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", self.s3_warehouse_path) \
            .getOrCreate()
        self.spark.sparkContext.setLogLevel("WARN")
        print("SparkSession ìƒì„± ì™„ë£Œ")

    def create_silver_table_if_not_exists(self):
        """Silver Iceberg í…Œì´ë¸” ìƒì„± (ê¸°ì¡´ ì½”ë“œì˜ ìŠ¤í‚¤ë§ˆ í™œìš©)"""
        print(f"Silver Iceberg í…Œì´ë¸” ìƒì„± í™•ì¸: {self.silver_table_name}")
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.silver_table_name} (
            event_id STRING, event_name STRING, user_id STRING, anonymous_id STRING, session_id STRING,
            kst_timestamp TIMESTAMP, utc_timestamp TIMESTAMP, date DATE,
            year INT, month INT, day INT, hour INT, day_of_week STRING,
            page_name STRING, page_url STRING, user_segment STRING, cooking_style STRING, ab_test_group STRING,
            prop_recipe_id BIGINT, prop_list_type STRING, prop_action STRING,
            prop_search_keyword STRING, prop_result_count INT,
            processed_at TIMESTAMP, data_source STRING, pipeline_version STRING
        )
        USING ICEBERG
        PARTITIONED BY (year, month, day)
        """
        self.spark.sql(create_table_sql)
        print("Silver Iceberg í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ")

    def read_new_data_from_bronze(self, execution_ts: str = None) -> Optional[DataFrame]:
        """ìˆ˜ì •ëœ ë²„ì „: execution_tsê°€ ì—†ìœ¼ë©´ Bronze í…Œì´ë¸” ì „ì²´ ì½ê¸°"""
        if execution_ts is None:
            print("Bronze í…Œì´ë¸”ì—ì„œ ì „ì²´ ë°ì´í„° ì½ê¸° (ë²Œí¬ ëª¨ë“œ)")
            bronze_df = self.spark.read.table(self.bronze_table_name)
        else:
            print(f"Bronze í…Œì´ë¸”ì—ì„œ íŠ¹ì • íŒŒí‹°ì…˜ ì½ê¸°: {execution_ts}")
            # ê¸°ì¡´ ë¡œì§
            kst_tz = pytz.timezone('Asia/Seoul')
            try:
                dt_obj = datetime.strptime(execution_ts, '%Y-%m-%d %H:%M')
                kst_dt = kst_tz.localize(dt_obj)
            except ValueError:
                utc_dt = isoparse(execution_ts)
                kst_dt = utc_dt.astimezone(kst_tz)
            
            target_date = kst_dt.strftime('%Y-%m-%d')
            bronze_df = self.spark.read.table(self.bronze_table_name).where(f"ingestion_date = '{target_date}'")
        
        if bronze_df.rdd.isEmpty():
            print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        count = bronze_df.count()
        print(f"ì´ {count:,} ê±´ì˜ ë°ì´í„°ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.")
        return bronze_df

    def transform_bronze_to_silver(self, bronze_df: DataFrame) -> DataFrame:
        """Bronze ë°ì´í„°ë¥¼ Silver ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤."""
        print("Bronze to Silver ë°ì´í„° ë³€í™˜ ì‹œì‘...")
        
        # 1. íŒŒì‹±ì— í•„ìš”í•œ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)
        json_event_schema = StructType([
            StructField("anonymous_id", StringType(), True), StructField("context", StringType(), True),
            StructField("date", StringType(), True), StructField("event_id", StringType(), True),
            StructField("event_name", StringType(), True), StructField("event_properties", StringType(), True),
            StructField("session_id", StringType(), True), StructField("timestamp", StringType(), True),
            StructField("user_id", StringType(), True)
        ])
        context_schema = StructType([
            StructField("page", StructType([
                StructField("name", StringType(), True), StructField("url", StringType(), True),
                StructField("path", StringType(), True)
            ]), True),
            StructField("user_segment", StringType(), True), StructField("activity_level", StringType(), True),
            StructField("cooking_style", StringType(), True),
            StructField("ab_test", StructType([
                StructField("scenario", StringType(), True), StructField("group", StringType(), True),
                StructField("start_date", StringType(), True), StructField("end_date", StringType(), True)
            ]), True)
        ])
        event_properties_schema = StructType([
            StructField("page_name", StringType(), True), StructField("referrer", StringType(), True),
            StructField("recipe_id", StringType(), True), StructField("list_type", StringType(), True),
            StructField("action", StringType(), True), StructField("search_keyword", StringType(), True),
            StructField("result_count", IntegerType(), True)
        ])

        # 2. raw_event_string ì»¬ëŸ¼ì„ JSONìœ¼ë¡œ íŒŒì‹±
        parsed_df = bronze_df.withColumn("event_data", from_json(col("raw_event_string"), json_event_schema))

        # 3. íŒŒì‹±ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³€í™˜ ìˆ˜í–‰
        df_transformed = parsed_df \
            .withColumn("parsed_context", from_json(col("event_data.context"), context_schema)) \
            .withColumn("parsed_properties", from_json(col("event_data.event_properties"), event_properties_schema)) \
            .withColumn("raw_timestamp_str", col("event_data.timestamp")) \
            .withColumn("kst_timestamp", 
                # ISO 8601 í˜•ì‹ì˜ timestampë¥¼ ì˜¬ë°”ë¥´ê²Œ íŒŒì‹±
                to_timestamp(col("raw_timestamp_str"), "yyyy-MM-dd'T'HH:mm:ss.SSSXXX")) \
            .withColumn("utc_timestamp", 
                # KSTì—ì„œ UTCë¡œ ë³€í™˜ (9ì‹œê°„ ë¹¼ê¸°)
                expr("kst_timestamp - INTERVAL 9 HOURS")) \
            .withColumn("date", 
                # KST ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ì¶”ì¶œ (ì´ê²Œ í•µì‹¬!)
                to_date(col("kst_timestamp"))) \
            .withColumn("year", year(col("kst_timestamp"))) \
            .withColumn("month", month(col("kst_timestamp"))) \
            .withColumn("day", dayofmonth(col("kst_timestamp"))) \
            .withColumn("hour", hour(col("kst_timestamp"))) \
            .withColumn("day_of_week", date_format(col("kst_timestamp"), "E"))

        # ìµœì¢… ì»¬ëŸ¼ ì„ íƒ
        df_final = df_transformed.select(
            col("event_data.event_id").alias("event_id"),
            col("event_data.event_name").alias("event_name"),
            col("event_data.user_id").alias("user_id"),
            col("event_data.anonymous_id").alias("anonymous_id"),
            col("event_data.session_id").alias("session_id"),
            "kst_timestamp", "utc_timestamp", "date",
            "year", "month", "day", "hour", "day_of_week",
            col("parsed_context.page.name").alias("page_name"),
            col("parsed_context.page.url").alias("page_url"),
            col("parsed_context.user_segment").alias("user_segment"),
            col("parsed_context.cooking_style").alias("cooking_style"),
            col("parsed_context.ab_test.group").alias("ab_test_group"),
            col("parsed_properties.recipe_id").cast(LongType()).alias("prop_recipe_id"),
            col("parsed_properties.list_type").alias("prop_list_type"),
            col("parsed_properties.action").alias("prop_action"),
            col("parsed_properties.search_keyword").alias("prop_search_keyword"),
            col("parsed_properties.result_count").alias("prop_result_count"),
            col("source_file").alias("data_source")
        ) \
        .withColumn("processed_at", current_timestamp()) \
        .withColumn("pipeline_version", lit("bulk_corrected_v1.0")) \
        .dropDuplicates(["event_id"])

        print(f"ë°ì´í„° ë³€í™˜ ì™„ë£Œ. í•„í„°ë§ í›„ ë ˆì½”ë“œ ìˆ˜: {df_final.count():,}")
        return df_final
    
#     def run_pipeline(self, execution_ts: Optional[str] = None, target_date: Optional[str] = None):
#         """ì¸ìì— ë”°ë¼ ì¦ë¶„ ë˜ëŠ” ë²Œí¬ ëª¨ë“œë¡œ Bronze to Silver ETLì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
#         try:
#             print("Bronze to Silver ETL íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            
#             self.create_spark_session()
#             self.create_silver_table_if_not_exists()

#             # === í•µì‹¬ ìˆ˜ì •: ë²Œí¬ ëª¨ë“œ ì²˜ë¦¬ ì¶”ê°€ ===
#             if target_date:
#                 # ì¦ë¶„ ëª¨ë“œ: íŠ¹ì • ë‚ ì§œë§Œ ì²˜ë¦¬
#                 print(f"ì¦ë¶„ ëª¨ë“œë¡œ ì‹¤í–‰ (ëŒ€ìƒ ë‚ ì§œ: {target_date})")
#                 bronze_df = self.spark.read.table(self.bronze_table_name).where(f"ingestion_date = '{target_date}'")
                
#             elif execution_ts:
#                 # ì¦ë¶„ ëª¨ë“œ: Airflowì—ì„œ í˜¸ì¶œ
#                 print(f"ì¦ë¶„ ëª¨ë“œë¡œ ì‹¤í–‰ (ì…ë ¥ ì‹œê°„: {execution_ts})")
#                 kst_tz = pytz.timezone('Asia/Seoul')
#                 try:
#                     dt_obj = datetime.strptime(execution_ts, '%Y-%m-%d %H:%M')
#                     kst_dt = kst_tz.localize(dt_obj)
#                 except ValueError:
#                     utc_dt = isoparse(execution_ts)
#                     kst_dt = utc_dt.astimezone(kst_tz)
#                 target_date_str = kst_dt.strftime('%Y-%m-%d')
#                 bronze_df = self.spark.read.table(self.bronze_table_name).where(f"ingestion_date = '{target_date_str}'")
                
#             else:
#                 # ë²Œí¬ ëª¨ë“œ: ì „ì²´ Bronze ë°ì´í„° ì²˜ë¦¬
#                 print("ë²Œí¬ ëª¨ë“œë¡œ ì‹¤í–‰ (Bronze í…Œì´ë¸” ì „ì²´ ì²˜ë¦¬)")
#                 print("ì£¼ì˜: ingestion_dateì™€ ê´€ê³„ì—†ì´ ëª¨ë“  ë°ì´í„°ë¥¼ ì‹¤ì œ ì´ë²¤íŠ¸ ë‚ ì§œë³„ë¡œ ì¬íŒŒí‹°ì…”ë‹í•©ë‹ˆë‹¤.")
#                 bronze_df = self.spark.read.table(self.bronze_table_name)
            
#             if bronze_df.rdd.isEmpty():
#                 print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
#                 return
            
#             count = bronze_df.count()
#             print(f"ì´ {count:,} ê±´ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            
#             # ë°ì´í„° ë³€í™˜ ë° ì €ì¥ (ì‹¤ì œ ì´ë²¤íŠ¸ ë‚ ì§œë³„ë¡œ íŒŒí‹°ì…”ë‹ë¨)
#             silver_data = self.transform_bronze_to_silver(bronze_df)
#             silver_data.writeTo(self.silver_table_name).append()
            
#             print("ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
            
#         except Exception as e:
#             logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨", exc_info=True)
#             raise
#         finally:
#             if self.spark:
#                 self.spark.stop()

# def main():
#     parser = argparse.ArgumentParser(description="Unified Bronze to Silver ETL Job")
#     # ë‘ ì¸ì ëª¨ë‘ ë°›ë˜, í•„ìˆ˜ëŠ” ì•„ë‹ˆë„ë¡ ì„¤ì •
#     parser.add_argument("--execution-ts", required=False, help="For incremental runs")
#     parser.add_argument("--target-date", required=False, help="For bulk runs (YYYY-MM-DD)")
#     parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True)
#     args = parser.parse_args()

#     pipeline = BronzeToSilverETL(test_mode=args.test_mode)
#     pipeline.run_pipeline(execution_ts=args.execution_ts, target_date=args.target_date)


    def run_pipeline(self, data_interval_start: Optional[str] = None, data_interval_end: Optional[str] = None):
        """
        [ìˆ˜ì •ë¨] data_interval_startë¥¼ ê¸°ë°˜ìœ¼ë¡œ Bronze í…Œì´ë¸”ì˜ íŠ¹ì • íŒŒí‹°ì…˜ì„ ì½ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        try:
            self.create_spark_session()
            self.create_silver_table_if_not_exists()

            if not data_interval_start:
                raise ValueError("ì¦ë¶„ ì²˜ë¦¬ë¥¼ ìœ„í•´ --data-interval-start ì¸ìê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.")

            # === ì¦ë¶„ ì²˜ë¦¬ ëª¨ë“œ: Airflowê°€ ì „ë‹¬í•œ ì‹œê°„ êµ¬ê°„ì„ ëª…í™•íˆ ì‚¬ìš© ===
            print(f"ì¦ë¶„ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰: {data_interval_start} ~ {data_interval_end}")
            
            # 1. data_interval_start(UTC)ë¥¼ KST ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ 'YYYY-MM-DD' ë‚ ì§œ íšë“
            start_time_utc = isoparse(data_interval_start)
            kst_tz = pytz.timezone('Asia/Seoul')
            start_time_kst = start_time_utc.astimezone(kst_tz)
            target_date_str = start_time_kst.strftime('%Y-%m-%d')
            
            print(f"Bronze í…Œì´ë¸”ì˜ ì²˜ë¦¬ ëŒ€ìƒ íŒŒí‹°ì…˜ ë‚ ì§œ: {target_date_str}")
            
            # 2. Bronze í…Œì´ë¸”ì—ì„œ í•´ë‹¹ ë‚ ì§œ íŒŒí‹°ì…˜ë§Œ ì •í™•íˆ ì½ì–´ì˜¤ê¸°
            source_bronze_df = self.spark.read.table(self.bronze_table_name).where(
                f"ingestion_date = '{target_date_str}'"
            )
            
            if source_bronze_df.rdd.isEmpty():
                print("ì²˜ë¦¬í•  Bronze ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return

            # --- ê³µí†µ ì‹¤í–‰ ë¡œì§ ---
            print(f"ì´ {source_bronze_df.count():,} ê±´ì˜ Bronze ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            silver_data = self.transform_bronze_to_silver(source_bronze_df)
            silver_data.createOrReplaceTempView("silver_updates")
            
            # MERGE INTOëŠ” ë©±ë“±ì„±ì„ ë³´ì¥í•˜ëŠ” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.
            merge_sql = f"""
            MERGE INTO {self.silver_table_name} t
            USING silver_updates s
            ON t.event_id = s.event_id
            WHEN MATCHED THEN UPDATE SET *
            WHEN NOT MATCHED THEN INSERT *
            """
            print("Silver Iceberg í…Œì´ë¸”ì— MERGE ì‹¤í–‰...")
            self.spark.sql(merge_sql)
            print("Bronze to Silver ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
                
        except Exception as e:
            logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨", exc_info=True)
            raise
        finally:
            if self.spark:
                self.spark.stop()


def main():
    parser = argparse.ArgumentParser(description="Stateless Bronze to Silver ETL Job")
    parser.add_argument("--data-interval-start", required=True)
    parser.add_argument("--data-interval-end", required=True)
    parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True)
    args = parser.parse_args()

    pipeline = BronzeToSilverETL(test_mode=args.test_mode)
    pipeline.run_pipeline(
        data_interval_start=args.data_interval_start, 
        data_interval_end=args.data_interval_end
    )

if __name__ == "__main__":
    main()