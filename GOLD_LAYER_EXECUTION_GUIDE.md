# Gold Layer Star Schema ì‹¤í–‰ ê°€ì´ë“œ

## ğŸŒŸ **Gold Layerì˜ ì—­í• ê³¼ ëª©ì **

ë„¤, ì •í™•íˆ ë§ìŠµë‹ˆë‹¤! **Gold Layer**ëŠ” ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤:

### **Gold Layer = ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤ & ë¶„ì„ ë ˆì´ì–´**
- âœ… **Star Schema êµ¬ì¶•**: ì°¨ì› ëª¨ë¸ë§ìœ¼ë¡œ ë¶„ì„ ìµœì í™”
- âœ… **í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ**: DAU, WAU, Retention, Conversion ë“±
- âœ… **ì§‘ê³„ëœ ë°ì´í„°**: ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì„ìš© ë°ì´í„°
- âœ… **ëŒ€ì‹œë³´ë“œ í”¼ë“œ**: BI ë„êµ¬(Tableau, PowerBI)ì™€ ì§ì ‘ ì—°ê²°

---

## ğŸ—ï¸ **êµ¬ì¶•ëœ Star Schema ì•„í‚¤í…ì²˜**

```
ğŸŒŸ Star Schema Gold Layer
â”œâ”€â”€ ğŸ“Š Fact Table
â”‚   â””â”€â”€ fact_user_events (ì¤‘ì‹¬ í…Œì´ë¸”)
â”‚
â”œâ”€â”€ ğŸ¯ Dimension Tables  
â”‚   â”œâ”€â”€ dim_users (ì‚¬ìš©ì ì°¨ì›)
â”‚   â”œâ”€â”€ dim_time (ì‹œê°„ ì°¨ì›) 
â”‚   â”œâ”€â”€ dim_recipes (ë ˆì‹œí”¼ ì°¨ì›)
â”‚   â”œâ”€â”€ dim_pages (í˜ì´ì§€ ì°¨ì›)
â”‚   â””â”€â”€ dim_events (ì´ë²¤íŠ¸ ì°¨ì›)
â”‚
â””â”€â”€ ğŸ“ˆ Business Metrics
    â”œâ”€â”€ metrics_dau (ì¼ì¼ í™œì„± ì‚¬ìš©ì)
    â”œâ”€â”€ metrics_retention (ì£¼ê°„ ë¦¬í…ì…˜)
    â””â”€â”€ metrics_recipe_performance (ë ˆì‹œí”¼ ì„±ê³¼)
```

---

## ğŸš€ **ì‹¤í–‰ ë°©ë²•**

### **1. Docker í™˜ê²½ì—ì„œ ì‹¤í–‰**
```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd c:\Users\aryij\Documents\DataStudy\reciping-data-pipeline

# Docker í™˜ê²½ ì‹œì‘ (ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ìƒëµ)
docker-compose up -d

# Gold Layer Star Schema íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
docker-compose exec spark-dev python /app/gold_layer_star_schema.py
```

### **2. ì‹¤í–‰ ê²°ê³¼ í™•ì¸**
```bash
# ìƒì„±ëœ í…Œì´ë¸” í™•ì¸
docker-compose exec spark-dev python -c "
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \
    .config('spark.sql.catalog.iceberg_catalog', 'org.apache.iceberg.spark.SparkCatalog') \
    .config('spark.sql.catalog.iceberg_catalog.type', 'hive') \
    .config('spark.sql.catalog.iceberg_catalog.uri', 'thrift://metastore:9083') \
    .getOrCreate()

print('=== Gold Layer í…Œì´ë¸” ëª©ë¡ ===')
spark.sql('SHOW TABLES IN iceberg_catalog.recipe_analytics').show()

print('=== DAU ë©”íŠ¸ë¦­ ìƒ˜í”Œ ===')
spark.sql('SELECT * FROM iceberg_catalog.recipe_analytics.metrics_dau ORDER BY date DESC LIMIT 5').show()

print('=== Weekly Retention ìƒ˜í”Œ ===')
spark.sql('SELECT cohort_week, retention_week, cohort_size, retained_users, retention_rate FROM iceberg_catalog.recipe_analytics.metrics_retention ORDER BY cohort_week DESC, retention_week LIMIT 10').show()

spark.stop()
"
```

---

## ğŸ“Š **ìƒì„±ë˜ëŠ” í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ**

### **1. DAU (Daily Active Users)**
```sql
SELECT 
    date,
    dau,
    new_users,
    returning_users,
    dau_growth_rate,
    dau_7d_avg
FROM iceberg_catalog.recipe_analytics.metrics_dau
ORDER BY date DESC;
```

**ì˜ˆìƒ ê²°ê³¼:**
```
+----------+----+---------+---------------+---------------+----------+
|      date| dau|new_users|returning_users|dau_growth_rate|dau_7d_avg|
+----------+----+---------+---------------+---------------+----------+
|2025-07-31|8543|      156|           8387|           2.3%|   8234.2 |
|2025-07-30|8341|      201|           8140|           1.8%|   8156.7 |
|2025-07-29|8194|      189|           8005|          -0.5%|   8089.1 |
+----------+----+---------+---------------+---------------+----------+
```

### **2. Weekly Retention**
```sql
SELECT 
    cohort_week,
    retention_week,
    cohort_size,
    retained_users,
    retention_rate
FROM iceberg_catalog.recipe_analytics.metrics_retention
WHERE cohort_week = '2025-07-28'
ORDER BY retention_week;
```

**ì˜ˆìƒ ê²°ê³¼:**
```
+-----------+--------------+-----------+--------------+--------------+
|cohort_week|retention_week|cohort_size|retained_users|retention_rate|
+-----------+--------------+-----------+--------------+--------------+
| 2025-07-28|             0|       1245|          1245|        100.00|
| 2025-07-28|             1|       1245|           856|         68.75|
| 2025-07-28|             2|       1245|           623|         50.04|
| 2025-07-28|             3|       1245|           467|         37.51|
+-----------+--------------+-----------+--------------+--------------+
```

### **3. Recipe Performance**
```sql
SELECT 
    recipe_id,
    total_views,
    unique_viewers,
    engagement_score,
    trending_score
FROM iceberg_catalog.recipe_analytics.metrics_recipe_performance
WHERE date = CURRENT_DATE()
ORDER BY total_views DESC
LIMIT 10;
```

---

## ğŸ¯ **ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜**

### **ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•œ ë¶„ì„**
1. **ğŸ“ˆ ì„±ì¥ ì§€í‘œ**
   - DAU/WAU/MAU íŠ¸ë Œë“œ
   - ì‚¬ìš©ì ì¦ê°€ìœ¨
   - ì„¸ê·¸ë¨¼íŠ¸ë³„ ì„±ì¥

2. **ğŸ”„ ë¦¬í…ì…˜ ë¶„ì„** 
   - ì½”í˜¸íŠ¸ë³„ ë¦¬í…ì…˜ ì»¤ë¸Œ
   - ì°¨ìš°ë§ íŒ¨í„´ ë¶„ì„
   - ì‚¬ìš©ì ë¼ì´í”„ì‚¬ì´í´

3. **ğŸ³ ì½˜í…ì¸  ì„±ê³¼**
   - ì¸ê¸° ë ˆì‹œí”¼ ìˆœìœ„
   - ì½˜í…ì¸  ì°¸ì—¬ë„
   - ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ í”¼ë“œë°±

### **BI ë„êµ¬ ì—°ê²° ì¤€ë¹„**
```python
# Tableau/PowerBI ì—°ê²°ìš© ì¿¼ë¦¬ ì˜ˆì‹œ
business_dashboard_query = """
SELECT 
    d.full_date,
    d.year,
    d.month,
    d.day_name,
    d.is_weekend,
    
    du.user_segment,
    du.cooking_style,
    du.user_tier,
    
    de.event_category,
    de.is_conversion_event,
    
    COUNT(*) as total_events,
    COUNT(DISTINCT f.user_dim_key) as unique_users,
    SUM(f.conversion_value) as total_conversion_value,
    AVG(f.engagement_score) as avg_engagement
    
FROM iceberg_catalog.recipe_analytics.fact_user_events f
JOIN iceberg_catalog.recipe_analytics.dim_time d ON f.time_dim_key = d.time_dim_key  
JOIN iceberg_catalog.recipe_analytics.dim_users du ON f.user_dim_key = du.user_dim_key
JOIN iceberg_catalog.recipe_analytics.dim_events de ON f.event_dim_key = de.event_dim_key

WHERE d.full_date >= CURRENT_DATE() - INTERVAL 30 DAYS
GROUP BY 1,2,3,4,5,6,7,8,9,10
ORDER BY d.full_date DESC
"""
```

---

## ğŸ”„ **ì •ê¸° ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ë§**

### **ì¼ì¼ ë°°ì¹˜ ì‹¤í–‰**
```bash
# cron job ì„¤ì • ì˜ˆì‹œ (ë§¤ì¼ ì˜¤ì „ 6ì‹œ)
0 6 * * * cd /path/to/reciping-data-pipeline && docker-compose exec -T spark-dev python /app/gold_layer_star_schema.py
```

### **ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œ**
```python
# ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ì²˜ë¦¬í•˜ëŠ” ì¦ë¶„ ì—…ë°ì´íŠ¸
def incremental_update():
    # ì–´ì œ ë‚ ì§œ ë°ì´í„°ë§Œ ì²˜ë¦¬
    yesterday = datetime.now() - timedelta(days=1)
    
    # DAU ì¦ë¶„ ê³„ì‚°
    spark.sql(f"""
        INSERT INTO iceberg_catalog.recipe_analytics.metrics_dau
        SELECT ... 
        FROM iceberg_catalog.recipe_analytics.user_events_silver
        WHERE date = '{yesterday.strftime('%Y-%m-%d')}'
    """)
```

---

## ğŸŠ **ê²°ë¡ : Gold Layerì˜ í•µì‹¬ ê°€ì¹˜**

**ì •í™•íˆ ë§ì”€í•˜ì‹  ëŒ€ë¡œ**, Gold LayerëŠ”:

1. **âœ… Star Schema ê¸°ë°˜ ì°¨ì› ëª¨ë¸ë§**
2. **âœ… DAU, Weekly Retention ë“± í•µì‹¬ ì§€í‘œ ìƒì„±**  
3. **âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€ê°€ ì¦‰ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ì œê³µ**
4. **âœ… BI ë„êµ¬ì™€ ì§ì ‘ ì—°ê²°ë˜ëŠ” ë¶„ì„ìš© ë°ì´í„° ë§ˆíŠ¸**

ì´ê²ƒì´ ë°”ë¡œ **ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ Gold Layerì˜ ë³¸ì§ˆ**ì…ë‹ˆë‹¤!

**ğŸ¯ ë‹¤ìŒ ë‹¨ê³„**: ìœ„ì˜ `gold_layer_star_schema.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ ì™„ì „í•œ Star Schemaì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œë¥¼ êµ¬ì¶•í•´ë³´ì„¸ìš”!
