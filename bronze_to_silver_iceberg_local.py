# bronze_to_silver_iceberg_local.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, year, month, dayofmonth, hour, date_format, to_timestamp, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType, DateType, TimestampType

def main():
    """
    ë¡œì»¬ í™˜ê²½ì—ì„œ Iceberg ê³ ê¸‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ”
    ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ ETL íŒŒì´í”„ë¼ì¸ (ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ì‚¬ìš©).
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì • (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
        import os
        import subprocess
        
        # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['HADOOP_USER_NAME'] = 'spark'
        os.environ['USER'] = 'spark'
        os.environ['HOME'] = '/tmp'
        os.environ['LOGNAME'] = 'spark'
        os.environ['USERNAME'] = 'spark'
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('/tmp/.ivy2', exist_ok=True)
        os.makedirs('/tmp/warehouse/iceberg', exist_ok=True)

        print("ğŸ¯ ë¡œì»¬ Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        # -----------------------------------------------------------------------------
        # 1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„± (ë¡œì»¬ Iceberg + ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ)
        # -----------------------------------------------------------------------------
        print("ğŸ”§ SparkSession with Iceberg (ë¡œì»¬ í…ŒìŠ¤íŠ¸) ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        
        spark = SparkSession.builder \
            .appName("Iceberg_Local_Advanced_Features_Test") \
            .master("local[2]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hadoop") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "file:///tmp/warehouse/iceberg") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.driver.maxResultSize", "1g") \
            .config("spark.sql.shuffle.partitions", "4") \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSession with Iceberg (ë¡œì»¬ í…ŒìŠ¤íŠ¸)ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # -----------------------------------------------------------------------------
        # 2. ğŸ—ï¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ë° ê´€ë¦¬
        # -----------------------------------------------------------------------------
        print("\nğŸ—ï¸ Iceberg ë„¤ì„ìŠ¤í˜ì´ìŠ¤ êµ¬ì„±...")
        
        # Bronze, Silver, Gold ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
        try:
            spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.bronze")
            spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.silver") 
            spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.gold")
            print("âœ… Iceberg ë„¤ì„ìŠ¤í˜ì´ìŠ¤ êµ¬ì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}")
        
        # í˜„ì¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª©ë¡ í™•ì¸
        print("\nğŸ“‹ í˜„ì¬ Iceberg ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª©ë¡:")
        spark.sql("SHOW NAMESPACES IN iceberg_catalog").show()

        # -----------------------------------------------------------------------------
        # 3. ğŸ§ª ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        # -----------------------------------------------------------------------------
        print("\nğŸ§ª Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±...")
        
        # ìƒ˜í”Œ ì´ë²¤íŠ¸ ë°ì´í„° ìƒì„±
        sample_data = [
            ("evt001", "page_view", "user001", "anon001", "session001", "2024-01-15 10:30:00", "2024-01-15", 2024, 1, 15, 10),
            ("evt002", "recipe_search", "user001", "anon001", "session001", "2024-01-15 10:31:00", "2024-01-15", 2024, 1, 15, 10),
            ("evt003", "recipe_view", "user001", "anon001", "session001", "2024-01-15 10:32:00", "2024-01-15", 2024, 1, 15, 10),
            ("evt004", "page_view", "user002", "anon002", "session002", "2024-01-15 11:30:00", "2024-01-15", 2024, 1, 15, 11),
            ("evt005", "recipe_search", "user002", "anon002", "session002", "2024-01-15 11:31:00", "2024-01-15", 2024, 1, 15, 11),
            ("evt006", "recipe_bookmark", "user002", "anon002", "session002", "2024-01-15 11:32:00", "2024-01-15", 2024, 1, 15, 11),
            ("evt007", "page_view", "user003", "anon003", "session003", "2024-01-16 09:15:00", "2024-01-16", 2024, 1, 16, 9),
            ("evt008", "recipe_view", "user003", "anon003", "session003", "2024-01-16 09:16:00", "2024-01-16", 2024, 1, 16, 9),
            ("evt009", "comment_write", "user003", "anon003", "session003", "2024-01-16 09:17:00", "2024-01-16", 2024, 1, 16, 9),
            ("evt010", "recipe_share", "user001", "anon001", "session004", "2024-01-16 14:20:00", "2024-01-16", 2024, 1, 16, 14)
        ]
        
        columns = ["event_id", "event_name", "user_id", "anonymous_id", "session_id", 
                  "event_timestamp", "event_date", "year", "month", "day", "hour"]
        
        df_sample = spark.createDataFrame(sample_data, columns) \
            .withColumn("event_timestamp", to_timestamp(col("event_timestamp"))) \
            .withColumn("event_date", col("event_date").cast(DateType())) \
            .withColumn("ingestion_timestamp", current_timestamp())
        
        print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {df_sample.count()}í–‰")
        df_sample.show(truncate=False)

        # -----------------------------------------------------------------------------
        # 4. ğŸ¥‰ Bronze Iceberg í…Œì´ë¸” ìƒì„±
        # -----------------------------------------------------------------------------
        print("\nğŸ¥‰ Bronze Iceberg í…Œì´ë¸” ìƒì„±...")
        
        try:
            # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
            spark.sql("DROP TABLE IF EXISTS iceberg_catalog.bronze.events_raw")
            
            df_sample.writeTo("iceberg_catalog.bronze.events_raw") \
                .tableProperty("format-version", "2") \
                .tableProperty("write.target-file-size-bytes", "67108864") \
                .create()
            
            print("âœ… Bronze Iceberg í…Œì´ë¸” 'iceberg_catalog.bronze.events_raw' ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Bronze í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            return

        # -----------------------------------------------------------------------------
        # 5. ğŸ¥ˆ Silver Iceberg í…Œì´ë¸” ìƒì„± (íŒŒí‹°ì…˜ ì ìš©)
        # -----------------------------------------------------------------------------
        print("\nğŸ¥ˆ Silver Iceberg í…Œì´ë¸” ìƒì„± (íŒŒí‹°ì…˜ ì ìš©)...")
        
        # Silver ë°ì´í„° ë³€í™˜
        df_silver = df_sample.select(
            "event_id", "event_name", "user_id", "anonymous_id", "session_id",
            "event_timestamp", "event_date", "ingestion_timestamp",
            "year", "month", "day", "hour"
        ).filter(col("event_id").isNotNull())
        
        try:
            # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
            spark.sql("DROP TABLE IF EXISTS iceberg_catalog.silver.events_clean")
            
            df_silver.writeTo("iceberg_catalog.silver.events_clean") \
                .partitionedBy("year", "month", "day") \
                .tableProperty("format-version", "2") \
                .tableProperty("write.target-file-size-bytes", "67108864") \
                .create()
            
            print("âœ… Silver Iceberg í…Œì´ë¸” 'iceberg_catalog.silver.events_clean' ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Silver í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            return

        # -----------------------------------------------------------------------------
        # 6. ğŸ“Š Iceberg ê³ ê¸‰ ê¸°ëŠ¥ ì‹œì—°
        # -----------------------------------------------------------------------------
        print("\nğŸ“Š Iceberg ê³ ê¸‰ ê¸°ëŠ¥ ì‹œì—° ì‹œì‘...")
        
        # 6.1. í…Œì´ë¸” ì •ë³´ ì¡°íšŒ
        print("\nğŸ—ƒï¸ Bronze í…Œì´ë¸” ì •ë³´:")
        try:
            bronze_info = spark.sql("DESCRIBE TABLE iceberg_catalog.bronze.events_raw")
            bronze_info.show(10, truncate=False)
        except Exception as e:
            print(f"âš ï¸ Bronze í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        print("\nğŸ—ƒï¸ Silver í…Œì´ë¸” ì •ë³´:")
        try:
            silver_info = spark.sql("DESCRIBE TABLE iceberg_catalog.silver.events_clean")
            silver_info.show(10, truncate=False)
        except Exception as e:
            print(f"âš ï¸ Silver í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # 6.2. ìŠ¤ëƒ…ìƒ· ì´ë ¥ ì¡°íšŒ
        print("\nğŸ“¸ Bronze í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì´ë ¥:")
        try:
            bronze_snapshots = spark.sql("SELECT snapshot_id, committed_at, summary FROM iceberg_catalog.bronze.events_raw.snapshots ORDER BY committed_at DESC")
            bronze_snapshots.show(truncate=False)
        except Exception as e:
            print(f"âš ï¸ Bronze ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        print("\nğŸ“¸ Silver í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì´ë ¥:")
        try:
            silver_snapshots = spark.sql("SELECT snapshot_id, committed_at, summary FROM iceberg_catalog.silver.events_clean.snapshots ORDER BY committed_at DESC")
            silver_snapshots.show(truncate=False)
        except Exception as e:
            print(f"âš ï¸ Silver ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # 6.3. íŒŒì¼ ì •ë³´ í™•ì¸
        print("\nğŸ“‹ Silver í…Œì´ë¸” íŒŒì¼ ì •ë³´:")
        try:
            silver_files = spark.sql("SELECT file_path, file_format, record_count FROM iceberg_catalog.silver.events_clean.files")
            silver_files.show(truncate=False)
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # 6.4. íŒŒí‹°ì…˜ ì •ë³´ í™•ì¸
        print("\nğŸ“ Silver í…Œì´ë¸” íŒŒí‹°ì…˜ ì •ë³´:")
        try:
            silver_partitions = spark.sql("SHOW PARTITIONS iceberg_catalog.silver.events_clean")
            silver_partitions.show(truncate=False)
        except Exception as e:
            print(f"âš ï¸ íŒŒí‹°ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 7. ğŸ”„ Iceberg Time Travel ê¸°ëŠ¥ ë°ëª¨
        # -----------------------------------------------------------------------------
        print("\nğŸ”„ Iceberg Time Travel ê¸°ëŠ¥ ë°ëª¨...")
        
        try:
            # ìµœì‹  ìŠ¤ëƒ…ìƒ· ID ì¡°íšŒ
            snapshots = spark.sql("SELECT snapshot_id, committed_at FROM iceberg_catalog.silver.events_clean.snapshots ORDER BY committed_at DESC LIMIT 1").collect()
            if snapshots:
                snapshot_id = snapshots[0]['snapshot_id']
                committed_at = snapshots[0]['committed_at']
                print(f"ğŸ“¸ ìµœì‹  ìŠ¤ëƒ…ìƒ·: {snapshot_id} (ìƒì„±ì‹œê°„: {committed_at})")
                
                # Time Travel ì¿¼ë¦¬ (íŠ¹ì • ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ)
                time_travel_count = spark.sql(f"""
                    SELECT COUNT(*) as record_count 
                    FROM iceberg_catalog.silver.events_clean 
                    VERSION AS OF {snapshot_id}
                """).collect()[0]['record_count']
                
                print(f"â° Time Travel ì¿¼ë¦¬ ê²°ê³¼: {time_travel_count:,}í–‰")
                
            else:
                print("âš ï¸ ìŠ¤ëƒ…ìƒ·ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âš ï¸ Time Travel ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 8. ğŸ“ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ìŠ¤í‚¤ë§ˆ ì§„í™” í…ŒìŠ¤íŠ¸
        # -----------------------------------------------------------------------------
        print("\nğŸ“ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ìŠ¤í‚¤ë§ˆ ì§„í™” í…ŒìŠ¤íŠ¸...")
        
        try:
            # 8.1. ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
            new_data = [
                ("evt011", "recipe_rating", "user004", "anon004", "session005", "2024-01-17 15:30:00", "2024-01-17", 2024, 1, 17, 15),
                ("evt012", "page_view", "user004", "anon004", "session005", "2024-01-17 15:31:00", "2024-01-17", 2024, 1, 17, 15)
            ]
            
            df_new = spark.createDataFrame(new_data, columns) \
                .withColumn("event_timestamp", to_timestamp(col("event_timestamp"))) \
                .withColumn("event_date", col("event_date").cast(DateType())) \
                .withColumn("ingestion_timestamp", current_timestamp())
            
            # ê¸°ì¡´ í…Œì´ë¸”ì— ìƒˆ ë°ì´í„° ì¶”ê°€
            df_new.writeTo("iceberg_catalog.silver.events_clean").append()
            
            print("âœ… ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
            
            # ì—…ë°ì´íŠ¸ í›„ ë°ì´í„° í™•ì¸
            updated_count = spark.table("iceberg_catalog.silver.events_clean").count()
            print(f"ğŸ“Š ì—…ë°ì´íŠ¸ í›„ ì´ í–‰ ìˆ˜: {updated_count}")
            
            # ìƒˆë¡œìš´ ìŠ¤ëƒ…ìƒ· í™•ì¸
            print("\nğŸ“¸ ì—…ë°ì´íŠ¸ í›„ ìŠ¤ëƒ…ìƒ· ì´ë ¥:")
            latest_snapshots = spark.sql("SELECT snapshot_id, committed_at, summary FROM iceberg_catalog.silver.events_clean.snapshots ORDER BY committed_at DESC LIMIT 3")
            latest_snapshots.show(truncate=False)
            
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 9. ğŸ§¹ í…Œì´ë¸” ìœ ì§€ë³´ìˆ˜ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        # -----------------------------------------------------------------------------
        print("\nğŸ§¹ Iceberg í…Œì´ë¸” ìœ ì§€ë³´ìˆ˜ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        try:
            # 9.1. í…Œì´ë¸” ì†ì„± í™•ì¸
            print("\nâš™ï¸ Silver í…Œì´ë¸” ì†ì„±:")
            table_props = spark.sql("SHOW TBLPROPERTIES iceberg_catalog.silver.events_clean")
            table_props.show(truncate=False)
            
        except Exception as e:
            print(f"âš ï¸ í…Œì´ë¸” ì†ì„± ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 10. ê²€ì¦ ë° ìš”ì•½
        # -----------------------------------------------------------------------------
        print("\nğŸ“ˆ Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ìš”ì•½:")
        
        # í…Œì´ë¸” ì¹´ìš´íŠ¸
        bronze_count = spark.table("iceberg_catalog.bronze.events_raw").count()
        silver_count = spark.table("iceberg_catalog.silver.events_clean").count()
        
        print(f"ğŸ¥‰ Bronze í…Œì´ë¸” í–‰ ìˆ˜: {bronze_count:,}")
        print(f"ğŸ¥ˆ Silver í…Œì´ë¸” í–‰ ìˆ˜: {silver_count:,}")
        print(f"ğŸ“Š ë„¤ì„ìŠ¤í˜ì´ìŠ¤: iceberg_catalog.bronze, iceberg_catalog.silver, iceberg_catalog.gold")
        print(f"ğŸ—ƒï¸ Bronze í…Œì´ë¸”: iceberg_catalog.bronze.events_raw (Iceberg v2)")
        print(f"ğŸ—ƒï¸ Silver í…Œì´ë¸”: iceberg_catalog.silver.events_clean (Iceberg v2, íŒŒí‹°ì…˜: year/month/day)")
        print(f"ğŸ—ï¸ ì¹´íƒˆë¡œê·¸: Hadoop Catalog (ë¡œì»¬)")
        print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ì†Œ: ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ (/tmp/warehouse/iceberg)")
        
        # êµ¬í˜„ëœ Iceberg ê³ ê¸‰ ê¸°ëŠ¥ë“¤
        print(f"\nğŸ’ ì„±ê³µì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ëœ Iceberg ê³ ê¸‰ ê¸°ëŠ¥:")
        print(f"   âœ… ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê´€ë¦¬: ë‹¤ì¤‘ ë ˆì´ì–´ êµ¬ì„±")
        print(f"   âœ… íŒŒí‹°ì…˜ í…Œì´ë¸”: ë…„/ì›”/ì¼ ê¸°ì¤€ íŒŒí‹°ì…”ë‹")
        print(f"   âœ… ë©”íƒ€ë°ì´í„° í…Œì´ë¸”: ìŠ¤ëƒ…ìƒ·, íŒŒì¼, íŒŒí‹°ì…˜ ì¡°íšŒ")
        print(f"   âœ… Time Travel: íŠ¹ì • ìŠ¤ëƒ…ìƒ· ì‹œì  ë°ì´í„° ì¡°íšŒ")
        print(f"   âœ… ë°ì´í„° ì¶”ê°€: ê¸°ì¡´ í…Œì´ë¸”ì— ìƒˆ ë°ì´í„° append")
        print(f"   âœ… ìŠ¤ëƒ…ìƒ· ì´ë ¥: ë³€ê²½ ì‚¬í•­ ì¶”ì  ë° ê´€ë¦¬")
        print(f"   âœ… í…Œì´ë¸” ì†ì„±: ì„¸ë¶€ ì„¤ì • ë° ìµœì í™” ì˜µì…˜")
        print(f"   ğŸ“ ì¤€ë¹„ ì™„ë£Œ: Schema Evolution, ACID Transactions, Compaction")

        # -----------------------------------------------------------------------------
        # 11. ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        spark.stop()
        print("âœ… Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # ìŠ¤íŒŒí¬ ì„¸ì…˜ì´ ìˆë‹¤ë©´ ì¢…ë£Œ
        try:
            spark.stop()
        except:
            pass

# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
if __name__ == "__main__":
    main()
