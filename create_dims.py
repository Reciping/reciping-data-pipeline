#!/usr/bin/env python3
"""
ğŸ§Š Dimension Tables Creation Script
====================================
Silver í…Œì´ë¸” ë° S3 ë§ˆìŠ¤í„° íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ Gold Layerì˜ ëª¨ë“  Dimension í…Œì´ë¸”ì„ ìƒì„±/ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""
import logging
import argparse
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, monotonically_increasing_id, to_date, year, month, dayofmonth, hour, date_format, lit, when, expr

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DimensionBuilder:
    def __init__(self, test_mode: bool = True):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        self.s3_master_path = "s3a://reciping-user-event-logs/meta-data/"

        if test_mode:
            self.source_database = "recipe_analytics_test"
            self.target_database = "recipe_analytics_test"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/test_warehouse/"
            self.table_suffix = "_test"
        else:
            self.source_database = "recipe_analytics"
            self.target_database = "gold_analytics"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/warehouse/"
            self.table_suffix = ""
            
        self.silver_table_name = f"{self.catalog_name}.{self.source_database}.user_events_silver{self.table_suffix}"
        
    def create_spark_session(self):
        """SparkSession ìƒì„± ë° ì¹´íƒˆë¡œê·¸/DB ì„¤ì •"""
        print("SparkSession ìƒì„± ì¤‘...")
        self.spark = SparkSession.builder \
            .appName("DimensionBuilder") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://10.0.11.86:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", self.s3_warehouse_path) \
            .getOrCreate()
        self.spark.sparkContext.setLogLevel("WARN")

        print(f"í˜„ì¬ ì¹´íƒˆë¡œê·¸ë¥¼ '{self.catalog_name}'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"USE {self.catalog_name}")
        
        print(f"ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ '{self.target_database}' ìƒì„± ë° ì‚¬ìš© ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.target_database}")
        self.spark.sql(f"USE {self.target_database}")
        
        print("SparkSession ìƒì„± ë° ì„¤ì • ì™„ë£Œ")

    def _create_dimension(self, table_name: str, source_df: DataFrame, id_cols: list, surrogate_key: str):
        """Silver í…Œì´ë¸” ê¸°ë°˜ Dimension ìƒì„± í—¬í¼ í•¨ìˆ˜"""
        print(f"Dimension í…Œì´ë¸” ìƒì„±/ì—…ë°ì´íŠ¸ ì¤‘: {table_name}")
        dim_df = source_df.select(*id_cols).where(col(id_cols[0]).isNotNull()).distinct()
        dim_df_with_sk = dim_df.withColumn(surrogate_key, monotonically_increasing_id())
        dim_df_with_sk.write.format("iceberg").mode("overwrite").saveAsTable(table_name)
        print(f"{table_name} ì²˜ë¦¬ ì™„ë£Œ. ì´ {dim_df_with_sk.count():,} ê±´")

    # --- [í•µì‹¬ ìˆ˜ì •] S3ì˜ Parquet ë§ˆìŠ¤í„° íŒŒì¼ì„ ì½ì–´ dim_recipeë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ---
    def _create_dim_recipe_from_master(self):
        table_name = f"dim_recipe{self.table_suffix}"
        master_file_path = f"{self.s3_master_path}total_recipes.parquet"
        
        print(f"Dimension í…Œì´ë¸” ìƒì„±/ì—…ë°ì´íŠ¸ ì¤‘: {table_name} (Source: {master_file_path})")

        # 1. S3ì— ìˆëŠ” ë ˆì‹œí”¼ ë§ˆìŠ¤í„° Parquet íŒŒì¼ ì½ê¸°
        recipe_master_df = self.spark.read.parquet(master_file_path)

        # 2. í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ ë° ì´ë¦„ ë³€ê²½
        dim_recipe_df = recipe_master_df.select(
            col("id").alias("recipe_id"),
            col("name").alias("recipe_name"),
            col("dish_type"),
            col("ingredient_type"),
            col("method_type"),
            col("situation_type"),
            col("difficulty"),
            col("cooking_time")
        )

        # 3. ê³ ìœ  ì‹ë³„ì„ ìœ„í•œ ëŒ€ë¦¬ í‚¤(Surrogate Key) ìƒì„±
        dim_recipe_with_sk = dim_recipe_df.withColumn("recipe_sk", monotonically_increasing_id())

        # 4. ìµœì¢… ìŠ¤í‚¤ë§ˆ ì„ íƒ ë° ì €ì¥
        final_dim_df = dim_recipe_with_sk.select(
            "recipe_sk", "recipe_id", "recipe_name", "dish_type", 
            "ingredient_type", "method_type", "situation_type", "difficulty", "cooking_time"
        )
        
        final_dim_df.write.format("iceberg").mode("overwrite").saveAsTable(table_name)
        print(f"{table_name} ì²˜ë¦¬ ì™„ë£Œ. ì´ {final_dim_df.count():,} ê±´")

    def run(self):
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            self.create_spark_session()
            
            print(f"Silver í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì½ê¸°: {self.silver_table_name}")
            silver_df = self.spark.read.table(self.silver_table_name)
            # ì„±ëŠ¥ì„ ìœ„í•´ í•œë²ˆ ìºì‹±
            silver_df.cache()

            # --- Silver í…Œì´ë¸” ê¸°ë°˜ Dimension í…Œì´ë¸” ìƒì„± ---
            # self._create_dimension(f"dim_user{self.table_suffix}", silver_df, ["user_id", "anonymous_id", "user_segment", "cooking_style"], "user_sk")
            # [ìˆ˜ì • í›„] ab_test_group ì»¬ëŸ¼ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            self._create_dimension(f"dim_user{self.table_suffix}", silver_df, ["user_id", "anonymous_id", "user_segment", "cooking_style", "ab_test_group"], "user_sk")
            self._create_dimension(f"dim_event{self.table_suffix}", silver_df, ["event_name"], "event_sk")
            self._create_dimension(f"dim_event{self.table_suffix}", silver_df, ["event_name"], "event_sk")
            self._create_dimension(f"dim_page{self.table_suffix}", silver_df, ["page_name", "page_url"], "page_sk")

            # --- S3 ë§ˆìŠ¤í„° íŒŒì¼ ê¸°ë°˜ dim_recipe ìƒì„± ---
            self._create_dim_recipe_from_master()
            
           # --- [ìˆ˜ì •] dim_time í…Œì´ë¸” ìƒì„± ë¡œì§ ---
            print(f"Dimension í…Œì´ë¸” ìƒì„±/ì—…ë°ì´íŠ¸ ì¤‘: dim_time{self.table_suffix}")
            time_df = self.spark.sql("""
                SELECT explode(sequence(to_timestamp('2025-01-01 00:00:00'), 
                                       to_timestamp('2026-12-31 23:00:00'), 
                                       interval 1 hour)) as ts
            """)
            
            # ì´ select êµ¬ë¬¸ì— to_date(...)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            dim_time = time_df.select(
                date_format(col("ts"), "yyyyMMddHH").cast("bigint").alias("time_dim_key"),
                col("ts").alias("datetime_kst"),
                to_date(col("ts")).alias("date"), # <-- ì´ ë¼ì¸ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
                year(col("ts")).alias("year"),
                month(col("ts")).alias("month"),
                dayofmonth(col("ts")).alias("day"),
                hour(col("ts")).alias("hour"),
                date_format(col("ts"), "E").alias("day_of_week"),
                when(date_format(col("ts"), "E").isin("Sat", "Sun"), True).otherwise(False).alias("is_weekend")
            )

            dim_time.write.format("iceberg").mode("overwrite").saveAsTable(f"dim_time{self.table_suffix}")
            print(f"dim_time{self.table_suffix} ì²˜ë¦¬ ì™„ë£Œ.")

            silver_df.unpersist() # ìºì‹œ í•´ì œ

        except Exception as e:
            logger.error("Dimension í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨", exc_info=True)
            raise
        finally:
            if self.spark:
                self.spark.stop()

def main():
    parser = argparse.ArgumentParser(description="Create All Dimension Tables for Gold Layer")
    parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True)
    args = parser.parse_args()
    builder = DimensionBuilder(test_mode=args.test_mode)
    builder.run()

if __name__ == "__main__":
    main()