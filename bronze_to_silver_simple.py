# bronze_to_silver_simple.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, year, month, dayofmonth, hour, date_format, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType, DateType, TimestampType

def main():
    """
    S3 ëœë”© ì¡´ì˜ ì›ë³¸ íŒŒì¼ì„ ì½ì–´ Bronze, Silver ë ˆì´ì–´ë¥¼ êµ¬ì¶•í•˜ëŠ”
    ë°ì´í„° ë ˆì´í¬ ETL íŒŒì´í”„ë¼ì¸ (Parquet ê¸°ë°˜).
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì • (ì„±ê³µí•œ ì„¤ì • ì ìš©)
        import os
        import subprocess
        
        # ì„±ê³µí•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['HADOOP_USER_NAME'] = 'root'
        os.environ['USER'] = 'root'
        os.environ['HOME'] = '/tmp'
        os.environ['JAVA_OPTS'] = '-Duser.name=root'
        # Ivy ì„¤ì •
        os.environ['IVY_HOME'] = '/tmp/.ivy2'
        os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.jars.ivy=/tmp/.ivy2 --conf spark.jars.packages= pyspark-shell'
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('/tmp/.ivy2', exist_ok=True)

        # -----------------------------------------------------------------------------
        # 1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„± (ë‹¨ìˆœí™”ëœ ì„¤ì • + S3)
        # -----------------------------------------------------------------------------
        print("ğŸ”§ SparkSession ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        
        spark = SparkSession.builder \
            .appName("Bronze_to_Silver_Simple_Pipeline") \
            .master("local[*]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "false") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.jars.ivy", "/tmp/.ivy2") \
            .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "")) \
            .config("spark.hadoop.fs.s3a.region", "ap-northeast-2") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.path.style.access", "false") \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSessionì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # -----------------------------------------------------------------------------
        # 2. ğŸ¥‰ Bronze Layer êµ¬ì¶• - S3ì—ì„œ ë°ì´í„° ì½ê¸°
        # -----------------------------------------------------------------------------
        print("\nğŸ¥‰ Bronze Layer êµ¬ì¶• ì‹œì‘...")
        
        # S3 ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°
        landing_zone_path = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
        print(f"ğŸ“‚ ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°: {landing_zone_path}")
        
        try:
            df_raw = spark.read.json(landing_zone_path)
            row_count = df_raw.count()
            print(f"âœ… ëœë”© ì¡´ ë°ì´í„° ë¡œë“œ ì„±ê³µ! í–‰ ìˆ˜: {row_count:,}")
            
            # Bronze Parquetìœ¼ë¡œ ì €ì¥
            bronze_path = "s3a://reciping-user-event-logs/warehouse/bronze/raw_events/"
            df_raw.write.mode("overwrite").parquet(bronze_path)
            print(f"âœ… Bronze ë°ì´í„°ë¥¼ {bronze_path}ì— ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëœë”© ì¡´ì—ì„œ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("ğŸ’¡ upload_to_landing_zone.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            spark.stop()
            return

        # -----------------------------------------------------------------------------
        # 3. ğŸ¥ˆ Silver Layer êµ¬ì¶• (ê¸°ì¡´ ë³€í™˜ ë¡œì§ ì „ì²´ ë°˜ì˜)
        # -----------------------------------------------------------------------------
        print("\nğŸ¥ˆ Silver Layer êµ¬ì¶• ì‹œì‘...")
        
        # Bronzeì—ì„œ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì½ìŠµë‹ˆë‹¤.
        df_bronze = spark.read.parquet(bronze_path)
        bronze_count = df_bronze.count()
        print(f"ğŸ“Š Bronzeì—ì„œ {bronze_count:,}í–‰ì˜ ë°ì´í„°ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.")

        # --- 3.1. ìŠ¤í‚¤ë§ˆ ì •ì˜ ---
        context_schema = StructType([
            StructField("page", StructType([
                StructField("name", StringType(), True),
                StructField("url", StringType(), True),
                StructField("path", StringType(), True)
            ]), True),
            StructField("user_segment", StringType(), True),
            StructField("activity_level", StringType(), True),
            StructField("cooking_style", StringType(), True),
            StructField("ab_test", StructType([
                StructField("scenario", StringType(), True),
                StructField("group", StringType(), True),
                StructField("start_date", StringType(), True),
                StructField("end_date", StringType(), True)
            ]), True)
        ])

        event_properties_schema = StructType([
            StructField("page_name", StringType(), True), StructField("referrer", StringType(), True),
            StructField("path", StringType(), True), StructField("method", StringType(), True),
            StructField("type", StringType(), True), StructField("search_type", StringType(), True),
            StructField("search_keyword", StringType(), True), StructField("selected_filters", ArrayType(StringType()), True),
            StructField("result_count", IntegerType(), True), StructField("list_type", StringType(), True),
            StructField("displayed_recipe_ids", ArrayType(StringType()), True), StructField("recipe_id", StringType(), True),
            StructField("rank", IntegerType(), True), StructField("action", StringType(), True),
            StructField("comment_length", IntegerType(), True), StructField("category", StringType(), True),
            StructField("ingredient_count", IntegerType(), True), StructField("ad_id", StringType(), True),
            StructField("ad_type", StringType(), True), StructField("position", StringType(), True),
            StructField("target_url", StringType(), True)
        ])

        # --- 3.2. JSON íŒŒì‹± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ---
        print("ğŸ”§ JSON íŒŒì‹± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ì¤‘...")
        df_transformed = df_bronze \
            .withColumn("parsed_context", from_json(col("context"), context_schema)) \
            .withColumn("parsed_properties", from_json(col("event_properties"), event_properties_schema)) \
            .withColumn("timestamp_parsed", to_timestamp(col("timestamp"))) \
            .withColumn("date_parsed", col("date").cast(DateType())) \
            .drop("context", "event_properties")

        # --- 3.3. íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„± (KST ê¸°ì¤€) ---
        print("ğŸ“… íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„± ì¤‘...")
        df_with_partitions = df_transformed \
            .withColumn("year", year(col("timestamp_parsed"))) \
            .withColumn("month", month(col("timestamp_parsed"))) \
            .withColumn("day", dayofmonth(col("timestamp_parsed"))) \
            .withColumn("hour", hour(col("timestamp_parsed")))

        # --- 3.4. ì»¬ëŸ¼ í‰íƒ„í™” ---
        print("ğŸ—‚ï¸ ì»¬ëŸ¼ í‰íƒ„í™” ì¤‘...")
        df_silver_flat = df_with_partitions.select(
            "event_id", "event_name", "user_id", "anonymous_id", "session_id",
            col("timestamp_parsed").alias("event_timestamp"), col("date_parsed").alias("event_date"),
            "year", "month", "day", "hour",
            col("parsed_context.page.name").alias("page_name"),
            col("parsed_context.page.url").alias("page_url"),
            col("parsed_context.page.path").alias("page_path"),
            col("parsed_context.user_segment").alias("user_segment"),
            col("parsed_context.activity_level").alias("activity_level"),
            col("parsed_context.cooking_style").alias("cooking_style"),
            col("parsed_context.ab_test.group").alias("ab_test_group"),
            col("parsed_context.ab_test.scenario").alias("ab_test_scenario"),
            col("parsed_properties.page_name").alias("prop_page_name"),
            col("parsed_properties.referrer").alias("prop_referrer"),
            col("parsed_properties.path").alias("prop_path"),
            col("parsed_properties.method").alias("prop_method"),
            col("parsed_properties.type").alias("prop_type"),
            col("parsed_properties.search_type").alias("prop_search_type"),
            col("parsed_properties.search_keyword").alias("prop_search_keyword"),
            col("parsed_properties.selected_filters").alias("prop_selected_filters"),
            col("parsed_properties.result_count").alias("prop_result_count"),
            col("parsed_properties.list_type").alias("prop_list_type"),
            col("parsed_properties.displayed_recipe_ids").alias("prop_displayed_recipe_ids"),
            col("parsed_properties.recipe_id").cast(LongType()).alias("prop_recipe_id"),
            col("parsed_properties.rank").alias("prop_rank"),
            col("parsed_properties.action").alias("prop_action"),
            col("parsed_properties.comment_length").alias("prop_comment_length"),
            col("parsed_properties.category").alias("prop_category"),
            col("parsed_properties.ingredient_count").alias("prop_ingredient_count"),
            col("parsed_properties.ad_id").alias("prop_ad_id"),
            col("parsed_properties.ad_type").alias("prop_ad_type"),
            col("parsed_properties.position").alias("prop_position"),
            col("parsed_properties.target_url").alias("prop_target_url")
        )
        
        # --- 3.5. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ---
        print("ğŸ” ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì¤‘...")
        df_silver_final = df_silver_flat.filter(col("event_id").isNotNull()).dropDuplicates(["event_id"])
        final_count = df_silver_final.count()
        print(f"âœ… ì»¬ëŸ¼ í‰íƒ„í™” ë° ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì™„ë£Œ. ìµœì¢… í–‰ ìˆ˜: {final_count:,}")
        
        # Silver ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        print("\nğŸ“Š Silver Layer ìƒ˜í”Œ ë°ì´í„° (ìƒìœ„ 3í–‰):")
        df_silver_final.show(3, truncate=True)
        
        # ì´ë²¤íŠ¸ë³„ ë¶„í¬ í™•ì¸
        print("\nğŸ“Š ì´ë²¤íŠ¸ë³„ ë¶„í¬:")
        df_silver_final.groupBy('event_name').count().orderBy('count', ascending=False).show(10)

        # --- 3.6. Silver Parquetìœ¼ë¡œ ì €ì¥ ---
        print("\nğŸ’¾ Silver Parquetìœ¼ë¡œ ì €ì¥ ì¤‘...")
        silver_path = "s3a://reciping-user-event-logs/warehouse/silver/cleaned_events/"
        
        # ì„±ëŠ¥ ìµœì í™”: íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •
        df_silver_optimized = df_silver_final.coalesce(4)
        
        df_silver_optimized.write \
            .mode("overwrite") \
            .partitionBy("year", "month", "day") \
            .option("compression", "snappy") \
            .parquet(silver_path)
        print(f"âœ… Silver ë°ì´í„°ë¥¼ {silver_path}ì— ì €ì¥ ì™„ë£Œ")

        # -----------------------------------------------------------------------------
        # 4. ê²€ì¦ ë° ìš”ì•½
        # -----------------------------------------------------------------------------
        print("\nğŸ“ˆ ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ìš”ì•½:")
        print(f"ğŸ¥‰ Bronze í–‰ ìˆ˜: {bronze_count:,}")
        print(f"ğŸ¥ˆ Silver í–‰ ìˆ˜: {final_count:,}")
        print(f"ğŸ“‚ Bronze ì €ì¥ ìœ„ì¹˜: {bronze_path}")
        print(f"ğŸ“‚ Silver ì €ì¥ ìœ„ì¹˜: {silver_path}")

        # -----------------------------------------------------------------------------
        # 5. ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        spark.stop()
        print("âœ… SparkSessionì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # ìŠ¤íŒŒí¬ ì„¸ì…˜ì´ ìˆë‹¤ë©´ ì¢…ë£Œ
        try:
            spark.stop()
        except:
            pass

# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
if __name__ == "__main__":
    main()
