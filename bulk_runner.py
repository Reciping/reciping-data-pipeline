# bulk_runner.py (ìµœì¢… ìˆ˜ì •ë³¸)
import subprocess

# --- ì„¤ì • ---
SPARK_SCRIPTS_DIR = "/home/ec2-user/spark_jobs"
# SPARK_SUBMIT_CMD = "/home/ec2-user/.local/bin/spark-submit"
SPARK_SUBMIT_CMD = "spark-submit"

# --- [ìˆ˜ì •] ì²˜ë¦¬í•  ëŒ€ìƒ íŒŒì¼ ì´ë¦„ ë³€ê²½ ---
# BULK_INPUT_FILE = "dask_events_3m.jsonl"
BULK_INPUT_FILE = "dask_events_1m.jsonl"
# ì´ ë²Œí¬ ë°ì´í„°ì˜ ë…¼ë¦¬ì  ë‚ ì§œ (Bronze/Silver íŒŒí‹°ì…˜ì— ì‚¬ìš©ë  ë‚ ì§œ)
# 6~8ì›” ë°ì´í„°ì´ë¯€ë¡œ, 8ì›”ì˜ ë§ˆì§€ë§‰ ë‚ ë¡œ ì§€ì •
TARGET_DATE = "2025-08-31" 

# Airflow DAGì™€ ë™ì¼í•œ ê³µí†µ íŒ¨í‚¤ì§€ ë° ì„¤ì •
PACKAGES = "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.iceberg:iceberg-aws-bundle:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.5.4"
# CONF = {
#     # # --- [ì¶”ê°€] Spark ì„ì‹œ ë””ë ‰í† ë¦¬ë¥¼ ë©”ì¸ ë””ìŠ¤í¬ ê²½ë¡œë¡œ ë³€ê²½ ---
#     # "spark.local.dir": "/home/ec2-user/spark_tmp",
#     # # --- [ì¶”ê°€] Executor ë©”ëª¨ë¦¬ ì„¤ì • ---
#     # "spark.executor.memory": "4g",

#     # # --- [ì¶”ê°€] ì…”í”Œ íŒŒí‹°ì…˜ ê°œìˆ˜ ì„¤ì • ---
#     # "spark.sql.shuffle.partitions": "400",

#     "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
#     "spark.hadoop.fs.s3a.path.style.access": "true",
#     "spark.hadoop.fs.s3a.endpoint": "s3.ap-northeast-2.amazonaws.com", 
#     "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.InstanceProfileCredentialsProvider",
#     "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
#     "spark.sql.catalog.iceberg_catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
# }

CONF = {
    # ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™”
    "spark.executor.memory": "3g",
    "spark.driver.memory": "3g", 
    "spark.executor.memoryFraction": "0.6",
    "spark.storage.memoryFraction": "0.3",
    "spark.sql.adaptive.coalescePartitions.maxBatchSize": "128MB",
    
    # íŒŒí‹°ì…˜ ê°œìˆ˜ ì¤„ì´ê¸°
    "spark.sql.shuffle.partitions": "100",  # 400ì—ì„œ 100ìœ¼ë¡œ ê°ì†Œ
    "spark.sql.adaptive.advisoryPartitionSizeInBytes": "64MB",
    
    # GC ì„¤ì • ê°œì„  (Java 8ìš©)
    "spark.executor.extraJavaOptions": "-XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:+UseStringDeduplication",
    "spark.driver.extraJavaOptions": "-XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:+UseStringDeduplication",
    
    # Iceberg ìµœì í™”
    "spark.sql.iceberg.vectorization.enabled": "false",  # ë©”ëª¨ë¦¬ ì ˆì•½
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.kryoserializer.buffer.max": "128m",
    
    # S3A ì„¤ì • ìµœì í™”
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
    "spark.hadoop.fs.s3a.path.style.access": "true",
    "spark.hadoop.fs.s3a.endpoint": "s3.ap-northeast-2.amazonaws.com",
    "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.InstanceProfileCredentialsProvider",
    "spark.hadoop.fs.s3a.connection.maximum": "10",  # ì—°ê²° ìˆ˜ ì œí•œ
    "spark.hadoop.fs.s3a.threads.max": "5",          # ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ
    
    "spark.sql.catalog.iceberg_catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
}

def run_spark_job(script_name, args):
    """ì£¼ì–´ì§„ ìŠ¤í¬ë¦½íŠ¸ì™€ ì¸ìë¡œ spark-submitì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\n{'='*20}\nRunning Spark job: {script_name} with args: {args}\n{'='*20}")
    
    command = [SPARK_SUBMIT_CMD]
    command.extend(["--packages", PACKAGES])
    for key, value in CONF.items():
        command.extend(["--conf", f"{key}={value}"])
    
    command.append(f"{SPARK_SCRIPTS_DIR}/{script_name}")
    command.extend(args)
    
    subprocess.run(command, check=True)

# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    try:
        # --- [ìˆ˜ì •] staging_to_bronze_iceberg.py í˜¸ì¶œ ì‹œ --target-date ì¸ì ì¶”ê°€ ---
        run_spark_job(
            "staging_to_bronze_iceberg.py",
            ["--input-file-name", BULK_INPUT_FILE, "--target-date", TARGET_DATE, "--test-mode", "false"]
        )
        # --- ìˆ˜ì • ë ---
        
        # 2. Bronze -> Silver (ìš´ì˜ ëª¨ë“œë¡œ ì‹¤í–‰)
        run_spark_job("bronze_to_silver_iceberg.py", ["--target-date", TARGET_DATE, "--test-mode", "false"])
        
        # 3. Create Dims (ìš´ì˜ ëª¨ë“œë¡œ ì‹¤í–‰)
        run_spark_job("create_dims.py", ["--test-mode", "false"])
        
        # 4. Silver -> Gold (ìš´ì˜ ëª¨ë“œë¡œ ì‹¤í–‰)
        run_spark_job("silver_to_gold_processor.py", ["--test-mode", "false"])
        
        print("\nğŸ‰ Bulk data loading completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ An error occurred: {e}")