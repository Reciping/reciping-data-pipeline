# gold_layer_analytics_advanced.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

def main():
    """
    ê³ ê¸‰ Gold Layer ë¶„ì„ íŒŒì´í”„ë¼ì¸
    - ì‚¬ìš©ì í–‰ë™ ë¶„ì„
    - ë ˆì‹œí”¼ ì¸ê¸°ë„ íŠ¸ë Œë“œ
    - ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¶„ì„
    - ì‹œê³„ì—´ ë¶„ì„
    - ì¶”ì²œ ì‹œìŠ¤í…œ ê¸°ì´ˆ ë°ì´í„°
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        import os
        os.environ['HADOOP_USER_NAME'] = 'root'
        os.environ['USER'] = 'root'
        
        print("ğŸ† Gold Layer ê³ ê¸‰ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")

        # -----------------------------------------------------------------------------
        # 1. SparkSession ìƒì„±
        # -----------------------------------------------------------------------------
        spark = SparkSession.builder \
            .appName("Advanced_Gold_Layer_Analytics") \
            .master("local[2]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "")) \
            .config("spark.hadoop.fs.s3a.region", "ap-northeast-2") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSession ìƒì„± ì™„ë£Œ")

        # -----------------------------------------------------------------------------
        # 2. Silver Layer ë°ì´í„° ë¡œë“œ
        # -----------------------------------------------------------------------------
        print("\nğŸ“Š Silver Layer ë°ì´í„° ë¡œë“œ...")
        
        # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© ê²½ë¡œ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” S3 ê²½ë¡œ ì‚¬ìš©)
        silver_path = "s3a://reciping-user-event-logs/silver/warehouse/silver_events/"
        
        # ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•  ê²½ìš° ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        if not os.getenv("AWS_ACCESS_KEY_ID"):
            print("âš ï¸ AWS ìê²© ì¦ëª…ì´ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
            df_silver = create_sample_data(spark)
        else:
            try:
                df_silver = spark.read.parquet(silver_path)
                print(f"âœ… Silver ë°ì´í„° ë¡œë“œ ì„±ê³µ: {df_silver.count():,}í–‰")
            except:
                print("âš ï¸ Silver ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                df_silver = create_sample_data(spark)

        # ë°ì´í„° í™•ì¸
        print(f"ğŸ“Š ì²˜ë¦¬í•  ë°ì´í„°: {df_silver.count():,}í–‰")
        df_silver.printSchema()

        # -----------------------------------------------------------------------------
        # 3. ğŸ¯ ì‚¬ìš©ì í–‰ë™ ë¶„ì„ (User Behavior Analytics)
        # -----------------------------------------------------------------------------
        print("\nğŸ¯ ì‚¬ìš©ì í–‰ë™ ë¶„ì„...")
        
        # 3.1. ì‚¬ìš©ìë³„ ì„¸ì…˜ ë¶„ì„
        user_session_analysis = df_silver.groupBy("user_id", "session_id") \
            .agg(
                count("*").alias("events_per_session"),
                countDistinct("event_name").alias("unique_event_types"),
                min("event_timestamp").alias("session_start"),
                max("event_timestamp").alias("session_end"),
                when(max("event_timestamp").isNotNull() & min("event_timestamp").isNotNull(),
                     (unix_timestamp("max(event_timestamp)") - unix_timestamp("min(event_timestamp)")) / 60)
                .otherwise(0).alias("session_duration_minutes")
            ) \
            .filter(col("events_per_session") >= 2)  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì´ë²¤íŠ¸ê°€ ìˆëŠ” ì„¸ì…˜ë§Œ
        
        print("ğŸ“ˆ ì‚¬ìš©ì ì„¸ì…˜ ë¶„ì„ ê²°ê³¼:")
        user_session_analysis.orderBy(desc("session_duration_minutes")).show(10)
        
        # 3.2. ì‚¬ìš©ìë³„ ëˆ„ì  í†µê³„
        user_cumulative_stats = df_silver.groupBy("user_id") \
            .agg(
                count("*").alias("total_events"),
                countDistinct("session_id").alias("total_sessions"),
                countDistinct("event_name").alias("unique_event_types"),
                countDistinct(date_format("event_timestamp", "yyyy-MM-dd")).alias("active_days"),
                min("event_timestamp").alias("first_seen"),
                max("event_timestamp").alias("last_seen")
            ) \
            .withColumn("avg_events_per_session", 
                       round(col("total_events") / col("total_sessions"), 2)) \
            .withColumn("user_lifetime_days",
                       datediff(col("last_seen"), col("first_seen")) + 1)
        
        print("ğŸ“Š ì‚¬ìš©ì ëˆ„ì  í†µê³„:")
        user_cumulative_stats.orderBy(desc("total_events")).show(10)

        # -----------------------------------------------------------------------------
        # 4. ğŸ”¥ ì´ë²¤íŠ¸ íŠ¸ë Œë“œ ë¶„ì„ (Event Trend Analytics)
        # -----------------------------------------------------------------------------
        print("\nğŸ”¥ ì´ë²¤íŠ¸ íŠ¸ë Œë“œ ë¶„ì„...")
        
        # 4.1. ì‹œê°„ëŒ€ë³„ ì´ë²¤íŠ¸ ë¶„í¬
        hourly_trends = df_silver.groupBy("hour", "event_name") \
            .agg(count("*").alias("event_count")) \
            .orderBy("hour", "event_name")
        
        print("â° ì‹œê°„ëŒ€ë³„ ì´ë²¤íŠ¸ ë¶„í¬:")
        hourly_trends.show(24)
        
        # 4.2. ì¼ë³„ ì´ë²¤íŠ¸ íŠ¸ë Œë“œ
        daily_trends = df_silver.groupBy(
            date_format("event_timestamp", "yyyy-MM-dd").alias("date"),
            "event_name"
        ).agg(
            count("*").alias("daily_count"),
            countDistinct("user_id").alias("unique_users")
        ).orderBy("date", "event_name")
        
        print("ğŸ“… ì¼ë³„ ì´ë²¤íŠ¸ íŠ¸ë Œë“œ:")
        daily_trends.show(20)

        # -----------------------------------------------------------------------------
        # 5. ğŸ·ï¸ ì‚¬ìš©ì ì„¸ê·¸ë©˜í…Œì´ì…˜ (User Segmentation)
        # -----------------------------------------------------------------------------
        print("\nğŸ·ï¸ ì‚¬ìš©ì ì„¸ê·¸ë©˜í…Œì´ì…˜...")
        
        # 5.1. í™œë™ ìˆ˜ì¤€ë³„ ì‚¬ìš©ì ë¶„ë¥˜
        user_activity_segments = user_cumulative_stats \
            .withColumn("activity_segment",
                when(col("total_events") >= 50, "High Activity")
                .when(col("total_events") >= 20, "Medium Activity")
                .when(col("total_events") >= 5, "Low Activity")
                .otherwise("Minimal Activity")
            ) \
            .withColumn("engagement_segment",
                when(col("unique_event_types") >= 5, "Highly Engaged")
                .when(col("unique_event_types") >= 3, "Moderately Engaged")
                .otherwise("Lightly Engaged")
            )
        
        # ì„¸ê·¸ë¨¼íŠ¸ë³„ í†µê³„
        segment_summary = user_activity_segments.groupBy("activity_segment", "engagement_segment") \
            .agg(
                count("*").alias("user_count"),
                avg("total_events").alias("avg_events"),
                avg("total_sessions").alias("avg_sessions"),
                avg("user_lifetime_days").alias("avg_lifetime_days")
            ).orderBy("activity_segment", "engagement_segment")
        
        print("ğŸ“Š ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„:")
        segment_summary.show()

        # -----------------------------------------------------------------------------
        # 6. ğŸ³ ë ˆì‹œí”¼ ìƒí˜¸ì‘ìš© ë¶„ì„ (Recipe Interaction Analytics)
        # -----------------------------------------------------------------------------
        print("\nğŸ³ ë ˆì‹œí”¼ ìƒí˜¸ì‘ìš© ë¶„ì„...")
        
        # ë ˆì‹œí”¼ ê´€ë ¨ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§
        recipe_events = df_silver.filter(
            col("event_name").isin(["recipe_view", "recipe_search", "recipe_bookmark", "recipe_rating", "recipe_share"])
            | col("prop_recipe_id").isNotNull()
        )
        
        if recipe_events.count() > 0:
            # 6.1. ë ˆì‹œí”¼ë³„ ì¸ê¸°ë„ ë¶„ì„
            recipe_popularity = recipe_events.filter(col("prop_recipe_id").isNotNull()) \
                .groupBy("prop_recipe_id") \
                .agg(
                    count("*").alias("total_interactions"),
                    countDistinct("user_id").alias("unique_users"),
                    countDistinct("session_id").alias("unique_sessions"),
                    sum(when(col("event_name") == "recipe_view", 1).otherwise(0)).alias("views"),
                    sum(when(col("event_name") == "recipe_bookmark", 1).otherwise(0)).alias("bookmarks"),
                    sum(when(col("event_name") == "recipe_share", 1).otherwise(0)).alias("shares"),
                    sum(when(col("event_name") == "recipe_rating", 1).otherwise(0)).alias("ratings")
                ) \
                .withColumn("engagement_score", 
                           col("views") * 1 + col("bookmarks") * 3 + col("shares") * 5 + col("ratings") * 4)
            
            print("ğŸ† ë ˆì‹œí”¼ ì¸ê¸°ë„ ìˆœìœ„:")
            recipe_popularity.orderBy(desc("engagement_score")).show(10)
        else:
            print("âš ï¸ ë ˆì‹œí”¼ ê´€ë ¨ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # -----------------------------------------------------------------------------
        # 7. ğŸ“ˆ ê³ ê¸‰ ë¶„ì„ ì§€í‘œ (Advanced Analytics Metrics)
        # -----------------------------------------------------------------------------
        print("\nğŸ“ˆ ê³ ê¸‰ ë¶„ì„ ì§€í‘œ ê³„ì‚°...")
        
        # 7.1. ë¦¬í…ì…˜ ë¶„ì„ (ë‹¨ìˆœí™”ëœ ë²„ì „)
        window_spec = Window.partitionBy("user_id").orderBy("event_date")
        
        retention_analysis = df_silver.select("user_id", "event_date") \
            .distinct() \
            .withColumn("day_number", row_number().over(window_spec)) \
            .groupBy("day_number") \
            .agg(countDistinct("user_id").alias("returning_users")) \
            .orderBy("day_number")
        
        print("ğŸ”„ ì¼ë³„ ë¦¬í…ì…˜ (ë‹¨ìˆœí™”):")
        retention_analysis.show(10)
        
        # 7.2. ì „í™˜ìœ¨ ë¶„ì„ (Conversion Funnel)
        funnel_events = ["page_view", "recipe_search", "recipe_view", "recipe_bookmark"]
        
        conversion_funnel = df_silver.filter(col("event_name").isin(funnel_events)) \
            .groupBy("user_id") \
            .agg(
                sum(when(col("event_name") == "page_view", 1).otherwise(0)).alias("page_views"),
                sum(when(col("event_name") == "recipe_search", 1).otherwise(0)).alias("searches"),
                sum(when(col("event_name") == "recipe_view", 1).otherwise(0)).alias("recipe_views"),
                sum(when(col("event_name") == "recipe_bookmark", 1).otherwise(0)).alias("bookmarks")
            ) \
            .agg(
                count("*").alias("total_users"),
                sum(when(col("page_views") > 0, 1).otherwise(0)).alias("users_with_page_views"),
                sum(when(col("searches") > 0, 1).otherwise(0)).alias("users_with_searches"),
                sum(when(col("recipe_views") > 0, 1).otherwise(0)).alias("users_with_recipe_views"),
                sum(when(col("bookmarks") > 0, 1).otherwise(0)).alias("users_with_bookmarks")
            )
        
        print("ğŸ¯ ì „í™˜ ê¹”ë•Œê¸° ë¶„ì„:")
        conversion_funnel.show()

        # -----------------------------------------------------------------------------
        # 8. ğŸ’¾ Gold Layer í…Œì´ë¸” ì €ì¥
        # -----------------------------------------------------------------------------
        print("\nğŸ’¾ Gold Layer ë¶„ì„ ê²°ê³¼ ì €ì¥...")
        
        # ë¡œì»¬ ì €ì¥ ê²½ë¡œ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” S3 ê²½ë¡œ)
        output_base = "data/output/gold_layer"
        
        try:
            # ê° ë¶„ì„ ê²°ê³¼ë¥¼ ë³„ë„ í…Œì´ë¸”ë¡œ ì €ì¥
            user_session_analysis.coalesce(1).write.mode("overwrite").parquet(f"{output_base}/user_sessions")
            user_cumulative_stats.coalesce(1).write.mode("overwrite").parquet(f"{output_base}/user_stats")
            user_activity_segments.coalesce(1).write.mode("overwrite").parquet(f"{output_base}/user_segments")
            hourly_trends.coalesce(1).write.mode("overwrite").parquet(f"{output_base}/hourly_trends")
            daily_trends.coalesce(1).write.mode("overwrite").parquet(f"{output_base}/daily_trends")
            
            if recipe_events.count() > 0:
                recipe_popularity.coalesce(1).write.mode("overwrite").parquet(f"{output_base}/recipe_popularity")
            
            print("âœ… Gold Layer ë¶„ì„ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ")
            
            # ìš”ì•½ í†µê³„ ìƒì„±
            summary_stats = spark.sql(f"""
                SELECT 
                    'user_sessions' as table_name,
                    {user_session_analysis.count()} as record_count,
                    current_timestamp() as created_at
                UNION ALL
                SELECT 
                    'user_stats' as table_name,
                    {user_cumulative_stats.count()} as record_count,
                    current_timestamp() as created_at
                UNION ALL
                SELECT 
                    'hourly_trends' as table_name,
                    {hourly_trends.count()} as record_count,
                    current_timestamp() as created_at
            """)
            
            summary_stats.coalesce(1).write.mode("overwrite").parquet(f"{output_base}/summary_stats")
            
        except Exception as e:
            print(f"âš ï¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ (ë¡œì»¬ í™˜ê²½): {e}")
            print("ğŸ“ ê²°ê³¼ëŠ” ë©”ëª¨ë¦¬ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        # -----------------------------------------------------------------------------
        # 9. ğŸ“‹ ì¢…í•© ìš”ì•½ ë¦¬í¬íŠ¸
        # -----------------------------------------------------------------------------
        print("\nğŸ“‹ Gold Layer ê³ ê¸‰ ë¶„ì„ ì™„ë£Œ ìš”ì•½:")
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ëœ ì´ë²¤íŠ¸: {df_silver.count():,}í–‰")
        print(f"ğŸ‘¥ ê³ ìœ  ì‚¬ìš©ì: {df_silver.select('user_id').distinct().count():,}ëª…")
        print(f"ğŸ¯ ê³ ìœ  ì„¸ì…˜: {df_silver.select('session_id').distinct().count():,}ê°œ")
        print(f"ğŸ“… ë¶„ì„ ê¸°ê°„: {df_silver.agg(min('event_timestamp'), max('event_timestamp')).collect()[0]}")
        
        print(f"\nğŸ† ìƒì„±ëœ Gold Layer ë¶„ì„ í…Œì´ë¸”:")
        print(f"   âœ… ì‚¬ìš©ì ì„¸ì…˜ ë¶„ì„: ì„¸ì…˜ë³„ í™œë™ íŒ¨í„´")
        print(f"   âœ… ì‚¬ìš©ì ëˆ„ì  í†µê³„: ì¥ê¸°ì  ì‚¬ìš©ì í–‰ë™")
        print(f"   âœ… ì‚¬ìš©ì ì„¸ê·¸ë©˜í…Œì´ì…˜: í™œë™ ìˆ˜ì¤€ë³„ ë¶„ë¥˜")
        print(f"   âœ… ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ: ì‹œê°„/ì¼ë³„ ì´ë²¤íŠ¸ íŒ¨í„´")
        print(f"   âœ… ì „í™˜ìœ¨ ë¶„ì„: ì‚¬ìš©ì ì—¬ì • ì¶”ì ")
        if recipe_events.count() > 0:
            print(f"   âœ… ë ˆì‹œí”¼ ì¸ê¸°ë„: ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„")
        
        print(f"\nğŸ’¡ ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ êµ¬í˜„ ì™„ë£Œ:")
        print(f"   ğŸ¯ ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ë¶„ì„")
        print(f"   ğŸ“ˆ ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„")
        print(f"   ğŸ·ï¸ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜")
        print(f"   ğŸ”„ ë¦¬í…ì…˜ ë° ì „í™˜ìœ¨ ë¶„ì„")
        print(f"   ğŸ’¾ í™•ì¥ ê°€ëŠ¥í•œ ë°ì´í„° ë§ˆíŠ¸ êµ¬ì¡°")

        spark.stop()
        print("âœ… Gold Layer ê³ ê¸‰ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        try:
            spark.stop()
        except:
            pass

def create_sample_data(spark):
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„± í•¨ìˆ˜"""
    from datetime import datetime, timedelta
    import random
    
    # ìƒ˜í”Œ ì´ë²¤íŠ¸ ë°ì´í„°
    sample_events = []
    users = [f"user{i:03d}" for i in range(1, 21)]  # 20ëª…ì˜ ì‚¬ìš©ì
    events = ["page_view", "recipe_search", "recipe_view", "recipe_bookmark", "recipe_rating", "recipe_share"]
    
    base_date = datetime(2024, 1, 15)
    
    for i in range(1000):  # 1000ê°œì˜ ì´ë²¤íŠ¸
        user = random.choice(users)
        event = random.choice(events)
        session = f"session{random.randint(1, 100):03d}"
        
        # ì‹œê°„ ìƒì„± (7ì¼ê°„)
        days_offset = random.randint(0, 6)
        hours_offset = random.randint(0, 23)
        minutes_offset = random.randint(0, 59)
        
        event_time = base_date + timedelta(days=days_offset, hours=hours_offset, minutes=minutes_offset)
        
        recipe_id = random.randint(1, 50) if event.startswith("recipe") else None
        
        sample_events.append((
            f"evt{i+1:04d}",
            event,
            user,
            f"anon{user[-3:]}",
            session,
            event_time,
            event_time.date(),
            event_time,
            event_time.year,
            event_time.month,
            event_time.day,
            event_time.hour,
            recipe_id
        ))
    
    columns = [
        "event_id", "event_name", "user_id", "anonymous_id", "session_id",
        "event_timestamp", "event_date", "ingestion_timestamp", 
        "year", "month", "day", "hour", "prop_recipe_id"
    ]
    
    return spark.createDataFrame(sample_events, columns)

if __name__ == "__main__":
    main()
