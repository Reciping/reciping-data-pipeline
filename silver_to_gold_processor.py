#!/usr/bin/env python3
"""
ğŸ§Š Silver to Gold ETL Processor (Incremental, Airflow-triggered) - Final Version
================================================================================
Silver Iceberg í…Œì´ë¸”ê³¼ Dimension í…Œì´ë¸”ë“¤ì„ ì¡°ì¸í•˜ì—¬ Gold Fact í…Œì´ë¸”ë¡œ ë³€í™˜/ì§‘ê³„í•©ë‹ˆë‹¤.
Airflowë¡œë¶€í„° ì‹¤í–‰ ì‹œê°„ì„ ë°›ì•„ ì ì§„ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë©°,
ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ë² ì´ìŠ¤ì— ìˆëŠ” í…Œì´ë¸”ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ì°¸ì¡°í•©ë‹ˆë‹¤.
"""
import logging
import argparse
from datetime import datetime
import pytz
from dateutil.parser import isoparse
from typing import Optional

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, from_json, current_timestamp, lit,
    year, month, dayofmonth, hour, date_format, expr, when, size, split, coalesce
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType,
    DoubleType, BooleanType, TimestampType, DateType, LongType, ArrayType
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SilverToGoldProcessor:
    def __init__(self, test_mode: bool = True):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        self.hive_metastore_uri = "thrift://10.0.11.86:9083" # ìì‹ ì˜ Hive Metastore URI

        if test_mode:
            self.silver_database = "recipe_analytics_test"
            self.gold_database = "recipe_analytics_test"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/test_warehouse/"
            self.table_suffix = "_test"
        else:
            self.silver_database = "recipe_analytics"
            self.gold_database = "gold_analytics"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/warehouse/"
            self.table_suffix = ""
            
        # ë³€ìˆ˜ì—ëŠ” ì¹´íƒˆë¡œê·¸/DBì´ë¦„ ì—†ì´ ìˆœìˆ˜ í…Œì´ë¸” ì´ë¦„ë§Œ ì €ì¥
        self.silver_table_name = f"user_events_silver{self.table_suffix}"
        self.gold_table_name = f"fact_user_events{self.table_suffix}"

    def create_spark_session(self):
        """SparkSession ìƒì„± ë° ì¹´íƒˆë¡œê·¸/DB ì„¤ì •"""
        print("SparkSession ìƒì„± ì¤‘...")
        self.spark = SparkSession.builder \
            .appName("SilverToGold_ETL") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", self.hive_metastore_uri) \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", self.s3_warehouse_path) \
            .getOrCreate()
        self.spark.sparkContext.setLogLevel("WARN")

        # ì„¸ì…˜ì˜ ê¸°ë³¸ ì¹´íƒˆë¡œê·¸ì™€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ Gold ìš©ìœ¼ë¡œ ì„¤ì •
        print(f"í˜„ì¬ ì¹´íƒˆë¡œê·¸ë¥¼ '{self.catalog_name}'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"USE {self.catalog_name}")
        
        print(f"ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ '{self.gold_database}' ìƒì„± ë° ì‚¬ìš© ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.gold_database}")
        self.spark.sql(f"USE {self.gold_database}")
        
        print("SparkSession ìƒì„± ë° ì„¤ì • ì™„ë£Œ")

    def create_gold_table_if_not_exists(self):
        """Gold Fact í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"Gold Fact í…Œì´ë¸” ìƒì„± í™•ì¸: {self.gold_table_name}")
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.gold_table_name} (
            event_id STRING, user_dim_key BIGINT, time_dim_key BIGINT, recipe_dim_key BIGINT,
            page_dim_key BIGINT, event_dim_key BIGINT, event_count INT,
            session_duration_seconds BIGINT, page_view_duration_seconds BIGINT,
            is_conversion BOOLEAN, conversion_value DOUBLE, engagement_score DOUBLE,
            session_id STRING, anonymous_id STRING, created_at TIMESTAMP, updated_at TIMESTAMP
        )
        USING ICEBERG
        PARTITIONED BY (days(created_at))
        """
        self.spark.sql(create_table_sql)
        print("Gold Fact í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ")

    def transform_and_load_gold_data(self, target_date: str):
        """Silver ë° Dimension í…Œì´ë¸”ë“¤ì„ ì¡°ì¸í•˜ì—¬ Gold Fact í…Œì´ë¸”ë¡œ ë³€í™˜ ë° ì ì¬í•©ë‹ˆë‹¤."""
        print(f"Silver to Gold ì²˜ë¦¬ ì‹œì‘ (ëŒ€ìƒ ë‚ ì§œ: {target_date})")
        
        # í…Œì´ë¸” ì „ì²´ ì´ë¦„(DB.TABLE)ì„ ëª…ì‹œì ìœ¼ë¡œ ìƒì„±
        full_silver_table = f"{self.silver_database}.{self.silver_table_name}"
        dim_user_table = f"dim_user{self.table_suffix}"
        dim_recipe_table = f"dim_recipe{self.table_suffix}"
        dim_event_table = f"dim_event{self.table_suffix}"
        dim_page_table = f"dim_page{self.table_suffix}"
        
        try:
            # 1. Silver í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì½ê¸° (ì „ì²´ ì´ë¦„ ì‚¬ìš©)
            silver_df = self.spark.read.table(full_silver_table).where(f"date = '{target_date}'")
            
            silver_count = silver_df.count()
            if silver_count == 0:
                print(f"{target_date} ë‚ ì§œì˜ Silver ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return

            print(f"{target_date} ë‚ ì§œì˜ Silver ë°ì´í„° {silver_count}ê±´ì„ Gold í…Œì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")

            # 2. í•„ìš”í•œ ëª¨ë“  Dimension í…Œì´ë¸” ì½ê¸°
            dim_user = self.spark.read.table(dim_user_table)
            dim_recipe = self.spark.read.table(dim_recipe_table)
            dim_event = self.spark.read.table(dim_event_table)
            dim_page = self.spark.read.table(dim_page_table)

            # 3. Silver ë°ì´í„°ì™€ ëª¨ë“  Dimensionì„ ìˆœì°¨ì ìœ¼ë¡œ ì¡°ì¸
            joined_df = silver_df \
                .join(dim_user, ["user_id", "anonymous_id", "user_segment", "cooking_style", "ab_test_group"], "left") \
                .join(dim_recipe, silver_df.prop_recipe_id == dim_recipe.recipe_id, "left") \
                .join(dim_event, "event_name", "left") \
                .join(dim_page, ["page_name", "page_url"], "left")

            # 4. ìµœì¢… Fact í…Œì´ë¸” í˜•íƒœ ìƒì„±
            fact_df = joined_df.select(
                col("event_id"),
                coalesce(col("user_sk"), lit(0)).alias("user_dim_key"),
                date_format(col("kst_timestamp"), "yyyyMMddHH").cast("bigint").alias("time_dim_key"),
                coalesce(col("recipe_sk"), lit(0)).alias("recipe_dim_key"),
                coalesce(col("page_sk"), lit(0)).alias("page_dim_key"),
                coalesce(col("event_sk"), lit(0)).alias("event_dim_key"),
                lit(1).alias("event_count"),
                when(col("prop_action").isNotNull() & (size(split(col("prop_action"), ":")) >= 2), 
                     coalesce(split(col("prop_action"), ":")[1].cast("bigint"), lit(60)))
                .otherwise(60).alias("session_duration_seconds"),
                lit(30).cast("bigint").alias("page_view_duration_seconds"),
                when(col("event_name").isin('auth_success', 'click_bookmark', 'create_comment'), True).otherwise(False).alias("is_conversion"),
                lit(1.0).alias("conversion_value"),
                when(col("event_name") == 'auth_success', 10.0).when(col("event_name") == 'create_comment', 9.0)
                .when(col("event_name") == 'click_bookmark', 8.0).when(col("event_name") == 'click_recipe', 7.0)
                .when(col("event_name") == 'search_recipe', 5.0).when(col("event_name") == 'view_recipe', 4.0)
                .when(col("event_name") == 'view_page', 2.0).otherwise(1.0).alias("engagement_score"),
                col("session_id"),
                col("anonymous_id"),
                col("kst_timestamp").alias("created_at"),
                col("kst_timestamp").alias("updated_at")
            )

            # 5. Gold í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€ (Append)
            print("Gold í…Œì´ë¸” ì ì¬ ì¤‘...")
            fact_df.writeTo(self.gold_table_name).append()
            print("Gold í…Œì´ë¸” ì ì¬ ì™„ë£Œ.")

        except Exception as e:
            logger.error("Gold ë°ì´í„° ë³€í™˜/ì ì¬ ì‹¤íŒ¨", exc_info=True)
            raise

    def run_pipeline(self, execution_ts: str):
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            self.create_spark_session()
            
            kst_tz = pytz.timezone('Asia/Seoul')
            try:
                dt_obj = datetime.strptime(execution_ts, '%Y-%m-%d %H:%M')
                kst_dt = kst_tz.localize(dt_obj)
            except ValueError:
                utc_dt = isoparse(execution_ts)
                kst_dt = utc_dt.astimezone(kst_tz)
            target_date = kst_dt.strftime('%Y-%m-%d')
            
            self.create_gold_table_if_not_exists()
            
            full_silver_table_name = f"{self.silver_database}.{self.silver_table_name}"
            print(f"Silver í…Œì´ë¸”ì˜ ìµœì‹  ì •ë³´ ìƒˆë¡œê³ ì¹¨: {full_silver_table_name}")
            self.spark.catalog.refreshTable(full_silver_table_name)
            
            self.transform_and_load_gold_data(target_date)
            print("Silver to Gold ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨", exc_info=True)
            raise
        finally:
            if self.spark:
                self.spark.stop()

def main():
    parser = argparse.ArgumentParser(description="Silver to Gold Iceberg ETL Job (Incremental)")
    parser.add_argument("--execution-ts", required=True, help="Airflow execution timestamp")
    parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True, help="Run in test mode")
    args = parser.parse_args()

    processor = SilverToGoldProcessor(test_mode=args.test_mode)
    processor.run_pipeline(execution_ts=args.execution_ts)

if __name__ == "__main__":
    main()