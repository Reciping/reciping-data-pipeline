#!/usr/bin/env python3
"""
ê¸°ì¡´ í˜¸í™˜ KST ìµœì í™” Fact ì²˜ë¦¬ê¸°
- ê¸°ì¡´ ultra_batch_processor êµ¬ì¡° ìœ ì§€
- KST ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ ì»¬ëŸ¼ ìµœì†Œí™”
- ë©”ëª¨ë¦¬ ì•ˆì „ ë³´ì¥
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import time
from datetime import datetime

class CompatibleKSTFactProcessor:
    """ê¸°ì¡´ í˜¸í™˜ KST ìµœì í™” Fact ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        self.batch_size = 5000  # ì„±ê³µ ê²€ì¦ëœ ë°°ì¹˜ í¬ê¸°
        
        print("ğŸ‡°ğŸ‡· í˜¸í™˜ KST Fact ì²˜ë¦¬ê¸° ì´ˆê¸°í™”")
        print(f"   ğŸ“¦ ì•ˆì „ ë°°ì¹˜ í¬ê¸°: {self.batch_size:,}ê°œ")
        
    def create_memory_safe_spark_session(self):
        """ë©”ëª¨ë¦¬ ì•ˆì „ SparkSession"""
        print("ğŸ”§ ë©”ëª¨ë¦¬ ì•ˆì „ SparkSession ìƒì„±...")
        
        self.spark = SparkSession.builder \
            .appName("CompatibleKSTFact_SilverToGold") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://10.0.11.86:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.hadoop.fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com") \
            .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.InstanceProfileCredentialsProvider") \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… ë©”ëª¨ë¦¬ ì•ˆì „ SparkSession ìƒì„± ì™„ë£Œ")
        
    def clear_and_rebuild_fact_table(self):
        """ê¸°ì¡´ Fact í…Œì´ë¸” í´ë¦¬ì–´ í›„ KST ë°ì´í„°ë¡œ ì¬êµ¬ì¶•"""
        print("\nğŸ”„ ê¸°ì¡´ Fact í…Œì´ë¸” í´ë¦¬ì–´ í›„ KST ë°ì´í„°ë¡œ ì¬êµ¬ì¶•...")
        
        try:
            # ê¸°ì¡´ ë°ì´í„° ë°±ì—… í™•ì¸
            current_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"   í˜„ì¬ ë°ì´í„°: {current_count:,}ê°œ")
            
            if current_count > 0:
                print("   ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° í´ë¦¬ì–´...")
                self.spark.sql(f"DELETE FROM {self.catalog_name}.{self.gold_database}.fact_user_events")
                print("   âœ… í´ë¦¬ì–´ ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            print(f"âŒ í´ë¦¬ì–´ ì‹¤íŒ¨: {str(e)}")
            return False
            
    def create_kst_optimized_batch(self, start_date: str, batch_num: int = 0):
        """KST ìµœì í™”ëœ ë°°ì¹˜ ìƒì„± (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ í˜¸í™˜)"""
        print(f"\nğŸ“… KST ë°°ì¹˜ ìƒì„±: {start_date} (ë°°ì¹˜ #{batch_num + 1})")
        
        try:
            offset = batch_num * self.batch_size
            
            # ê¸°ì¡´ ìŠ¤í‚¤ë§ˆì™€ í˜¸í™˜ë˜ëŠ” KST ìµœì í™” ì¿¼ë¦¬
            kst_batch_query = f"""
            INSERT INTO {self.catalog_name}.{self.gold_database}.fact_user_events
            SELECT 
                s.event_id,
                
                -- ì°¨ì› í‚¤ë“¤ (KST ê¸°ë°˜ìœ¼ë¡œ ìƒì„±)
                0 as user_dim_key,
                
                -- time_dim_keyë¥¼ KST ê¸°ì¤€ìœ¼ë¡œ ìƒì„± (YYYYMMDDHH í˜•ì‹)
                CAST(DATE_FORMAT(s.kst_timestamp, 'yyyyMMddHH') AS BIGINT) as time_dim_key,
                
                COALESCE(s.prop_recipe_id, 0) as recipe_dim_key,
                0 as page_dim_key,
                
                -- event_dim_key (ì´ë²¤íŠ¸ ìœ í˜•ë³„ êµ¬ë¶„)
                CASE 
                    WHEN s.event_name = 'auth_success' THEN 1
                    WHEN s.event_name = 'create_comment' THEN 2
                    WHEN s.event_name = 'click_bookmark' THEN 3
                    WHEN s.event_name = 'click_recipe' THEN 4
                    WHEN s.event_name = 'search_recipe' THEN 5
                    WHEN s.event_name = 'view_recipe' THEN 6
                    WHEN s.event_name = 'view_page' THEN 7
                    ELSE 0
                END as event_dim_key,
                
                -- ì¸¡ì •ê°’
                1 as event_count,
                
                -- ì„¸ì…˜ ì‹œê°„ (prop_actionì—ì„œ ì¶”ì¶œ)
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 2
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[1] AS BIGINT), 60)
                    ELSE 60
                END as session_duration_seconds,
                
                30 as page_view_duration_seconds,
                
                -- ì „í™˜ í”Œë˜ê·¸
                CASE WHEN s.event_name IN ('auth_success', 'click_bookmark', 'create_comment') THEN TRUE ELSE FALSE END as is_conversion,
                
                1.0 as conversion_value,
                
                -- KST ì‹œê°„ëŒ€ë³„ ì°¸ì—¬ë„ ì ìˆ˜ (í•œêµ­ ì‚¬ìš© íŒ¨í„´ ìµœì í™”)
                CASE 
                    WHEN s.event_name = 'auth_success' THEN 10.0
                    WHEN s.event_name = 'create_comment' THEN 9.0
                    WHEN s.event_name = 'click_bookmark' THEN 8.0
                    WHEN s.event_name = 'click_recipe' THEN 7.0
                    WHEN s.event_name = 'search_recipe' THEN 5.0
                    WHEN s.event_name = 'view_recipe' THEN 4.0
                    WHEN s.event_name = 'view_page' THEN 2.0
                    ELSE 1.0
                END as engagement_score,
                
                -- Degenerate Dimensions
                s.session_id,
                s.anonymous_id,
                
                -- ETL ë©”íƒ€ë°ì´í„° (KST íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©)
                s.kst_timestamp as created_at,
                s.kst_timestamp as updated_at
                
            FROM (
                SELECT 
                    event_id, kst_timestamp, utc_timestamp, date, year, month, day, hour,
                    user_id, user_segment, cooking_style, ab_test_group,
                    event_name, page_name, prop_recipe_id, prop_action,
                    session_id, anonymous_id,
                    ROW_NUMBER() OVER (ORDER BY kst_timestamp, event_id) as row_num
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                WHERE date = '{start_date}' AND event_id IS NOT NULL
            ) s
            WHERE s.row_num > {offset} AND s.row_num <= {offset + self.batch_size}
            """
            
            start_time = time.time()
            self.spark.sql(kst_batch_query)
            end_time = time.time()
            
            # ê²°ê³¼ í™•ì¸
            current_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            
            batch_time = end_time - start_time
            print(f"   âœ… ë°°ì¹˜ ì™„ë£Œ: +{self.batch_size:,}ê°œ (ëˆ„ì : {current_count:,}ê°œ, {batch_time:.1f}ì´ˆ)")
            
            return self.batch_size
            
        except Exception as e:
            print(f"   âŒ ë°°ì¹˜ ì‹¤íŒ¨: {str(e)[:100]}...")
            return 0
            
    def process_kst_date_range(self, start_date: str, end_date: str):
        """KST ê¸°ë°˜ ë‚ ì§œ ë²”ìœ„ ì²˜ë¦¬"""
        print(f"\nğŸ—“ï¸ KST ë‚ ì§œ ë²”ìœ„ ì²˜ë¦¬: {start_date} ~ {end_date}")
        
        # ë‚ ì§œë³„ ì²˜ë¦¬
        from datetime import datetime, timedelta
        
        current_date = datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')
        
        total_processed = 0
        overall_batches = 0
        
        while current_date <= end_date_obj:
            date_str = current_date.strftime('%Y-%m-%d')
            print(f"\nğŸ“… {date_str} KST ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
            
            try:
                # í•´ë‹¹ ë‚ ì§œì˜ ì´ë²¤íŠ¸ ìˆ˜ í™•ì¸
                date_count = self.spark.sql(f"""
                    SELECT COUNT(*) as cnt 
                    FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                    WHERE date = '{date_str}'
                """).collect()[0]['cnt']
                
                if date_count == 0:
                    print(f"   âš ï¸ {date_str}: ë°ì´í„° ì—†ìŒ, ê±´ë„ˆëœ€")
                    current_date += timedelta(days=1)
                    continue
                
                # í•´ë‹¹ ë‚ ì§œì˜ ë°°ì¹˜ ìˆ˜ ê³„ì‚°
                needed_batches = (date_count + self.batch_size - 1) // self.batch_size
                print(f"   ğŸ“Š {date_count:,}ê°œ ì´ë²¤íŠ¸ â†’ {needed_batches}ê°œ ë°°ì¹˜ (KST ìµœì í™”)")
                
                # ë‚ ì§œë³„ ë°°ì¹˜ ì²˜ë¦¬
                date_processed = 0
                for batch_num in range(needed_batches):
                    processed_count = self.create_kst_optimized_batch(date_str, batch_num)
                    
                    if processed_count > 0:
                        date_processed += processed_count
                        total_processed += processed_count
                        overall_batches += 1
                        
                        # ë©”ëª¨ë¦¬ ì•ˆì •ì„±ì„ ìœ„í•œ ëŒ€ê¸°
                        time.sleep(1)
                    else:
                        print(f"   âš ï¸ ë°°ì¹˜ {batch_num + 1} ì‹¤íŒ¨, ì¤‘ë‹¨")
                        break
                
                print(f"   âœ… {date_str} KST ì²˜ë¦¬ ì™„ë£Œ: {date_processed:,}ê°œ")
                
            except Exception as e:
                print(f"   âŒ {date_str} KST ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}...")
                break
                
            current_date += timedelta(days=1)
        
        return total_processed, overall_batches
        
    def analyze_kst_patterns(self):
        """KST íŒ¨í„´ ë¶„ì„ (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ í™œìš©)"""
        print(f"\nğŸ‡°ğŸ‡· KST íŒ¨í„´ ë¶„ì„...")
        
        try:
            # ì‹œê°„ë³„ íŒ¨í„´ (time_dim_keyì—ì„œ ì‹œê°„ ì¶”ì¶œ)
            print("â° KST ì‹œê°„ëŒ€ë³„ í™œë™ íŒ¨í„´:")
            hourly_pattern = self.spark.sql(f"""
            SELECT 
                (time_dim_key % 100) as kst_hour,
                COUNT(*) as total_events,
                COUNT(DISTINCT session_id) as unique_sessions,
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            WHERE time_dim_key >= 2025070100 AND time_dim_key <= 2025071023
            GROUP BY (time_dim_key % 100)
            ORDER BY total_events DESC
            LIMIT 10
            """).collect()
            
            for row in hourly_pattern:
                print(f"   {row.kst_hour:2d}ì‹œ: {row.total_events:,}ê°œ ì´ë²¤íŠ¸, {row.conversions}ê±´ ì „í™˜, ì°¸ì—¬ë„ {row.avg_engagement}")
            
            # ì¼ë³„ íŒ¨í„´
            print(f"\nğŸ“… KST ì¼ë³„ í™œë™ íŒ¨í„´:")
            daily_pattern = self.spark.sql(f"""
            SELECT 
                FLOOR(time_dim_key / 100) as kst_date,
                COUNT(*) as total_events,
                COUNT(DISTINCT session_id) as unique_sessions,
                SUM(event_count) as total_event_count,
                ROUND(AVG(engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            WHERE time_dim_key >= 2025070100 AND time_dim_key <= 2025071023
            GROUP BY FLOOR(time_dim_key / 100)
            ORDER BY kst_date
            """).collect()
            
            for row in daily_pattern:
                date_str = str(row.kst_date)
                formatted_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                print(f"   {formatted_date}: {row.total_events:,}ê°œ ì´ë²¤íŠ¸, {row.unique_sessions:,}ê°œ ì„¸ì…˜")
            
            # ì´ë²¤íŠ¸ ìœ í˜•ë³„ íŒ¨í„´
            print(f"\nğŸ¯ ì´ë²¤íŠ¸ ìœ í˜•ë³„ KST íŒ¨í„´:")
            event_pattern = self.spark.sql(f"""
            SELECT 
                event_dim_key,
                COUNT(*) as total_events,
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement,
                ROUND(AVG(session_duration_seconds), 0) as avg_session_duration
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            GROUP BY event_dim_key
            ORDER BY total_events DESC
            """).collect()
            
            event_types = {
                0: "ê¸°íƒ€", 1: "ì¸ì¦ì„±ê³µ", 2: "ëŒ“ê¸€ì‘ì„±", 3: "ë¶ë§ˆí¬", 
                4: "ë ˆì‹œí”¼í´ë¦­", 5: "ë ˆì‹œí”¼ê²€ìƒ‰", 6: "ë ˆì‹œí”¼ì¡°íšŒ", 7: "í˜ì´ì§€ì¡°íšŒ"
            }
            
            for row in event_pattern:
                event_name = event_types.get(row.event_dim_key, f"ìœ í˜•{row.event_dim_key}")
                print(f"   {event_name}: {row.total_events:,}ê°œ, ì „í™˜ {row.conversions}ê±´, ì°¸ì—¬ë„ {row.avg_engagement}")
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ KST ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return False
            
    def generate_dashboard_kpi(self):
        """ğŸ¯ ì‹¤ì‹œê°„ KPI ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
        print(f"\nğŸ¯ ì‹¤ì‹œê°„ KPI ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±...")
        
        try:
            # ì¼ê°„ í•µì‹¬ KPI
            kpi_stats = self.spark.sql(f"""
            SELECT 
                CURRENT_DATE() as report_date,
                COUNT(DISTINCT session_id) as dau,
                COUNT(*) as total_events,
                ROUND(
                    SUM(CASE WHEN is_conversion THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 
                    2
                ) as conversion_rate,
                ROUND(AVG(engagement_score), 2) as avg_engagement,
                ROUND(COUNT(*) * 1.0 / COUNT(DISTINCT session_id), 2) as events_per_session
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            WHERE time_dim_key >= CAST(DATE_FORMAT(CURRENT_DATE(), 'yyyyMMdd00') AS BIGINT)
            """).collect()[0]
            
            print("ğŸ“Š ì¼ê°„ í•µì‹¬ KPI:")
            print(f"   DAU: {kpi_stats.dau:,}ëª…")
            print(f"   ì´ ì´ë²¤íŠ¸: {kpi_stats.total_events:,}ê°œ")
            print(f"   ì „í™˜ìœ¨: {kpi_stats.conversion_rate}%")
            print(f"   í‰ê·  ì°¸ì—¬ë„: {kpi_stats.avg_engagement}")
            print(f"   ì„¸ì…˜ë‹¹ ì´ë²¤íŠ¸: {kpi_stats.events_per_session}ê°œ")
            
            return kpi_stats
            
        except Exception as e:
            print(f"âš ï¸ KPI ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
    
    def generate_conversion_funnel(self):
        """ğŸª ì „í™˜ í¼ë„ ì°¨íŠ¸ ë°ì´í„° ìƒì„±"""
        print(f"\nğŸª ì „í™˜ í¼ë„ ì°¨íŠ¸ ë°ì´í„° ìƒì„±...")
        
        try:
            funnel_data = self.spark.sql(f"""
            WITH user_journey AS (
                SELECT 
                    session_id,
                    MAX(CASE WHEN event_dim_key = 7 THEN 1 ELSE 0 END) as viewed_page,
                    MAX(CASE WHEN event_dim_key = 5 THEN 1 ELSE 0 END) as searched_recipe,
                    MAX(CASE WHEN event_dim_key = 4 THEN 1 ELSE 0 END) as clicked_recipe,
                    MAX(CASE WHEN event_dim_key = 3 THEN 1 ELSE 0 END) as bookmarked
                FROM {self.catalog_name}.{self.gold_database}.fact_user_events
                GROUP BY session_id
            )
            SELECT 
                'Page View' as stage, 1 as stage_order, SUM(viewed_page) as users
            FROM user_journey
            UNION ALL
            SELECT 'Recipe Search', 2, SUM(searched_recipe) FROM user_journey
            UNION ALL  
            SELECT 'Recipe Click', 3, SUM(clicked_recipe) FROM user_journey
            UNION ALL
            SELECT 'Bookmark', 4, SUM(bookmarked) FROM user_journey
            ORDER BY stage_order
            """).collect()
            
            print("ğŸ“Š ì „í™˜ í¼ë„ ë‹¨ê³„:")
            for row in funnel_data:
                print(f"   {row.stage}: {row.users:,}ëª…")
            
            return funnel_data
            
        except Exception as e:
            print(f"âš ï¸ í¼ë„ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
    
    def generate_recipe_performance(self):
        """ğŸ³ ë ˆì‹œí”¼ ì„±ê³¼ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
        print(f"\nğŸ³ ë ˆì‹œí”¼ ì„±ê³¼ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±...")
        
        try:
            recipe_stats = self.spark.sql(f"""
            SELECT 
                recipe_dim_key,
                COUNT(*) as interactions,
                COUNT(DISTINCT session_id) as unique_users,
                SUM(CASE WHEN is_conversion THEN 1 ELSE 0 END) as conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement,
                ROUND(
                    SUM(CASE WHEN is_conversion THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 
                    2
                ) as conversion_rate
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            WHERE recipe_dim_key > 0
            GROUP BY recipe_dim_key
            ORDER BY interactions DESC
            LIMIT 10
            """).collect()
            
            print("ğŸ“Š ì¸ê¸° ë ˆì‹œí”¼ TOP 10:")
            for i, row in enumerate(recipe_stats, 1):
                print(f"   {i:2d}. ë ˆì‹œí”¼ {row.recipe_dim_key}: {row.interactions:,}íšŒ ìƒí˜¸ì‘ìš©, ì „í™˜ìœ¨ {row.conversion_rate}%")
            
            return recipe_stats
            
        except Exception as e:
            print(f"âš ï¸ ë ˆì‹œí”¼ ì„±ê³¼ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None

    def validate_kst_fact_data(self):
        """KST Fact ë°ì´í„° ê²€ì¦"""
        print(f"\nğŸ” KST Fact ë°ì´í„° ê²€ì¦...")
        
        try:
            validation_stats = self.spark.sql(f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT time_dim_key) as unique_time_keys,
                COUNT(DISTINCT session_id) as unique_sessions,
                COUNT(DISTINCT event_dim_key) as unique_event_types,
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as total_conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement,
                MIN(time_dim_key) as min_time_key,
                MAX(time_dim_key) as max_time_key,
                MIN(created_at) as min_created_at,
                MAX(created_at) as max_created_at
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            print("ğŸ“Š KST Fact ê²€ì¦ ê²°ê³¼:")
            print(f"   ì´ ë ˆì½”ë“œ: {validation_stats.total_records:,}ê°œ")
            print(f"   ì‹œê°„ í‚¤ ë²”ìœ„: {validation_stats.min_time_key} ~ {validation_stats.max_time_key}")
            print(f"   ê³ ìœ  ì„¸ì…˜: {validation_stats.unique_sessions:,}ê°œ")
            print(f"   ì´ë²¤íŠ¸ ìœ í˜•: {validation_stats.unique_event_types}ê°œ")
            print(f"   ì´ ì „í™˜: {validation_stats.total_conversions:,}ê±´")
            print(f"   í‰ê·  ì°¸ì—¬ë„: {validation_stats.avg_engagement}")
            print(f"   ìƒì„± ì‹œê°„: {validation_stats.min_created_at} ~ {validation_stats.max_created_at}")
            
            return validation_stats.total_records > 0
            
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return False
            
    def execute_compatible_kst_processing(self, start_date: str = "2025-07-01", end_date: str = "2025-07-05"):
        """í˜¸í™˜ KST ì²˜ë¦¬ ì „ì²´ ì‹¤í–‰"""
        print("ğŸš€ ê¸°ì¡´ í˜¸í™˜ KST ìµœì í™” ì²˜ë¦¬ ì‹œì‘!")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. ë©”ëª¨ë¦¬ ì•ˆì „ SparkSession ìƒì„±
            self.create_memory_safe_spark_session()
            
            # 2. ê¸°ì¡´ ë°ì´í„° í´ë¦¬ì–´
            if not self.clear_and_rebuild_fact_table():
                print("âŒ í…Œì´ë¸” í´ë¦¬ì–´ ì‹¤íŒ¨")
                return False
            
            # 3. KST ê¸°ë°˜ ë‚ ì§œ ë²”ìœ„ ì²˜ë¦¬
            total_processed, total_batches = self.process_kst_date_range(start_date, end_date)
            
            # 4. ê²°ê³¼ ê²€ì¦
            success = self.validate_kst_fact_data()
            
            # 5. KST íŒ¨í„´ ë¶„ì„
            if success:
                self.analyze_kst_patterns()
                
                # ğŸ¯ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„± (ìƒˆë¡œ ì¶”ê°€)
                print(f"\nğŸ¨ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±...")
                self.generate_dashboard_kpi()
                self.generate_conversion_funnel() 
                self.generate_recipe_performance()
            
            end_time = time.time()
            elapsed_hours = (end_time - start_time) / 3600
            
            print(f"\n" + "=" * 60)
            if success and total_processed > 0:
                print(f"ğŸ‰ KST ìµœì í™” í˜¸í™˜ ì²˜ë¦¬ ì™„ë£Œ!")
                print(f"   ğŸ“… ì²˜ë¦¬ ê¸°ê°„: {start_date} ~ {end_date}")
                print(f"   ğŸ“Š ì²˜ë¦¬ëŸ‰: {total_processed:,}ê°œ ì´ë²¤íŠ¸")
                print(f"   ğŸ”„ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ")
                print(f"   â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_hours:.1f}ì‹œê°„")
                print(f"   ğŸ‡°ğŸ‡· KST ìµœì í™”: time_dim_keyì— í•œêµ­ì‹œê°„ ë°˜ì˜")
                print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì•ˆì „: 5,000ê°œ ë°°ì¹˜ë¡œ ì•ˆì • ì²˜ë¦¬")
                print(f"   ğŸ”„ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ í˜¸í™˜: ì™„ì „ í˜¸í™˜")
            else:
                print(f"âš ï¸ ë¶€ë¶„ ì™„ë£Œ ë˜ëŠ” ì‹¤íŒ¨")
                print(f"   ì²˜ë¦¬ëŸ‰: {total_processed:,}ê°œ")
                
            return success
            
        except Exception as e:
            print(f"âŒ KST í˜¸í™˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return False
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    processor = CompatibleKSTFactProcessor()
    
    # 5ì¼ê°„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì•ˆì „)
    success = processor.execute_compatible_kst_processing(
        start_date="2025-07-01", 
        end_date="2025-07-05"
    )
    
    if success:
        print(f"\nğŸ‰ KST í˜¸í™˜ ìµœì í™” ì™„ë£Œ!")
        print(f"   ğŸ‡°ğŸ‡· time_dim_keyì— í•œêµ­ ì‹œê°„ ë°˜ì˜")
        print(f"   ğŸ“Š KST ê¸°ë°˜ ì‹œê°„ëŒ€ë³„ ë¶„ì„ ê°€ëŠ¥")
        print(f"   ğŸ”’ ë©”ëª¨ë¦¬ ì•ˆì „ ë³´ì¥ (JVM í¬ë˜ì‹œ ì—†ìŒ)")
        print(f"   âœ… ê¸°ì¡´ ìŠ¤í‚¤ë§ˆì™€ ì™„ì „ í˜¸í™˜")
    else:
        print(f"\nâš ï¸ ì¶”ê°€ ìµœì í™” í•„ìš”")
