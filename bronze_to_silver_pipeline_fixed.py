# bronze_to_silver_pipeline.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, year, month, dayofmonth, hour, date_format, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType, DateType, TimestampType

def main():
    """
    S3 ëœë”© ì¡´ì˜ ì›ë³¸ íŒŒì¼ì„ ì½ì–´ Bronze, Silver ì•„ì´ìŠ¤ë²„ê·¸ í…Œì´ë¸”ì„ êµ¬ì¶•í•˜ëŠ”
    ì „ì²´ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ ETL íŒŒì´í”„ë¼ì¸.
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì • (ì„±ê³µí•œ ì„¤ì • ì ìš©)
        import os
        import subprocess
        
        # ì„±ê³µí•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['HADOOP_USER_NAME'] = 'root'
        os.environ['USER'] = 'root'
        os.environ['HOME'] = '/tmp'
        os.environ['JAVA_OPTS'] = '-Duser.name=root'
        # Ivy ì„¤ì •
        os.environ['IVY_HOME'] = '/tmp/.ivy2'
        os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.jars.ivy=/tmp/.ivy2 --conf spark.jars.packages= pyspark-shell'
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('/tmp/.ivy2', exist_ok=True)

        # -----------------------------------------------------------------------------
        # 1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„± (ì„±ê³µí•œ ì„¤ì • + S3 + Iceberg ì„¤ì •)
        # -----------------------------------------------------------------------------
        print("ğŸ”§ SparkSession ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        
        spark = SparkSession.builder \
            .appName("Bronze_to_Silver_Lakehouse_Pipeline") \
            .master("local[*]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "false") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.jars.ivy", "/tmp/.ivy2") \
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.3") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.hive_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.hive_catalog.type", "hive") \
            .config("spark.sql.catalog.hive_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.hive_catalog.warehouse", "s3a://reciping-user-event-logs/warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "")) \
            .config("spark.hadoop.fs.s3a.region", "ap-northeast-2") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.path.style.access", "false") \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSessionì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # -----------------------------------------------------------------------------
        # 2. ğŸ¥‰ Bronze Layer êµ¬ì¶• - ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ìƒì„±
        # -----------------------------------------------------------------------------
        print("\nğŸ¥‰ Bronze Layer êµ¬ì¶• ì‹œì‘...")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°)
        spark.sql("CREATE DATABASE IF NOT EXISTS hive_catalog.bronze_db")
        print("âœ… Bronze ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±/í™•ì¸ ì™„ë£Œ")
        
        # S3 ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°
        landing_zone_path = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
        print(f"ğŸ“‚ ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°: {landing_zone_path}")
        
        try:
            df_raw = spark.read.json(landing_zone_path)
            row_count = df_raw.count()
            print(f"âœ… ëœë”© ì¡´ ë°ì´í„° ë¡œë“œ ì„±ê³µ! í–‰ ìˆ˜: {row_count:,}")
            
            # Bronze í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            try:
                existing_df = spark.table("hive_catalog.bronze_db.raw_events")
                existing_count = existing_df.count()
                print(f"ğŸ“Š ê¸°ì¡´ Bronze í…Œì´ë¸” í–‰ ìˆ˜: {existing_count:,}")
            except:
                print("ğŸ“‹ Bronze í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                existing_count = 0
            
            # Bronze ì•„ì´ìŠ¤ë²„ê·¸ í…Œì´ë¸”ë¡œ ì €ì¥ (ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€)
            df_raw.write.mode("append").saveAsTable("hive_catalog.bronze_db.raw_events")
            print("âœ… Bronze í…Œì´ë¸” 'bronze_db.raw_events'ì— ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëœë”© ì¡´ì—ì„œ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("ğŸ’¡ upload_to_landing_zone.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            spark.stop()
            return

        # -----------------------------------------------------------------------------
        # 3. ğŸ¥ˆ Silver Layer êµ¬ì¶• (ê¸°ì¡´ ë³€í™˜ ë¡œì§ ì „ì²´ ë°˜ì˜)
        # -----------------------------------------------------------------------------
        print("\nğŸ¥ˆ Silver Layer êµ¬ì¶• ì‹œì‘...")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°)
        spark.sql("CREATE DATABASE IF NOT EXISTS hive_catalog.silver_db")
        print("âœ… Silver ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±/í™•ì¸ ì™„ë£Œ")
        
        # Bronze í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì½ìŠµë‹ˆë‹¤.
        df_bronze_table = spark.table("hive_catalog.bronze_db.raw_events")
        bronze_count = df_bronze_table.count()
        print(f"ğŸ“Š Bronze í…Œì´ë¸”ì—ì„œ {bronze_count:,}í–‰ì˜ ë°ì´í„°ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.")

        # --- 3.1. ê¸°ì¡´ê³¼ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆ ì •ì˜ ---
        context_schema = StructType([
            StructField("page", StructType([
                StructField("name", StringType(), True),
                StructField("url", StringType(), True),
                StructField("path", StringType(), True)
            ]), True),
            StructField("user_segment", StringType(), True),
            StructField("activity_level", StringType(), True),
            StructField("cooking_style", StringType(), True),
            StructField("ab_test", StructType([
                StructField("scenario", StringType(), True),
                StructField("group", StringType(), True),
                StructField("start_date", StringType(), True),
                StructField("end_date", StringType(), True)
            ]), True)
        ])

        event_properties_schema = StructType([
            StructField("page_name", StringType(), True), StructField("referrer", StringType(), True),
            StructField("path", StringType(), True), StructField("method", StringType(), True),
            StructField("type", StringType(), True), StructField("search_type", StringType(), True),
            StructField("search_keyword", StringType(), True), StructField("selected_filters", ArrayType(StringType()), True),
            StructField("result_count", IntegerType(), True), StructField("list_type", StringType(), True),
            StructField("displayed_recipe_ids", ArrayType(StringType()), True), StructField("recipe_id", StringType(), True),
            StructField("rank", IntegerType(), True), StructField("action", StringType(), True),
            StructField("comment_length", IntegerType(), True), StructField("category", StringType(), True),
            StructField("ingredient_count", IntegerType(), True), StructField("ad_id", StringType(), True),
            StructField("ad_type", StringType(), True), StructField("position", StringType(), True),
            StructField("target_url", StringType(), True)
        ])

        # --- 3.2. JSON íŒŒì‹± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ---
        print("ğŸ”§ JSON íŒŒì‹± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ì¤‘...")
        df_transformed = df_bronze_table \
            .withColumn("parsed_context", from_json(col("context"), context_schema)) \
            .withColumn("parsed_properties", from_json(col("event_properties"), event_properties_schema)) \
            .withColumn("timestamp_parsed", to_timestamp(col("timestamp"))) \
            .withColumn("date_parsed", col("date").cast(DateType())) \
            .drop("context", "event_properties")

        # --- 3.3. íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„± (KST ê¸°ì¤€) ---
        print("ğŸ“… íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„± ì¤‘...")
        df_with_partitions = df_transformed \
            .withColumn("year", year(col("timestamp_parsed"))) \
            .withColumn("month", month(col("timestamp_parsed"))) \
            .withColumn("day", dayofmonth(col("timestamp_parsed"))) \
            .withColumn("hour", hour(col("timestamp_parsed")))

        # --- 3.4. ì»¬ëŸ¼ í‰íƒ„í™” (ê¸°ì¡´ ë¡œì§ ì „ì²´ ë³µì›) ---
        print("ğŸ—‚ï¸ ì»¬ëŸ¼ í‰íƒ„í™” ì¤‘...")
        df_silver_flat = df_with_partitions.select(
            "event_id", "event_name", "user_id", "anonymous_id", "session_id",
            col("timestamp_parsed").alias("event_timestamp"), col("date_parsed").alias("event_date"),
            "year", "month", "day", "hour",
            col("parsed_context.page.name").alias("page_name"),
            col("parsed_context.page.url").alias("page_url"),
            col("parsed_context.page.path").alias("page_path"),
            col("parsed_context.user_segment").alias("user_segment"),
            col("parsed_context.activity_level").alias("activity_level"),
            col("parsed_context.cooking_style").alias("cooking_style"),
            col("parsed_context.ab_test.group").alias("ab_test_group"),
            col("parsed_context.ab_test.scenario").alias("ab_test_scenario"),
            col("parsed_properties.page_name").alias("prop_page_name"),
            col("parsed_properties.referrer").alias("prop_referrer"),
            col("parsed_properties.path").alias("prop_path"),
            col("parsed_properties.method").alias("prop_method"),
            col("parsed_properties.type").alias("prop_type"),
            col("parsed_properties.search_type").alias("prop_search_type"),
            col("parsed_properties.search_keyword").alias("prop_search_keyword"),
            col("parsed_properties.selected_filters").alias("prop_selected_filters"),
            col("parsed_properties.result_count").alias("prop_result_count"),
            col("parsed_properties.list_type").alias("prop_list_type"),
            col("parsed_properties.displayed_recipe_ids").alias("prop_displayed_recipe_ids"),
            col("parsed_properties.recipe_id").cast(LongType()).alias("prop_recipe_id"),
            col("parsed_properties.rank").alias("prop_rank"),
            col("parsed_properties.action").alias("prop_action"),
            col("parsed_properties.comment_length").alias("prop_comment_length"),
            col("parsed_properties.category").alias("prop_category"),
            col("parsed_properties.ingredient_count").alias("prop_ingredient_count"),
            col("parsed_properties.ad_id").alias("prop_ad_id"),
            col("parsed_properties.ad_type").alias("prop_ad_type"),
            col("parsed_properties.position").alias("prop_position"),
            col("parsed_properties.target_url").alias("prop_target_url")
        )
        
        # --- 3.5. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ---
        print("ğŸ” ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì¤‘...")
        df_silver_final = df_silver_flat.filter(col("event_id").isNotNull()).dropDuplicates(["event_id"])
        final_count = df_silver_final.count()
        print(f"âœ… ì»¬ëŸ¼ í‰íƒ„í™” ë° ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì™„ë£Œ. ìµœì¢… í–‰ ìˆ˜: {final_count:,}")
        
        # Silver ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        print("\nğŸ“Š Silver Layer ìƒ˜í”Œ ë°ì´í„° (ìƒìœ„ 3í–‰):")
        df_silver_final.show(3, truncate=True)
        
        # ì´ë²¤íŠ¸ë³„ ë¶„í¬ í™•ì¸
        print("\nğŸ“Š ì´ë²¤íŠ¸ë³„ ë¶„í¬:")
        df_silver_final.groupBy('event_name').count().orderBy('count', ascending=False).show(10)

        # --- 3.6. Silver ì•„ì´ìŠ¤ë²„ê·¸ í…Œì´ë¸”ë¡œ ì €ì¥ ---
        print("\nğŸ’¾ Silver ì•„ì´ìŠ¤ë²„ê·¸ í…Œì´ë¸”ë¡œ ì €ì¥ ì¤‘...")
        (df_silver_final.write
         .mode("overwrite")
         .partitionBy("year", "month", "day") # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ hour íŒŒí‹°ì…˜ ì œê±°
         .saveAsTable("hive_catalog.silver_db.cleaned_events"))
        print("âœ… Silver í…Œì´ë¸” 'silver_db.cleaned_events' ìƒì„± ì™„ë£Œ")

        # -----------------------------------------------------------------------------
        # 4. ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        spark.stop()
        print("âœ… SparkSessionì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # ìŠ¤íŒŒí¬ ì„¸ì…˜ì´ ìˆë‹¤ë©´ ì¢…ë£Œ
        try:
            spark.stop()
        except:
            pass

# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
if __name__ == "__main__":
    main()
