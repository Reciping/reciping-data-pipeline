#!/usr/bin/env python3
"""
ì™„ì „í•œ ì†”ë£¨ì…˜ - ì ì§„ì  í™•ì¥ ë²„ì „
ë©”ëª¨ë¦¬ ì•ˆì „í•œ ê¸°ë°˜ì—ì„œ ì™„ì „í•œ Star Schema êµ¬ì¶•
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

class CompleteGoldLayer:
    """ì™„ì „í•œ Gold Layer - ì ì§„ì  í™•ì¥ ë²„ì „"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        
    def create_optimized_spark_session(self):
        """ìµœì í™”ëœ SparkSession ìƒì„±"""
        print("ğŸ§Š ì™„ì „í•œ ì†”ë£¨ì…˜ìš© SparkSession ìƒì„± ì¤‘...")
        
        self.spark = SparkSession.builder \
            .appName("CompleteGoldLayer") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "3g") \
            .config("spark.executor.memory", "3g") \
            .config("spark.driver.maxResultSize", "1g") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.initialPartitionNum", "8") \
            .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.kryo.unsafe", "true") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
            .config("spark.sql.adaptive.localShuffleReader.enabled", "true") \
            .config("spark.sql.adaptive.skewJoin.enabled", "true") \
            .config("spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold", "64MB") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… ì™„ì „í•œ ì†”ë£¨ì…˜ìš© SparkSession ìƒì„± ì™„ë£Œ!")
        
    def create_complete_dimensions(self):
        """ì™„ì „í•œ Dimension í…Œì´ë¸”ë“¤ ìƒì„±"""
        print("\nğŸŒŸ ì™„ì „í•œ Dimension í…Œì´ë¸” ìƒì„± ì¤‘...")
        
        # User Dimension ì™„ì„±
        print("ğŸ‘¥ ì‚¬ìš©ì Dimension ì™„ì„±...")
        user_dimension_query = f"""
        WITH user_stats AS (
            SELECT 
                user_id,
                user_segment,
                cooking_style,
                ab_test_group,
                MIN(date) as first_seen_date,
                MAX(date) as last_activity_date,
                COUNT(DISTINCT session_id) as total_sessions,
                COUNT(CASE WHEN event_name = 'view_recipe' THEN 1 END) as total_recipe_views,
                COUNT(*) as total_events,
                COUNT(CASE WHEN event_name = 'auth_success' THEN 1 END) as auth_events
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE user_id IS NOT NULL
            GROUP BY user_id, user_segment, cooking_style, ab_test_group
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.dim_users
        SELECT 
            ROW_NUMBER() OVER (ORDER BY first_seen_date, user_id) as user_dim_key,
            user_id,
            user_segment,
            cooking_style,
            ab_test_group,
            first_seen_date,
            last_activity_date,
            total_sessions,
            total_recipe_views,
            CASE 
                WHEN total_events >= 100 THEN 'Power User'
                WHEN total_events >= 20 THEN 'Active User'
                WHEN total_events >= 5 THEN 'Regular User'
                ELSE 'New User'
            END as user_tier,
            total_events * 0.1 as lifetime_value,
            first_seen_date as effective_date,
            NULL as expiry_date,
            TRUE as is_current,
            CURRENT_TIMESTAMP() as created_at,
            CURRENT_TIMESTAMP() as updated_at
        FROM user_stats
        """
        
        self.spark.sql(user_dimension_query)
        
        # Recipe Dimension ì™„ì„±
        print("ğŸ³ ë ˆì‹œí”¼ Dimension ì™„ì„±...")
        recipe_dimension_query = f"""
        WITH recipe_stats AS (
            SELECT DISTINCT
                prop_recipe_id as recipe_id,
                COUNT(*) OVER (PARTITION BY prop_recipe_id) as view_count
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE prop_recipe_id IS NOT NULL AND prop_recipe_id > 0
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.dim_recipes
        SELECT 
            ROW_NUMBER() OVER (ORDER BY view_count DESC, recipe_id) as recipe_dim_key,
            recipe_id,
            CASE 
                WHEN view_count >= 1000 THEN 'Popular'
                WHEN view_count >= 100 THEN 'Trending'
                ELSE 'Standard'
            END as recipe_category,
            CASE 
                WHEN view_count >= 1000 THEN 3
                WHEN view_count >= 100 THEN 5
                ELSE 7
            END as ingredient_count,
            CASE 
                WHEN view_count >= 1000 THEN 'Easy'
                WHEN view_count >= 100 THEN 'Medium'
                ELSE 'Hard'
            END as difficulty_level,
            'Korean' as cuisine_type,
            30 as prep_time_minutes,
            CASE WHEN view_count >= 1000 THEN TRUE ELSE FALSE END as is_premium,
            CURRENT_TIMESTAMP() as created_at,
            CURRENT_TIMESTAMP() as updated_at
        FROM recipe_stats
        
        UNION ALL
        
        SELECT 
            0 as recipe_dim_key,
            NULL as recipe_id,
            'N/A' as recipe_category,
            0 as ingredient_count,
            'N/A' as difficulty_level,
            'N/A' as cuisine_type,
            0 as prep_time_minutes,
            FALSE as is_premium,
            CURRENT_TIMESTAMP() as created_at,
            CURRENT_TIMESTAMP() as updated_at
        """
        
        self.spark.sql(recipe_dimension_query)
        
        # Page Dimension ì™„ì„±
        print("ğŸ“± í˜ì´ì§€ Dimension ì™„ì„±...")
        page_dimension_query = f"""
        WITH page_stats AS (
            SELECT 
                page_name,
                page_url,
                COUNT(*) as page_views
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE page_name IS NOT NULL
            GROUP BY page_name, page_url
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.dim_pages
        SELECT 
            ROW_NUMBER() OVER (ORDER BY page_views DESC) as page_dim_key,
            page_name,
            page_url,
            CASE 
                WHEN page_name LIKE '%recipe%' THEN 'Recipe'
                WHEN page_name LIKE '%search%' THEN 'Search'
                WHEN page_name LIKE '%list%' THEN 'Browse'
                WHEN page_name LIKE '%auth%' THEN 'Authentication'
                ELSE 'Other'
            END as page_category,
            CASE 
                WHEN page_name = 'home' THEN 'Awareness'
                WHEN page_name LIKE '%list%' THEN 'Interest'
                WHEN page_name LIKE '%recipe%' THEN 'Consideration'
                WHEN page_name LIKE '%auth%' THEN 'Conversion'
                ELSE 'Other'
            END as funnel_stage,
            CASE WHEN page_url LIKE '%mobile%' THEN TRUE ELSE FALSE END as is_mobile,
            CURRENT_TIMESTAMP() as created_at
        FROM page_stats
        
        UNION ALL
        
        SELECT 
            0 as page_dim_key,
            'Unknown' as page_name,
            'Unknown' as page_url,
            'Unknown' as page_category,
            'Unknown' as funnel_stage,
            FALSE as is_mobile,
            CURRENT_TIMESTAMP() as created_at
        """
        
        self.spark.sql(page_dimension_query)
        
        # Event Dimension ì™„ì„±
        print("ğŸ¬ ì´ë²¤íŠ¸ Dimension ì™„ì„±...")
        event_dimension_query = f"""
        WITH event_stats AS (
            SELECT 
                event_name,
                COUNT(*) as event_count
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            GROUP BY event_name
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.dim_events
        SELECT 
            ROW_NUMBER() OVER (ORDER BY event_count DESC) as event_dim_key,
            event_name,
            CASE 
                WHEN event_name LIKE '%view%' THEN 'Engagement'
                WHEN event_name LIKE '%click%' THEN 'Interaction'
                WHEN event_name LIKE '%search%' THEN 'Discovery'
                WHEN event_name LIKE '%auth%' THEN 'Conversion'
                ELSE 'Other'
            END as event_category,
            CASE 
                WHEN event_name = 'auth_success' THEN 10.0
                WHEN event_name = 'click_recipe' THEN 5.0
                WHEN event_name = 'search_recipe' THEN 3.0
                WHEN event_name = 'view_page' THEN 1.0
                ELSE 1.0
            END as conversion_value,
            CASE WHEN event_name IN ('auth_success', 'create_comment', 'click_bookmark') THEN TRUE ELSE FALSE END as is_conversion_event,
            1.0 as event_weight,
            CURRENT_TIMESTAMP() as created_at
        FROM event_stats
        """
        
        self.spark.sql(event_dimension_query)
        
        print("âœ… ëª¨ë“  Dimension í…Œì´ë¸” ì™„ì„±!")
        
    def create_complete_fact_table_batch(self):
        """ì™„ì „í•œ Fact í…Œì´ë¸”ì„ ë°°ì¹˜ë¡œ ìƒì„±"""
        print("\nğŸ“Š ì™„ì „í•œ Fact í…Œì´ë¸” ë°°ì¹˜ ìƒì„±...")
        
        # 7ì›” 1-15ì¼ ë°°ì¹˜ (ì²« ë²ˆì§¸ ì ˆë°˜)
        print("ğŸ”„ ì²« ë²ˆì§¸ ë°°ì¹˜ (7ì›” 1-15ì¼) ì²˜ë¦¬ ì¤‘...")
        
        batch1_query = f"""
        WITH silver_batch1 AS (
            SELECT 
                event_id,
                user_id,
                session_id,
                anonymous_id,
                event_name,
                page_name,
                prop_recipe_id,
                utc_timestamp,
                date,
                prop_action
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE date >= '2025-07-01' AND date <= '2025-07-15'
        ),
        enriched_batch1 AS (
            SELECT 
                s.event_id,
                COALESCE(u.user_dim_key, 0) as user_dim_key,
                CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                COALESCE(r.recipe_dim_key, 0) as recipe_dim_key,
                COALESCE(p.page_dim_key, 0) as page_dim_key,
                COALESCE(e.event_dim_key, 1) as event_dim_key,
                
                1 as event_count,
                
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 2
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[1] AS BIGINT), 0)
                    ELSE 0
                END as session_duration_seconds,
                
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 3
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[2] AS BIGINT), 30)
                    ELSE 30
                END as page_view_duration_seconds,
                
                COALESCE(e.is_conversion_event, FALSE) as is_conversion,
                COALESCE(e.conversion_value, 1.0) as conversion_value,
                
                CASE 
                    WHEN s.event_name = 'auth_success' THEN 10.0
                    WHEN s.event_name = 'create_comment' THEN 9.0
                    WHEN s.event_name = 'click_bookmark' THEN 8.0
                    WHEN s.event_name = 'click_recipe' THEN 7.0
                    WHEN s.event_name = 'search_recipe' THEN 5.0
                    WHEN s.event_name = 'view_recipe' THEN 4.0
                    WHEN s.event_name = 'view_page' THEN 2.0
                    ELSE 1.0
                END as engagement_score,
                
                s.session_id,
                s.anonymous_id,
                
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM silver_batch1 s
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_users u ON s.user_id = u.user_id AND u.is_current = TRUE
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r ON s.prop_recipe_id = r.recipe_id
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p ON s.page_name = p.page_name AND p.page_name != 'Unknown'
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_events e ON s.event_name = e.event_name
            
            WHERE s.event_id IS NOT NULL
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
        SELECT * FROM enriched_batch1
        """
        
        try:
            self.spark.sql(batch1_query)
            
            # ì²« ë²ˆì§¸ ë°°ì¹˜ ê²°ê³¼ í™•ì¸
            batch1_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"âœ… ì²« ë²ˆì§¸ ë°°ì¹˜ ì™„ë£Œ: {batch1_count:,}ê°œ ë ˆì½”ë“œ")
            
            # ë‘ ë²ˆì§¸ ë°°ì¹˜ ì¶”ê°€ (7ì›” 16-31ì¼)
            print("ğŸ”„ ë‘ ë²ˆì§¸ ë°°ì¹˜ (7ì›” 16-31ì¼) ì¶”ê°€ ì¤‘...")
            
            batch2_query = f"""
            INSERT INTO {self.catalog_name}.{self.gold_database}.fact_user_events
            SELECT 
                s.event_id,
                COALESCE(u.user_dim_key, 0) as user_dim_key,
                CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                COALESCE(r.recipe_dim_key, 0) as recipe_dim_key,
                COALESCE(p.page_dim_key, 0) as page_dim_key,
                COALESCE(e.event_dim_key, 1) as event_dim_key,
                
                1 as event_count,
                
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 2
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[1] AS BIGINT), 0)
                    ELSE 0
                END as session_duration_seconds,
                
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 3
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[2] AS BIGINT), 30)
                    ELSE 30
                END as page_view_duration_seconds,
                
                COALESCE(e.is_conversion_event, FALSE) as is_conversion,
                COALESCE(e.conversion_value, 1.0) as conversion_value,
                
                CASE 
                    WHEN s.event_name = 'auth_success' THEN 10.0
                    WHEN s.event_name = 'create_comment' THEN 9.0
                    WHEN s.event_name = 'click_bookmark' THEN 8.0
                    WHEN s.event_name = 'click_recipe' THEN 7.0
                    WHEN s.event_name = 'search_recipe' THEN 5.0
                    WHEN s.event_name = 'view_recipe' THEN 4.0
                    WHEN s.event_name = 'view_page' THEN 2.0
                    ELSE 1.0
                END as engagement_score,
                
                s.session_id,
                s.anonymous_id,
                
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver s
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_users u ON s.user_id = u.user_id AND u.is_current = TRUE
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r ON s.prop_recipe_id = r.recipe_id
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p ON s.page_name = p.page_name AND p.page_name != 'Unknown'
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_events e ON s.event_name = e.event_name
            
            WHERE s.date >= '2025-07-16' AND s.date <= '2025-07-31'
            AND s.event_id IS NOT NULL
            """
            
            self.spark.sql(batch2_query)
            
            # ìµœì¢… ê²°ê³¼ í™•ì¸
            final_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"âœ… ë‘ ë²ˆì§¸ ë°°ì¹˜ ì™„ë£Œ: ì´ {final_count:,}ê°œ ë ˆì½”ë“œ")
            
        except Exception as e:
            print(f"âŒ ì™„ì „í•œ Fact í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
    def validate_complete_solution(self):
        """ì™„ì „í•œ ì†”ë£¨ì…˜ ê²€ì¦"""
        print("\nğŸ† ì™„ì „í•œ ì†”ë£¨ì…˜ ê²€ì¦...")
        
        try:
            # ì¢…í•© í†µê³„
            comprehensive_stats = self.spark.sql(f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT user_dim_key) as unique_users,
                COUNT(DISTINCT recipe_dim_key) as unique_recipes,
                COUNT(DISTINCT page_dim_key) as unique_pages,
                COUNT(DISTINCT event_dim_key) as unique_events,
                COUNT(DISTINCT session_id) as unique_sessions,
                
                SUM(CASE WHEN user_dim_key > 0 THEN 1 ELSE 0 END) as mapped_users,
                SUM(CASE WHEN recipe_dim_key > 0 THEN 1 ELSE 0 END) as mapped_recipes,
                SUM(CASE WHEN page_dim_key > 0 THEN 1 ELSE 0 END) as mapped_pages,
                
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as total_conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement,
                SUM(conversion_value) as total_conversion_value
                
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            print("ğŸ¯ ì™„ì „í•œ ì†”ë£¨ì…˜ ì„±ê³¼:")
            print(f"   ğŸ“Š ì´ ì´ë²¤íŠ¸: {comprehensive_stats['total_records']:,}ê°œ")
            print(f"   ğŸ‘¥ ê³ ìœ  ì‚¬ìš©ì: {comprehensive_stats['unique_users']:,}ëª…")
            print(f"   ğŸ³ ê³ ìœ  ë ˆì‹œí”¼: {comprehensive_stats['unique_recipes']:,}ê°œ")
            print(f"   ğŸ“± ê³ ìœ  í˜ì´ì§€: {comprehensive_stats['unique_pages']}ê°œ")
            print(f"   ğŸ¬ ê³ ìœ  ì´ë²¤íŠ¸: {comprehensive_stats['unique_events']}ê°œ")
            print(f"   ğŸ”— ê³ ìœ  ì„¸ì…˜: {comprehensive_stats['unique_sessions']:,}ê°œ")
            
            # ë§¤í•‘ ì„±ê³µë¥ 
            user_mapping_pct = (comprehensive_stats['mapped_users'] / comprehensive_stats['total_records']) * 100
            recipe_mapping_pct = (comprehensive_stats['mapped_recipes'] / comprehensive_stats['total_records']) * 100
            page_mapping_pct = (comprehensive_stats['mapped_pages'] / comprehensive_stats['total_records']) * 100
            
            print(f"\nğŸ“ˆ ì°¨ì› ë§¤í•‘ ì„±ê³µë¥ :")
            print(f"   ğŸ‘¥ ì‚¬ìš©ì: {user_mapping_pct:.1f}% ({comprehensive_stats['mapped_users']:,}ê°œ)")
            print(f"   ğŸ³ ë ˆì‹œí”¼: {recipe_mapping_pct:.1f}% ({comprehensive_stats['mapped_recipes']:,}ê°œ)")
            print(f"   ğŸ“± í˜ì´ì§€: {page_mapping_pct:.1f}% ({comprehensive_stats['mapped_pages']:,}ê°œ)")
            
            print(f"\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­:")
            print(f"   ğŸ¯ ì´ ì „í™˜: {comprehensive_stats['total_conversions']:,}ê±´")
            print(f"   â­ í‰ê·  ì°¸ì—¬ë„: {comprehensive_stats['avg_engagement']}")
            print(f"   ğŸ’° ì´ ì „í™˜ ê°€ì¹˜: ${comprehensive_stats['total_conversion_value']:,.2f}")
            
            # ë¶„ì„ ë°ëª¨
            if user_mapping_pct >= 50 and recipe_mapping_pct >= 30:
                print(f"\nğŸ‰ ì™„ì „í•œ ì†”ë£¨ì…˜ ì„±ê³µ!")
                self.demonstrate_complete_analytics()
            else:
                print(f"\nâ­ ë¶€ë¶„ì  ì„±ê³µ! ê¸°ë³¸ ë¶„ì„ ê°€ëŠ¥")
                
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            
    def demonstrate_complete_analytics(self):
        """ì™„ì „í•œ ë¶„ì„ ë°ëª¨"""
        print(f"\nğŸš€ ì™„ì „í•œ ì†”ë£¨ì…˜ ë¶„ì„ ë°ëª¨...")
        
        try:
            # 1. ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¶„ì„
            print("   ğŸ“Š ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ë³„ ì„±ê³¼:")
            segment_analysis = self.spark.sql(f"""
            SELECT 
                u.user_tier,
                COUNT(DISTINCT f.user_dim_key) as users,
                COUNT(*) as total_events,
                SUM(CASE WHEN f.is_conversion THEN 1 ELSE 0 END) as conversions,
                ROUND(AVG(f.engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
            JOIN {self.catalog_name}.{self.gold_database}.dim_users u ON f.user_dim_key = u.user_dim_key
            WHERE f.user_dim_key > 0
            GROUP BY u.user_tier
            ORDER BY total_events DESC
            """).collect()
            
            for row in segment_analysis:
                conversion_rate = (row['conversions'] / row['total_events']) * 100 if row['total_events'] > 0 else 0
                print(f"     {row['user_tier']}: {row['users']}ëª…, {conversion_rate:.1f}% ì „í™˜ìœ¨, {row['avg_engagement']}ì  ì°¸ì—¬ë„")
            
            # 2. A/B í…ŒìŠ¤íŠ¸ ë¶„ì„
            print("   ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ê·¸ë£¹ë³„ ì„±ê³¼:")
            ab_analysis = self.spark.sql(f"""
            SELECT 
                u.ab_test_group,
                COUNT(DISTINCT f.user_dim_key) as users,
                SUM(CASE WHEN f.is_conversion THEN 1 ELSE 0 END) as conversions,
                COUNT(*) as total_events,
                ROUND(SUM(CASE WHEN f.is_conversion THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as conversion_rate
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
            JOIN {self.catalog_name}.{self.gold_database}.dim_users u ON f.user_dim_key = u.user_dim_key
            WHERE u.ab_test_group IS NOT NULL
            GROUP BY u.ab_test_group
            ORDER BY conversion_rate DESC
            """).collect()
            
            for row in ab_analysis:
                print(f"     {row['ab_test_group']}: {row['conversion_rate']}% ì „í™˜ìœ¨ ({row['conversions']}ê±´/{row['users']}ëª…)")
            
            # 3. ë ˆì‹œí”¼ ì„±ê³¼ ë¶„ì„
            print("   ğŸ³ ì¸ê¸° ë ˆì‹œí”¼ TOP 5:")
            recipe_analysis = self.spark.sql(f"""
            SELECT 
                r.recipe_category,
                f.recipe_dim_key,
                COUNT(DISTINCT f.user_dim_key) as unique_viewers,
                COUNT(*) as total_views,
                ROUND(AVG(f.engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
            JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r ON f.recipe_dim_key = r.recipe_dim_key
            WHERE f.recipe_dim_key > 0
            GROUP BY r.recipe_category, f.recipe_dim_key
            ORDER BY unique_viewers DESC
            LIMIT 5
            """).collect()
            
            for row in recipe_analysis:
                print(f"     {row['recipe_category']} ë ˆì‹œí”¼ #{row['recipe_dim_key']}: {row['unique_viewers']}ëª… ì¡°íšŒ, {row['avg_engagement']}ì  ì°¸ì—¬ë„")
            
            print(f"\nâœ… ì™„ì „í•œ ì†”ë£¨ì…˜ìœ¼ë¡œ ëª¨ë“  ê³ ê¸‰ ë¶„ì„ ì™„ë£Œ!")
            print(f"   ğŸ¯ 10ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ + A/B í…ŒìŠ¤íŠ¸ ë¶„ì„ ê°€ëŠ¥")
            print(f"   ğŸš€ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ì¤€ë¹„ ì™„ë£Œ")
            print(f"   ğŸ’¡ ê°œì¸í™” ì¶”ì²œ ì—”ì§„ ë°ì´í„° ì™„ë¹„")
            
        except Exception as e:
            print(f"   âš ï¸ ì¼ë¶€ ê³ ê¸‰ ë¶„ì„ ì œí•œ: {str(e)}")
            
    def execute_complete_solution(self):
        """ì™„ì „í•œ ì†”ë£¨ì…˜ ì „ì²´ ì‹¤í–‰"""
        print("ğŸš€ ì™„ì „í•œ ì†”ë£¨ì…˜ ì‹¤í–‰ ì‹œì‘...")
        print("=" * 60)
        
        try:
            # 1. SparkSession ìƒì„±
            self.create_optimized_spark_session()
            
            # 2. ì™„ì „í•œ Dimensions ìƒì„±
            self.create_complete_dimensions()
            
            # 3. ì™„ì „í•œ Fact í…Œì´ë¸” ìƒì„±
            self.create_complete_fact_table_batch()
            
            # 4. ê²€ì¦
            self.validate_complete_solution()
            
            print("\nğŸ‰ ì™„ì „í•œ ì†”ë£¨ì…˜ êµ¬ì¶• ì™„ë£Œ!")
            print("   âœ… ë©”ëª¨ë¦¬ í¬ë˜ì‹œ ì—†ì´ ì•ˆì •ì  ì‹¤í–‰")
            print("   âœ… ëª¨ë“  10ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ + A/B í…ŒìŠ¤íŠ¸ ë¶„ì„ ê°€ëŠ¥")
            print("   âœ… ì™„ì „í•œ Star Schema êµ¬ì¶• ì™„ë£Œ")
            print("   âœ… ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë° ì¶”ì²œ ì‹œìŠ¤í…œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì™„ì „í•œ ì†”ë£¨ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    complete_gold = CompleteGoldLayer()
    complete_gold.execute_complete_solution()
