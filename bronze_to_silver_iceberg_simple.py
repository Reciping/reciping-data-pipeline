# bronze_to_silver_iceberg_simple.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, year, month, dayofmonth, hour, date_format, to_timestamp, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType, DateType, TimestampType

def main():
    """
    Iceberg ê³ ê¸‰ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ Bronze, Silver ë ˆì´ì–´ë¥¼ êµ¬ì¶•í•˜ëŠ”
    ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ ETL íŒŒì´í”„ë¼ì¸ (Hive Metastore ì—†ì´ Hadoop Catalog ì‚¬ìš©).
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì • (ì„±ê³µí•œ ì„¤ì • ì ìš©)
        import os
        import subprocess
        
        # ì„±ê³µí•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (JVM ìµœì í™”)
        os.environ['HADOOP_USER_NAME'] = 'root'
        os.environ['USER'] = 'root'
        os.environ['HOME'] = '/tmp'
        os.environ['JAVA_OPTS'] = '-Duser.name=root -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:MaxGCPauseMillis=200'
        # Ivy ì„¤ì •
        os.environ['IVY_HOME'] = '/tmp/.ivy2'
        os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.jars.ivy=/tmp/.ivy2 pyspark-shell'
        
        # AWS ìê²© ì¦ëª… í™•ì¸
        aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
        aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
        
        if not aws_access_key or not aws_secret_key:
            print("âš ï¸ AWS ìê²© ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Docker Compose í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            print(f"AWS_ACCESS_KEY_ID: {'âœ… ì„¤ì •ë¨' if aws_access_key else 'âŒ ì—†ìŒ'}")
            print(f"AWS_SECRET_ACCESS_KEY: {'âœ… ì„¤ì •ë¨' if aws_secret_key else 'âŒ ì—†ìŒ'}")
        else:
            print("âœ… AWS ìê²© ì¦ëª…ì´ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('/tmp/.ivy2', exist_ok=True)

        # -----------------------------------------------------------------------------
        # 1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„± (Iceberg + Hadoop Catalog)
        # -----------------------------------------------------------------------------
        print("ğŸ”§ SparkSession with Iceberg (Hadoop Catalog) ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        
        spark = SparkSession.builder \
            .appName("Bronze_to_Silver_Iceberg_Simple_Pipeline") \
            .master("local[2]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.jars.ivy", "/tmp/.ivy2") \
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,org.apache.hadoop:hadoop-aws:3.3.4") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hadoop") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/warehouse/iceberg") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "")) \
            .config("spark.hadoop.fs.s3a.region", "ap-northeast-2") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.path.style.access", "false") \
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true") \
            .config("spark.hadoop.fs.s3a.fast.upload", "true") \
            .config("spark.hadoop.fs.s3a.block.size", "134217728") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.driver.maxResultSize", "1g") \
            .config("spark.sql.shuffle.partitions", "8") \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSession with Iceberg (Hadoop Catalog)ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # -----------------------------------------------------------------------------
        # 2. ğŸ—ï¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ë° ê´€ë¦¬
        # -----------------------------------------------------------------------------
        print("\nğŸ—ï¸ Iceberg ë„¤ì„ìŠ¤í˜ì´ìŠ¤ êµ¬ì„±...")
        
        # Bronze, Silver, Gold ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
        try:
            spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.bronze")
            spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.silver") 
            spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.gold")
            print("âœ… Iceberg ë„¤ì„ìŠ¤í˜ì´ìŠ¤ êµ¬ì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}")
        
        # í˜„ì¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª©ë¡ í™•ì¸
        print("\nğŸ“‹ í˜„ì¬ Iceberg ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª©ë¡:")
        spark.sql("SHOW NAMESPACES IN iceberg_catalog").show()

        # -----------------------------------------------------------------------------
        # 3. ğŸ¥‰ Bronze Layer - Iceberg í…Œì´ë¸” êµ¬ì¶•
        # -----------------------------------------------------------------------------
        print("\nğŸ¥‰ Bronze Layer (Iceberg) êµ¬ì¶• ì‹œì‘...")
        
        # S3 ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°
        landing_zone_path = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
        print(f"ğŸ“‚ ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°: {landing_zone_path}")
        
        try:
            df_raw = spark.read.json(landing_zone_path)
            row_count = df_raw.count()
            print(f"âœ… ëœë”© ì¡´ ë°ì´í„° ë¡œë“œ ì„±ê³µ! í–‰ ìˆ˜: {row_count:,}")
            
            # ì²˜ë¦¬ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            df_raw_with_metadata = df_raw.withColumn("ingestion_timestamp", current_timestamp())
            
            # Bronze Iceberg í…Œì´ë¸” ìƒì„± (ì•ˆì „í•œ ë°©ì‹)
            try:
                # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
                spark.sql("DROP TABLE IF EXISTS iceberg_catalog.bronze.raw_events")
                
                # ì‘ì€ ìƒ˜í”Œë¶€í„° í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ë¶€ë‹´ ê°ì†Œ)
                sample_size = min(50000, row_count)  # ìµœëŒ€ 5ë§Œ ê°œ ë ˆì½”ë“œë¡œ ì‹œì‘
                df_sample = df_raw_with_metadata.limit(sample_size)
                
                print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±: {sample_size:,}í–‰")
                
                df_sample.writeTo("iceberg_catalog.bronze.raw_events") \
                    .tableProperty("format-version", "2") \
                    .tableProperty("write.target-file-size-bytes", "67108864") \
                    .tableProperty("write.parquet.compression-codec", "snappy") \
                    .create()
                
                print("âœ… Bronze Iceberg í…Œì´ë¸” 'iceberg_catalog.bronze.raw_events' ìƒì„± ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ Bronze í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
                return
            
        except Exception as e:
            print(f"âŒ ëœë”© ì¡´ì—ì„œ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("ğŸ’¡ upload_to_landing_zone.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            spark.stop()
            return

        # -----------------------------------------------------------------------------
        # 4. ğŸ¥ˆ Silver Layer - Iceberg í…Œì´ë¸” êµ¬ì¶• (ê³ ê¸‰ ë³€í™˜)
        # -----------------------------------------------------------------------------
        print("\nğŸ¥ˆ Silver Layer (Iceberg) êµ¬ì¶• ì‹œì‘...")
        
        # Bronze Iceberg í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì½ê¸°
        df_bronze = spark.table("iceberg_catalog.bronze.raw_events")
        bronze_count = df_bronze.count()
        print(f"ğŸ“Š Bronzeì—ì„œ {bronze_count:,}í–‰ì˜ ë°ì´í„°ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.")

        # --- 4.1. ìŠ¤í‚¤ë§ˆ ì •ì˜ (ê°œì„ ëœ ë²„ì „) ---
        context_schema = StructType([
            StructField("page", StructType([
                StructField("name", StringType(), True),
                StructField("url", StringType(), True),
                StructField("path", StringType(), True)
            ]), True),
            StructField("user_segment", StringType(), True),
            StructField("activity_level", StringType(), True),
            StructField("cooking_style", StringType(), True),
            StructField("ab_test", StructType([
                StructField("scenario", StringType(), True),
                StructField("group", StringType(), True),
                StructField("start_date", StringType(), True),
                StructField("end_date", StringType(), True)
            ]), True)
        ])

        event_properties_schema = StructType([
            StructField("page_name", StringType(), True), StructField("referrer", StringType(), True),
            StructField("path", StringType(), True), StructField("method", StringType(), True),
            StructField("type", StringType(), True), StructField("search_type", StringType(), True),
            StructField("search_keyword", StringType(), True), StructField("selected_filters", ArrayType(StringType()), True),
            StructField("result_count", IntegerType(), True), StructField("list_type", StringType(), True),
            StructField("displayed_recipe_ids", ArrayType(StringType()), True), StructField("recipe_id", StringType(), True),
            StructField("rank", IntegerType(), True), StructField("action", StringType(), True),
            StructField("comment_length", IntegerType(), True), StructField("category", StringType(), True),
            StructField("ingredient_count", IntegerType(), True), StructField("ad_id", StringType(), True),
            StructField("ad_type", StringType(), True), StructField("position", StringType(), True),
            StructField("target_url", StringType(), True)
        ])

        # --- 4.2. JSON íŒŒì‹± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ---
        print("ğŸ”§ JSON íŒŒì‹± ë° íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ì¤‘...")
        df_transformed = df_bronze \
            .withColumn("parsed_context", from_json(col("context"), context_schema)) \
            .withColumn("parsed_properties", from_json(col("event_properties"), event_properties_schema)) \
            .withColumn("timestamp_parsed", to_timestamp(col("timestamp"))) \
            .withColumn("date_parsed", col("date").cast(DateType())) \
            .withColumn("processing_timestamp", current_timestamp()) \
            .drop("context", "event_properties")

        # --- 4.3. íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„± (KST ê¸°ì¤€) ---
        print("ğŸ“… íŒŒí‹°ì…˜ ì»¬ëŸ¼ ìƒì„± ì¤‘...")
        df_with_partitions = df_transformed \
            .withColumn("year", year(col("timestamp_parsed"))) \
            .withColumn("month", month(col("timestamp_parsed"))) \
            .withColumn("day", dayofmonth(col("timestamp_parsed"))) \
            .withColumn("hour", hour(col("timestamp_parsed")))

        # --- 4.4. ì»¬ëŸ¼ í‰íƒ„í™” (í–¥ìƒëœ ë²„ì „) ---
        print("ğŸ—‚ï¸ ì»¬ëŸ¼ í‰íƒ„í™” ì¤‘...")
        df_silver_flat = df_with_partitions.select(
            # ê¸°ë³¸ ì´ë²¤íŠ¸ ì •ë³´
            "event_id", "event_name", "user_id", "anonymous_id", "session_id",
            col("timestamp_parsed").alias("event_timestamp"), 
            col("date_parsed").alias("event_date"),
            col("ingestion_timestamp"),
            col("processing_timestamp"),
            
            # íŒŒí‹°ì…˜ ì»¬ëŸ¼
            "year", "month", "day", "hour",
            
            # Context ì •ë³´
            col("parsed_context.page.name").alias("page_name"),
            col("parsed_context.page.url").alias("page_url"),
            col("parsed_context.page.path").alias("page_path"),
            col("parsed_context.user_segment").alias("user_segment"),
            col("parsed_context.activity_level").alias("activity_level"),
            col("parsed_context.cooking_style").alias("cooking_style"),
            col("parsed_context.ab_test.group").alias("ab_test_group"),
            col("parsed_context.ab_test.scenario").alias("ab_test_scenario"),
            
            # Event Properties
            col("parsed_properties.page_name").alias("prop_page_name"),
            col("parsed_properties.referrer").alias("prop_referrer"),
            col("parsed_properties.path").alias("prop_path"),
            col("parsed_properties.method").alias("prop_method"),
            col("parsed_properties.type").alias("prop_type"),
            col("parsed_properties.search_type").alias("prop_search_type"),
            col("parsed_properties.search_keyword").alias("prop_search_keyword"),
            col("parsed_properties.selected_filters").alias("prop_selected_filters"),
            col("parsed_properties.result_count").alias("prop_result_count"),
            col("parsed_properties.list_type").alias("prop_list_type"),
            col("parsed_properties.displayed_recipe_ids").alias("prop_displayed_recipe_ids"),
            col("parsed_properties.recipe_id").cast(LongType()).alias("prop_recipe_id"),
            col("parsed_properties.rank").alias("prop_rank"),
            col("parsed_properties.action").alias("prop_action"),
            col("parsed_properties.comment_length").alias("prop_comment_length"),
            col("parsed_properties.category").alias("prop_category"),
            col("parsed_properties.ingredient_count").alias("prop_ingredient_count"),
            col("parsed_properties.ad_id").alias("prop_ad_id"),
            col("parsed_properties.ad_type").alias("prop_ad_type"),
            col("parsed_properties.position").alias("prop_position"),
            col("parsed_properties.target_url").alias("prop_target_url")
        )
        
        # --- 4.5. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ (í–¥ìƒëœ ë²„ì „) ---
        print("ğŸ” ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì¤‘...")
        df_silver_clean = df_silver_flat \
            .filter(col("event_id").isNotNull()) \
            .filter(col("event_timestamp").isNotNull()) \
            .dropDuplicates(["event_id"])
        
        final_count = df_silver_clean.count()
        print(f"âœ… ì»¬ëŸ¼ í‰íƒ„í™” ë° ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì™„ë£Œ. ìµœì¢… í–‰ ìˆ˜: {final_count:,}")
        
        # Silver ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        print("\nğŸ“Š Silver Layer ìƒ˜í”Œ ë°ì´í„° (ìƒìœ„ 3í–‰):")
        df_silver_clean.show(3, truncate=True)
        
        # ì´ë²¤íŠ¸ë³„ ë¶„í¬ í™•ì¸
        print("\nğŸ“Š ì´ë²¤íŠ¸ë³„ ë¶„í¬:")
        df_silver_clean.groupBy('event_name').count().orderBy('count', ascending=False).show(10)

        # --- 4.6. Silver Iceberg í…Œì´ë¸” ìƒì„± (íŒŒí‹°ì…˜ ì ìš©) ---
        print("\nğŸ’¾ Silver Iceberg í…Œì´ë¸”ë¡œ ì €ì¥ ì¤‘...")
        
        # ì„±ëŠ¥ ìµœì í™”: íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •
        df_silver_optimized = df_silver_clean.coalesce(4)
        
        # Iceberg í…Œì´ë¸” ìƒì„± (íŒŒí‹°ì…˜ ì ìš©)
        try:
            # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
            spark.sql("DROP TABLE IF EXISTS iceberg_catalog.silver.cleaned_events")
            
            df_silver_optimized.writeTo("iceberg_catalog.silver.cleaned_events") \
                .partitionedBy("year", "month", "day") \
                .tableProperty("format-version", "2") \
                .tableProperty("write.target-file-size-bytes", "134217728") \
                .create()
            
            print("âœ… Silver Iceberg í…Œì´ë¸” 'iceberg_catalog.silver.cleaned_events' ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Silver í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            return

        # -----------------------------------------------------------------------------
        # 5. ğŸ“Š Iceberg í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ë° ê³ ê¸‰ ê¸°ëŠ¥ ì‹œì—°
        # -----------------------------------------------------------------------------
        print("\nğŸ“Š Iceberg í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ì •ë³´:")
        
        # í…Œì´ë¸” ì •ë³´ ì¡°íšŒ (ì•ˆì „í•œ ë°©ì‹)
        print("\nğŸ—ƒï¸ Bronze í…Œì´ë¸” ì •ë³´:")
        try:
            bronze_info = spark.sql("DESCRIBE TABLE iceberg_catalog.bronze.raw_events")
            bronze_info.show(10, truncate=False)
        except Exception as e:
            print(f"âš ï¸ Bronze í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        print("\nğŸ—ƒï¸ Silver í…Œì´ë¸” ì •ë³´:")
        try:
            silver_info = spark.sql("DESCRIBE TABLE iceberg_catalog.silver.cleaned_events")
            silver_info.show(10, truncate=False)
        except Exception as e:
            print(f"âš ï¸ Silver í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # íŒŒí‹°ì…˜ ì •ë³´ í™•ì¸
        print("\nğŸ“ Silver í…Œì´ë¸” íŒŒí‹°ì…˜ ì •ë³´:")
        try:
            spark.sql("SHOW PARTITIONS iceberg_catalog.silver.cleaned_events").show()
        except Exception as e:
            print(f"âš ï¸ íŒŒí‹°ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 6. ğŸ§ª Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        # -----------------------------------------------------------------------------
        print("\nğŸ§ª Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        # ìŠ¤ëƒ…ìƒ· ì´ë ¥ ì¡°íšŒ (ì•ˆì „í•œ ë°©ì‹)
        print("\nğŸ“¸ Bronze í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì´ë ¥:")
        try:
            bronze_snapshots = spark.sql("SELECT snapshot_id, committed_at, summary FROM iceberg_catalog.bronze.raw_events.snapshots ORDER BY committed_at DESC LIMIT 3")
            bronze_snapshots.show(truncate=False)
        except Exception as e:
            print(f"âš ï¸ Bronze ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        print("\nğŸ“¸ Silver í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì´ë ¥:")
        try:
            silver_snapshots = spark.sql("SELECT snapshot_id, committed_at, summary FROM iceberg_catalog.silver.cleaned_events.snapshots ORDER BY committed_at DESC LIMIT 3")
            silver_snapshots.show(truncate=False)
        except Exception as e:
            print(f"âš ï¸ Silver ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # íŒŒì¼ ì •ë³´ í™•ì¸
        print("\nğŸ“‹ Silver í…Œì´ë¸” íŒŒì¼ ì •ë³´:")
        try:
            spark.sql("SELECT * FROM iceberg_catalog.silver.cleaned_events.files LIMIT 5").show(truncate=False)
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # í…Œì´ë¸” ì†ì„± í™•ì¸
        print("\nâš™ï¸ Silver í…Œì´ë¸” ì†ì„±:")
        try:
            spark.sql("SHOW TBLPROPERTIES iceberg_catalog.silver.cleaned_events").show(truncate=False)
        except Exception as e:
            print(f"âš ï¸ í…Œì´ë¸” ì†ì„± ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 7. ğŸ”„ Iceberg Time Travel ê¸°ëŠ¥ ë°ëª¨
        # -----------------------------------------------------------------------------
        print("\nğŸ”„ Iceberg Time Travel ê¸°ëŠ¥ ë°ëª¨...")
        
        try:
            # ìµœì‹  ìŠ¤ëƒ…ìƒ· ID ì¡°íšŒ
            snapshots = spark.sql("SELECT snapshot_id, committed_at FROM iceberg_catalog.silver.cleaned_events.snapshots ORDER BY committed_at DESC LIMIT 1").collect()
            if snapshots:
                snapshot_id = snapshots[0]['snapshot_id']
                committed_at = snapshots[0]['committed_at']
                print(f"ğŸ“¸ ìµœì‹  ìŠ¤ëƒ…ìƒ·: {snapshot_id} (ìƒì„±ì‹œê°„: {committed_at})")
                
                # Time Travel ì¿¼ë¦¬ (íŠ¹ì • ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ)
                time_travel_count = spark.sql(f"""
                    SELECT COUNT(*) as record_count 
                    FROM iceberg_catalog.silver.cleaned_events 
                    VERSION AS OF {snapshot_id}
                """).collect()[0]['record_count']
                
                print(f"â° Time Travel ì¿¼ë¦¬ ê²°ê³¼: {time_travel_count:,}í–‰")
                
        except Exception as e:
            print(f"âš ï¸ Time Travel ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 8. ê²€ì¦ ë° ìš”ì•½
        # -----------------------------------------------------------------------------
        print("\nğŸ“ˆ Iceberg ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ìš”ì•½:")
        print(f"ğŸ¥‰ Bronze í–‰ ìˆ˜: {bronze_count:,}")
        print(f"ğŸ¥ˆ Silver í–‰ ìˆ˜: {final_count:,}")
        print(f"ğŸ“Š ìƒì„±ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤: iceberg_catalog.bronze, iceberg_catalog.silver, iceberg_catalog.gold")
        print(f"ğŸ—ƒï¸ Bronze í…Œì´ë¸”: iceberg_catalog.bronze.raw_events (Iceberg v2)")
        print(f"ğŸ—ƒï¸ Silver í…Œì´ë¸”: iceberg_catalog.silver.cleaned_events (Iceberg v2, íŒŒí‹°ì…˜: year/month/day)")
        print(f"ğŸ—ï¸ ì¹´íƒˆë¡œê·¸: Hadoop Catalog")
        print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ì†Œ: S3 (s3a://reciping-user-event-logs/warehouse/iceberg)")
        
        # ì¶”ê°€ Iceberg ê¸°ëŠ¥ë“¤
        print(f"\nğŸ’ ì§€ì›ë˜ëŠ” Iceberg ê³ ê¸‰ ê¸°ëŠ¥:")
        print(f"   âœ… Time Travel: íŠ¹ì • ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ë˜ëŒì•„ê°€ê¸°")
        print(f"   âœ… Schema Evolution: ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì§€ì›")
        print(f"   âœ… Partition Evolution: íŒŒí‹°ì…˜ êµ¬ì¡° ë³€ê²½")
        print(f"   âœ… ACID Transactions: ì›ìì„±, ì¼ê´€ì„±, ê²©ë¦¬ì„±, ì§€ì†ì„±")
        print(f"   âœ… Metadata Tables: ìŠ¤ëƒ…ìƒ·, íŒŒì¼, ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì¡°íšŒ")

        # -----------------------------------------------------------------------------
        # 9. ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        spark.stop()
        print("âœ… Iceberg ETL íŒŒì´í”„ë¼ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # ìŠ¤íŒŒí¬ ì„¸ì…˜ì´ ìˆë‹¤ë©´ ì¢…ë£Œ
        try:
            spark.stop()
        except:
            pass

# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
if __name__ == "__main__":
    main()
