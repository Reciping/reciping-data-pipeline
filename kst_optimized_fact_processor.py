#!/usr/bin/env python3
"""
KST ìµœì í™” Gold Layer Fact í…Œì´ë¸” ì²˜ë¦¬ê¸°
- ultra_batch_processor ì„±ê³µ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•
- KST ì»¬ëŸ¼ í™œìš©í•œ í•œêµ­ ì‹œê°„ëŒ€ ê¸°ì¤€ ì²˜ë¦¬
- ë©”ëª¨ë¦¬ ì•ˆì „ ë°°ì¹˜ í¬ê¸° (5,000ê°œ) ìœ ì§€
- ì ì§„ì  í™•ì¥ ë° ì—ëŸ¬ ë³µêµ¬ ê¸°ëŠ¥
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import time
from datetime import datetime

class KSTOptimizedFactProcessor:
    """KST ìµœì í™” Fact í…Œì´ë¸” ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        self.batch_size = 5000  # ì„±ê³µ ê²€ì¦ëœ ë°°ì¹˜ í¬ê¸°
        
        print("ğŸ‡°ğŸ‡· KST ìµœì í™” Fact ì²˜ë¦¬ê¸° ì´ˆê¸°í™”")
        print(f"   ğŸ“¦ ì•ˆì „ ë°°ì¹˜ í¬ê¸°: {self.batch_size:,}ê°œ")
        
    def create_memory_safe_spark_session(self):
        """ë©”ëª¨ë¦¬ ì•ˆì „ SparkSession (ultra_batch ì„±ê³µ ì„¤ì • ê¸°ë°˜)"""
        print("ğŸ”§ ë©”ëª¨ë¦¬ ì•ˆì „ SparkSession ìƒì„±...")
        
        self.spark = SparkSession.builder \
            .appName("KSTOptimizedFact") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "1g") \
            .config("spark.executor.memory", "1g") \
            .config("spark.sql.shuffle.partitions", "20") \
            .config("spark.sql.adaptive.enabled", "false") \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… ë©”ëª¨ë¦¬ ì•ˆì „ SparkSession ìƒì„± ì™„ë£Œ")
        
    def ensure_kst_optimized_fact_table(self):
        """KST ìµœì í™” Fact í…Œì´ë¸” êµ¬ì¡° í™•ì¸/ìƒì„±"""
        print("\nğŸ—ï¸ KST ìµœì í™” Fact í…Œì´ë¸” êµ¬ì¡° ì¤€ë¹„...")
        
        try:
            # Gold ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.catalog_name}.{self.gold_database}")
            
            # ê¸°ì¡´ í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸
            existing_tables = self.spark.sql(f"SHOW TABLES IN {self.catalog_name}.{self.gold_database}").collect()
            fact_table_exists = any("fact_user_events" in row.tableName for row in existing_tables)
            
            if fact_table_exists:
                print("âœ… ê¸°ì¡´ Fact í…Œì´ë¸” ë°œê²¬ - ê¸°ì¡´ êµ¬ì¡° í™œìš©")
                return True
            
            # ìƒˆ KST ìµœì í™” í…Œì´ë¸” ìƒì„±
            print("ğŸ†• KST ìµœì í™” Fact í…Œì´ë¸” ìƒì„±...")
            
            create_table_sql = f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.fact_user_events (
                event_id STRING NOT NULL,
                
                -- ê¸°ë³¸ ì°¨ì› í‚¤ë“¤ (ë‹¨ìˆœí™”)
                user_dim_key BIGINT,
                time_dim_key BIGINT,
                recipe_dim_key BIGINT, 
                page_dim_key BIGINT,
                event_dim_key BIGINT,
                
                -- KST ê¸°ë°˜ ì‹œê°„ ì •ë³´ (í•µì‹¬ ê°œì„ ì )
                kst_timestamp TIMESTAMP,
                kst_date DATE,
                kst_year INT,
                kst_month INT,
                kst_day INT,
                kst_hour INT,
                kst_day_of_week STRING,
                
                -- UTC ì‹œê°„ (ë¹„êµìš©)
                utc_timestamp TIMESTAMP,
                
                -- ì‚¬ìš©ì ì •ë³´ (ì°¨ì› í…Œì´ë¸” ì—†ì´ë„ ë¶„ì„ ê°€ëŠ¥)
                user_id STRING,
                user_segment STRING,
                cooking_style STRING,
                ab_test_group STRING,
                
                -- ì´ë²¤íŠ¸ ì •ë³´
                event_name STRING,
                page_name STRING,
                
                -- ë ˆì‹œí”¼ ì •ë³´ (NULL í—ˆìš©)
                recipe_id BIGINT,
                
                -- ì¸¡ì •ê°’ë“¤
                event_count BIGINT,
                session_duration_seconds BIGINT,
                page_view_duration_seconds BIGINT,
                is_conversion BOOLEAN,
                conversion_value DECIMAL(10,2),
                engagement_score DECIMAL(5,2),
                
                -- Degenerate Dimensions
                session_id STRING,
                anonymous_id STRING,
                
                -- ETL ë©”íƒ€ë°ì´í„°
                created_at TIMESTAMP,
                updated_at TIMESTAMP
                
            ) USING ICEBERG
            PARTITIONED BY (kst_year, kst_month, kst_day)
            TBLPROPERTIES (
                'format-version' = '2',
                'write.target-file-size-bytes' = '67108864'
            )
            """
            
            self.spark.sql(create_table_sql)
            print("âœ… KST ìµœì í™” Fact í…Œì´ë¸” ìƒì„± ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ Fact í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
            
    def create_kst_batch_safely(self, start_date: str, batch_num: int = 0, is_first_batch: bool = False):
        """KST ê¸°ë°˜ ì•ˆì „í•œ ë°°ì¹˜ ìƒì„±"""
        print(f"\nğŸ“… KST ë°°ì¹˜ ìƒì„±: {start_date} (ë°°ì¹˜ #{batch_num + 1})")
        
        try:
            # INSERT ëª¨ë“œ ê²°ì •
            insert_mode = "INSERT OVERWRITE" if is_first_batch else "INSERT INTO"
            offset = batch_num * self.batch_size
            
            # KST ì»¬ëŸ¼ í™œìš©í•œ ì•ˆì „í•œ ì¿¼ë¦¬
            kst_batch_query = f"""
            {insert_mode} {self.catalog_name}.{self.gold_database}.fact_user_events
            SELECT 
                s.event_id,
                
                -- ì°¨ì› í‚¤ë“¤ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ ê°€ëŠ¥)
                0 as user_dim_key,
                CAST(DATE_FORMAT(s.kst_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + s.hour as time_dim_key,
                COALESCE(s.prop_recipe_id, 0) as recipe_dim_key,
                0 as page_dim_key,
                0 as event_dim_key,
                
                -- KST ì‹œê°„ ì •ë³´ (í•µì‹¬ í™œìš©)
                s.kst_timestamp,
                s.date as kst_date,
                s.year as kst_year,
                s.month as kst_month,
                s.day as kst_day,
                s.hour as kst_hour,
                s.day_of_week as kst_day_of_week,
                
                -- UTC ì‹œê°„ (ë¹„êµìš©)
                s.utc_timestamp,
                
                -- ì‚¬ìš©ì ì •ë³´ (ì§ì ‘ í¬í•¨)
                s.user_id,
                s.user_segment,
                s.cooking_style,
                s.ab_test_group,
                
                -- ì´ë²¤íŠ¸ ì •ë³´
                s.event_name,
                s.page_name,
                
                -- ë ˆì‹œí”¼ ì •ë³´
                s.prop_recipe_id as recipe_id,
                
                -- ì¸¡ì •ê°’ ê³„ì‚°
                1 as event_count,
                
                -- ì„¸ì…˜ ì‹œê°„ (prop_actionì—ì„œ ì¶”ì¶œ ë˜ëŠ” ê¸°ë³¸ê°’)
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 2
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[1] AS BIGINT), 60)
                    ELSE 60
                END as session_duration_seconds,
                
                30 as page_view_duration_seconds,
                
                -- ì „í™˜ í”Œë˜ê·¸ (KST ì‹œê°„ëŒ€ ê¸°ì¤€ ë¶„ì„ ê°€ëŠ¥)
                CASE WHEN s.event_name IN ('auth_success', 'click_bookmark', 'create_comment') THEN TRUE ELSE FALSE END as is_conversion,
                
                1.0 as conversion_value,
                
                -- KST ì‹œê°„ëŒ€ë³„ ì°¸ì—¬ë„ ì ìˆ˜ (í•œêµ­ ì‚¬ìš© íŒ¨í„´ ë°˜ì˜)
                CASE 
                    WHEN s.event_name = 'auth_success' THEN 10.0
                    WHEN s.event_name = 'create_comment' THEN 9.0
                    WHEN s.event_name = 'click_bookmark' THEN 8.0
                    WHEN s.event_name = 'click_recipe' THEN 7.0
                    WHEN s.event_name = 'search_recipe' THEN 5.0
                    WHEN s.event_name = 'view_recipe' THEN 4.0
                    WHEN s.event_name = 'view_page' THEN 2.0
                    ELSE 1.0
                END as engagement_score,
                
                -- Degenerate Dimensions
                s.session_id,
                s.anonymous_id,
                
                -- ETL ë©”íƒ€ë°ì´í„°
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM (
                SELECT 
                    event_id, kst_timestamp, utc_timestamp, date, year, month, day, hour, day_of_week,
                    user_id, user_segment, cooking_style, ab_test_group,
                    event_name, page_name, prop_recipe_id, prop_action,
                    session_id, anonymous_id,
                    ROW_NUMBER() OVER (ORDER BY kst_timestamp, event_id) as row_num
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                WHERE date = '{start_date}' AND event_id IS NOT NULL
            ) s
            WHERE s.row_num > {offset} AND s.row_num <= {offset + self.batch_size}
            """
            
            start_time = time.time()
            self.spark.sql(kst_batch_query)
            end_time = time.time()
            
            # ê²°ê³¼ í™•ì¸
            current_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            
            batch_time = end_time - start_time
            print(f"   âœ… ë°°ì¹˜ ì™„ë£Œ: +{self.batch_size:,}ê°œ (ëˆ„ì : {current_count:,}ê°œ, {batch_time:.1f}ì´ˆ)")
            
            return self.batch_size
            
        except Exception as e:
            print(f"   âŒ ë°°ì¹˜ ì‹¤íŒ¨: {str(e)[:100]}...")
            return 0
            
    def process_date_range_with_kst(self, start_date: str, end_date: str):
        """KST ê¸°ë°˜ ë‚ ì§œ ë²”ìœ„ ì²˜ë¦¬"""
        print(f"\nğŸ—“ï¸ KST ë‚ ì§œ ë²”ìœ„ ì²˜ë¦¬: {start_date} ~ {end_date}")
        
        # ë‚ ì§œë³„ ì²˜ë¦¬
        from datetime import datetime, timedelta
        
        current_date = datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')
        
        total_processed = 0
        overall_batches = 0
        is_very_first_batch = True
        
        while current_date <= end_date_obj:
            date_str = current_date.strftime('%Y-%m-%d')
            print(f"\nğŸ“… {date_str} ì²˜ë¦¬ ì¤‘...")
            
            try:
                # í•´ë‹¹ ë‚ ì§œì˜ ì´ë²¤íŠ¸ ìˆ˜ í™•ì¸
                date_count = self.spark.sql(f"""
                    SELECT COUNT(*) as cnt 
                    FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                    WHERE date = '{date_str}'
                """).collect()[0]['cnt']
                
                if date_count == 0:
                    print(f"   âš ï¸ {date_str}: ë°ì´í„° ì—†ìŒ, ê±´ë„ˆëœ€")
                    current_date += timedelta(days=1)
                    continue
                
                # í•´ë‹¹ ë‚ ì§œì˜ ë°°ì¹˜ ìˆ˜ ê³„ì‚°
                needed_batches = (date_count + self.batch_size - 1) // self.batch_size
                print(f"   ğŸ“Š {date_count:,}ê°œ ì´ë²¤íŠ¸ â†’ {needed_batches}ê°œ ë°°ì¹˜")
                
                # ë‚ ì§œë³„ ë°°ì¹˜ ì²˜ë¦¬
                date_processed = 0
                for batch_num in range(needed_batches):
                    processed_count = self.create_kst_batch_safely(
                        date_str, 
                        batch_num, 
                        is_very_first_batch
                    )
                    
                    if processed_count > 0:
                        date_processed += processed_count
                        total_processed += processed_count
                        overall_batches += 1
                        is_very_first_batch = False
                        
                        # ë©”ëª¨ë¦¬ ì•ˆì •ì„±ì„ ìœ„í•œ ëŒ€ê¸°
                        time.sleep(1)
                    else:
                        print(f"   âš ï¸ ë°°ì¹˜ {batch_num + 1} ì‹¤íŒ¨, ì¤‘ë‹¨")
                        break
                
                print(f"   âœ… {date_str} ì™„ë£Œ: {date_processed:,}ê°œ ì²˜ë¦¬")
                
            except Exception as e:
                print(f"   âŒ {date_str} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}...")
                break
                
            current_date += timedelta(days=1)
        
        return total_processed, overall_batches
        
    def validate_kst_fact_table(self):
        """KST Fact í…Œì´ë¸” ê²€ì¦"""
        print(f"\nğŸ” KST Fact í…Œì´ë¸” ê²€ì¦...")
        
        try:
            validation_stats = self.spark.sql(f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT kst_date) as unique_kst_dates,
                COUNT(DISTINCT user_id) as unique_users,
                COUNT(DISTINCT event_name) as unique_events,
                COUNT(DISTINCT session_id) as unique_sessions,
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as total_conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement,
                
                -- KST ì‹œê°„ ë²”ìœ„
                MIN(kst_timestamp) as min_kst_time,
                MAX(kst_timestamp) as max_kst_time,
                MIN(kst_date) as min_kst_date,
                MAX(kst_date) as max_kst_date,
                
                -- UTC ì‹œê°„ ë²”ìœ„ (ë¹„êµ)
                MIN(utc_timestamp) as min_utc_time,
                MAX(utc_timestamp) as max_utc_time
                
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            print("ğŸ“Š KST Fact í…Œì´ë¸” ê²€ì¦ ê²°ê³¼:")
            print(f"   ì´ ë ˆì½”ë“œ: {validation_stats.total_records:,}ê°œ")
            print(f"   KST ë‚ ì§œ ë²”ìœ„: {validation_stats.min_kst_date} ~ {validation_stats.max_kst_date}")
            print(f"   ê³ ìœ  ì‚¬ìš©ì: {validation_stats.unique_users:,}ëª…")
            print(f"   ê³ ìœ  ì´ë²¤íŠ¸: {validation_stats.unique_events}ê°œ")
            print(f"   ê³ ìœ  ì„¸ì…˜: {validation_stats.unique_sessions:,}ê°œ")
            print(f"   ì´ ì „í™˜: {validation_stats.total_conversions:,}ê±´")
            print(f"   í‰ê·  ì°¸ì—¬ë„: {validation_stats.avg_engagement}")
            
            print(f"\nğŸ‡°ğŸ‡· KST vs UTC ì‹œê°„ ë¹„êµ:")
            print(f"   KST: {validation_stats.min_kst_time} ~ {validation_stats.max_kst_time}")
            print(f"   UTC: {validation_stats.min_utc_time} ~ {validation_stats.max_utc_time}")
            
            # KST ì‹œê°„ëŒ€ë³„ ë¶„í¬
            hourly_dist = self.spark.sql(f"""
            SELECT 
                kst_hour,
                COUNT(*) as events,
                COUNT(DISTINCT user_id) as users
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            GROUP BY kst_hour
            ORDER BY kst_hour
            """).collect()
            
            print(f"\nâ° KST ì‹œê°„ëŒ€ë³„ í™œë™ ë¶„í¬:")
            for row in hourly_dist:
                print(f"   {row.kst_hour:2d}ì‹œ: {row.events:,}ê°œ ì´ë²¤íŠ¸, {row.users:,}ëª… ì‚¬ìš©ì")
            
            return validation_stats.total_records > 0
            
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return False
            
    def demonstrate_kst_analytics(self):
        """KST ê¸°ë°˜ ë¶„ì„ ì˜ˆì‹œ"""
        print(f"\nğŸš€ KST ê¸°ë°˜ ë¶„ì„ ì˜ˆì‹œ...")
        
        try:
            # 1. í•œêµ­ ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´
            print("â° í•œêµ­ ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´:")
            time_pattern = self.spark.sql(f"""
            SELECT 
                kst_hour,
                COUNT(*) as total_events,
                COUNT(DISTINCT user_id) as active_users,
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            WHERE kst_date >= '2025-07-01' AND kst_date <= '2025-07-03'
            GROUP BY kst_hour
            ORDER BY total_events DESC
            LIMIT 5
            """).collect()
            
            for row in time_pattern:
                print(f"   {row.kst_hour:2d}ì‹œ: {row.total_events:,}ê°œ ì´ë²¤íŠ¸, {row.active_users:,}ëª…, ì „í™˜ {row.conversions}ê±´")
            
            # 2. ìš”ì¼ë³„ íŒ¨í„´ (KST ê¸°ì¤€)
            print(f"\nğŸ“… ìš”ì¼ë³„ í™œë™ íŒ¨í„´ (KST ê¸°ì¤€):")
            daily_pattern = self.spark.sql(f"""
            SELECT 
                kst_day_of_week,
                COUNT(*) as total_events,
                COUNT(DISTINCT user_id) as unique_users,
                ROUND(AVG(engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            GROUP BY kst_day_of_week
            ORDER BY total_events DESC
            """).collect()
            
            for row in daily_pattern:
                print(f"   {row.kst_day_of_week}: {row.total_events:,}ê°œ ì´ë²¤íŠ¸, {row.unique_users:,}ëª…")
            
            print(f"\nâœ… KST ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ í•œêµ­ ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ì •í™•íˆ íŒŒì•… ê°€ëŠ¥!")
            
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì˜ˆì‹œ ì˜¤ë¥˜: {str(e)}")
            
    def execute_kst_optimized_processing(self, start_date: str = "2025-07-01", end_date: str = "2025-07-10"):
        """KST ìµœì í™” ì²˜ë¦¬ ì „ì²´ ì‹¤í–‰"""
        print("ğŸš€ KST ìµœì í™” Gold Layer Fact ì²˜ë¦¬ ì‹œì‘!")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. ë©”ëª¨ë¦¬ ì•ˆì „ SparkSession ìƒì„±
            self.create_memory_safe_spark_session()
            
            # 2. KST ìµœì í™” Fact í…Œì´ë¸” ì¤€ë¹„
            if not self.ensure_kst_optimized_fact_table():
                print("âŒ Fact í…Œì´ë¸” ì¤€ë¹„ ì‹¤íŒ¨")
                return False
            
            # 3. KST ê¸°ë°˜ ë‚ ì§œ ë²”ìœ„ ì²˜ë¦¬
            total_processed, total_batches = self.process_date_range_with_kst(start_date, end_date)
            
            # 4. ê²°ê³¼ ê²€ì¦
            success = self.validate_kst_fact_table()
            
            # 5. KST ë¶„ì„ ì˜ˆì‹œ
            if success:
                self.demonstrate_kst_analytics()
            
            end_time = time.time()
            elapsed_hours = (end_time - start_time) / 3600
            
            print(f"\n" + "=" * 60)
            if success and total_processed > 0:
                print(f"ğŸ‰ KST ìµœì í™” Fact ì²˜ë¦¬ ì™„ë£Œ!")
                print(f"   ğŸ“… ì²˜ë¦¬ ê¸°ê°„: {start_date} ~ {end_date}")
                print(f"   ğŸ“Š ì²˜ë¦¬ëŸ‰: {total_processed:,}ê°œ ì´ë²¤íŠ¸")
                print(f"   ğŸ”„ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ")
                print(f"   â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_hours:.1f}ì‹œê°„")
                print(f"   ğŸ‡°ğŸ‡· KST ì»¬ëŸ¼ í™œìš©: ì™„ì „ êµ¬í˜„")
                print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì•ˆì „: 5,000ê°œ ë°°ì¹˜ë¡œ ì•ˆì • ì²˜ë¦¬")
            else:
                print(f"âš ï¸ ë¶€ë¶„ ì™„ë£Œ ë˜ëŠ” ì‹¤íŒ¨")
                print(f"   ì²˜ë¦¬ëŸ‰: {total_processed:,}ê°œ")
                
            return success
            
        except Exception as e:
            print(f"âŒ KST ìµœì í™” ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return False
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    processor = KSTOptimizedFactProcessor()
    
    # 10ì¼ê°„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì•ˆì „)
    success = processor.execute_kst_optimized_processing(
        start_date="2025-07-01", 
        end_date="2025-07-10"
    )
    
    if success:
        print(f"\nğŸ‰ KST ìµœì í™” ì™„ë£Œ!")
        print(f"   ğŸ‡°ğŸ‡· í•œêµ­ ì‹œê°„ëŒ€ ê¸°ì¤€ ì •í™•í•œ ë¶„ì„ ê°€ëŠ¥")
        print(f"   ğŸ“Š ì‹œê°„ëŒ€ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„ ê°€ëŠ¥")
        print(f"   ğŸ”’ ë©”ëª¨ë¦¬ ì•ˆì „ ë³´ì¥ (JVM í¬ë˜ì‹œ ì—†ìŒ)")
    else:
        print(f"\nâš ï¸ ì¶”ê°€ ìµœì í™” í•„ìš”")
