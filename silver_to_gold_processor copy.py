#!/usr/bin/env python3
"""
ğŸ§Š Silver to Gold ETL Processor (Incremental, Airflow-triggered)
================================================================
Silver Iceberg í…Œì´ë¸”ì—ì„œ íŠ¹ì • íŒŒí‹°ì…˜ì˜ ë°ì´í„°ë¥¼ ì½ì–´ Gold Fact í…Œì´ë¸”ë¡œ ë³€í™˜/ì§‘ê³„í•©ë‹ˆë‹¤.
Airflowë¡œë¶€í„° ì‹¤í–‰ ì‹œê°„ì„ ë°›ì•„ ì ì§„ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import logging
import argparse
from datetime import datetime
import pytz
from dateutil.parser import isoparse

from pyspark.sql import SparkSession

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SilverToGoldProcessor:
    def __init__(self, test_mode: bool = True):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        
        if test_mode:
            print("=== í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
            self.silver_database = "recipe_analytics_test"
            self.gold_database = "recipe_analytics_test" # Goldë„ í…ŒìŠ¤íŠ¸ DB ì‚¬ìš©
            self.table_suffix = "_test"
        else:
            print("=== ìš´ì˜ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
            self.silver_database = "recipe_analytics"
            self.gold_database = "gold_analytics"
            self.table_suffix = ""
            
        self.silver_table_name = f"{self.catalog_name}.{self.silver_database}.user_events_silver{self.table_suffix}"
        self.gold_table_name = f"{self.catalog_name}.{self.gold_database}.fact_user_events{self.table_suffix}"

    def create_spark_session(self):
        print("SparkSession ìƒì„± ì¤‘...")
        self.spark = SparkSession.builder \
            .appName("SilverToGold_ETL") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://10.0.11.86:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .getOrCreate()
        self.spark.sparkContext.setLogLevel("WARN")
        print("SparkSession ìƒì„± ì™„ë£Œ")

    # --- [ì‹ ê·œ ì¶”ê°€] Gold í…Œì´ë¸” ìƒì„± í•¨ìˆ˜ ---
    def create_gold_table_if_not_exists(self):
        """Gold Fact í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"Gold Fact í…Œì´ë¸” ìƒì„± í™•ì¸: {self.gold_table_name}")
        # Silver í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ Gold í…Œì´ë¸” DDL ì‘ì„±
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.gold_table_name} (
            event_id STRING,
            user_dim_key BIGINT,
            time_dim_key BIGINT,
            recipe_dim_key BIGINT,
            page_dim_key BIGINT,
            event_dim_key INT,
            event_count INT,
            session_duration_seconds BIGINT,
            page_view_duration_seconds BIGINT,
            is_conversion BOOLEAN,
            conversion_value DOUBLE,
            engagement_score DOUBLE,
            session_id STRING,
            anonymous_id STRING,
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        )
        USING ICEBERG
        PARTITIONED BY (days(created_at)) -- Gold í…Œì´ë¸”ì€ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ íŒŒí‹°ì…”ë‹
        """
        self.spark.sql(create_table_sql)
        print("Gold Fact í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ")

    def transform_and_load_gold_data(self, target_date: str):
        """íŠ¹ì • ë‚ ì§œì˜ Silver ë°ì´í„°ë¥¼ Gold Fact í…Œì´ë¸”ë¡œ ë³€í™˜ ë° ì ì¬í•©ë‹ˆë‹¤."""
        print(f"Silver to Gold ì²˜ë¦¬ ì‹œì‘ (ëŒ€ìƒ ë‚ ì§œ: {target_date})")
        
        # Gold í…Œì´ë¸”ì— ë°ì´í„°ë¥¼ ë°”ë¡œ INSERT í•˜ëŠ” SQL ì¿¼ë¦¬
        # ê¸°ì¡´ ì½”ë“œì˜ í•µì‹¬ ë³€í™˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        insert_gold_sql = f"""
        INSERT INTO {self.gold_table_name}
        SELECT 
            s.event_id,
            0 as user_dim_key,
            CAST(DATE_FORMAT(s.kst_timestamp, 'yyyyMMddHH') AS BIGINT) as time_dim_key,
            COALESCE(s.prop_recipe_id, 0) as recipe_dim_key,
            0 as page_dim_key,
            CASE 
                WHEN s.event_name = 'auth_success' THEN 1 WHEN s.event_name = 'create_comment' THEN 2
                WHEN s.event_name = 'click_bookmark' THEN 3 WHEN s.event_name = 'click_recipe' THEN 4
                WHEN s.event_name = 'search_recipe' THEN 5 WHEN s.event_name = 'view_recipe' THEN 6
                WHEN s.event_name = 'view_page' THEN 7 ELSE 0
            END as event_dim_key,
            1 as event_count,
            CASE 
                WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 2
                THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[1] AS BIGINT), 60)
                ELSE 60
            END as session_duration_seconds,
            30 as page_view_duration_seconds,
            CASE WHEN s.event_name IN ('auth_success', 'click_bookmark', 'create_comment') THEN TRUE ELSE FALSE END as is_conversion,
            1.0 as conversion_value,
            CASE 
                WHEN s.event_name = 'auth_success' THEN 10.0 WHEN s.event_name = 'create_comment' THEN 9.0
                WHEN s.event_name = 'click_bookmark' THEN 8.0 WHEN s.event_name = 'click_recipe' THEN 7.0
                WHEN s.event_name = 'search_recipe' THEN 5.0 WHEN s.event_name = 'view_recipe' THEN 4.0
                WHEN s.event_name = 'view_page' THEN 2.0 ELSE 1.0
            END as engagement_score,
            s.session_id,
            s.anonymous_id,
            s.kst_timestamp as created_at,
            s.kst_timestamp as updated_at
        FROM {self.silver_table_name} s
        WHERE s.date = '{target_date}' AND s.event_id IS NOT NULL
        """
        
        try:
            silver_count = self.spark.read.table(self.silver_table_name).where(f"date = '{target_date}'").count()
            if silver_count == 0:
                print(f"{target_date} ë‚ ì§œì˜ Silver ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return

            print(f"{target_date} ë‚ ì§œì˜ Silver ë°ì´í„° {silver_count}ê±´ì„ Gold í…Œì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
            self.spark.sql(insert_gold_sql)
            print("Gold í…Œì´ë¸” ì ì¬ ì™„ë£Œ.")
        except Exception as e:
            logger.error("Gold ë°ì´í„° ë³€í™˜/ì ì¬ ì‹¤íŒ¨", exc_info=True)
            raise

    def run_pipeline(self, execution_ts: str):
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            self.create_spark_session()
            
            # Airflowê°€ ë„˜ê²¨ì¤€ ì‹¤í–‰ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬í•  ë‚ ì§œ(íŒŒí‹°ì…˜) ê²°ì •
            kst_tz = pytz.timezone('Asia/Seoul')
            try:
                dt_obj = datetime.strptime(execution_ts, '%Y-%m-%d %H:%M')
                kst_dt = kst_tz.localize(dt_obj)
            except ValueError:
                utc_dt = isoparse(execution_ts)
                kst_dt = utc_dt.astimezone(kst_tz)
            
            target_date = kst_dt.strftime('%Y-%m-%d')
            
            # --- [ì‹ ê·œ ì¶”ê°€] Gold í…Œì´ë¸” ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ ---
            self.create_gold_table_if_not_exists()
            
            # --- [ì‹ ê·œ ì¶”ê°€] Silver í…Œì´ë¸”ì˜ ìµœì‹  ë©”íƒ€ë°ì´í„° ê°•ì œ ìƒˆë¡œê³ ì¹¨ ---
            print(f"Silver í…Œì´ë¸”ì˜ ìµœì‹  ì •ë³´ ìƒˆë¡œê³ ì¹¨: {self.silver_table_name}")
            self.spark.catalog.refreshTable(self.silver_table_name)
            # -----------------------------------------------------------
            
            # ì´ì œ ìµœì‹  ìƒíƒœê°€ ë³´ì¥ëœ Silver í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì½ìŠµë‹ˆë‹¤.
            self.transform_and_load_gold_data(target_date)
            
            print("Silver to Gold ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨", exc_info=True)
            raise
        finally:
            if self.spark:
                self.spark.stop()

def main():
    parser = argparse.ArgumentParser(description="Silver to Gold Iceberg ETL Job (Incremental)")
    parser.add_argument("--execution-ts", required=True, help="Airflow execution timestamp")
    parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True, help="Run in test mode")
    args = parser.parse_args()

    processor = SilverToGoldProcessor(test_mode=args.test_mode)
    processor.run_pipeline(execution_ts=args.execution_ts)

if __name__ == "__main__":
    main()