#!/usr/bin/env python3
"""
ë‚ ì§œ ë²”ìœ„ ë¶„í•  Gold Layer ì²˜ë¦¬ê¸°
- 7/1~7/10, 7/11~7/20, 7/21~7/31ë¡œ ë¶„í•  ì²˜ë¦¬
- KST ì‹œê°„ ì»¬ëŸ¼ í™œìš© (date, year, month, day, hour)
- ë©”ëª¨ë¦¬ ìµœì í™” ë° ì•ˆì „í•œ ë°°ì¹˜ ì²˜ë¦¬
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import time

class DateRangeGoldProcessor:
    """ë‚ ì§œ ë²”ìœ„ë³„ Gold Layer ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        
        # ë‚ ì§œ ë²”ìœ„ ì •ì˜
        self.date_ranges = [
            {
                'name': 'Period1_7ì›”ì´ˆ',
                'start_date': '2025-07-01',
                'end_date': '2025-07-10',
                'description': '7ì›” 1ì¼~10ì¼ (10ì¼ê°„)'
            },
            {
                'name': 'Period2_7ì›”ì¤‘',
                'start_date': '2025-07-11', 
                'end_date': '2025-07-20',
                'description': '7ì›” 11ì¼~20ì¼ (10ì¼ê°„)'
            },
            {
                'name': 'Period3_7ì›”ë§',
                'start_date': '2025-07-21',
                'end_date': '2025-07-31', 
                'description': '7ì›” 21ì¼~31ì¼ (11ì¼ê°„)'
            }
        ]
        
    def create_optimized_spark_session(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”ëœ SparkSession"""
        print("ğŸ”§ ë‚ ì§œ ë²”ìœ„ ë¶„í• ìš© ìµœì í™”ëœ SparkSession...")
        
        self.spark = SparkSession.builder \
            .appName("DateRangeGoldProcessor") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.sql.shuffle.partitions", "50") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… ìµœì í™”ëœ SparkSession ìƒì„± ì™„ë£Œ")
        
    def create_gold_database(self):
        """Gold Analytics ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
        print(f"\nğŸ—ï¸ Gold Analytics ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±...")
        
        self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.catalog_name}.{self.gold_database}")
        print(f"âœ… {self.catalog_name}.{self.gold_database} ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")
        
    def analyze_date_range(self, start_date, end_date, range_name):
        """íŠ¹ì • ë‚ ì§œ ë²”ìœ„ ë°ì´í„° ë¶„ì„"""
        print(f"\nğŸ“Š {range_name} ë°ì´í„° ë¶„ì„...")
        
        try:
            # KST ê¸°ì¤€ ë‚ ì§œ í•„í„°ë§ (date ì»¬ëŸ¼ ì‚¬ìš©)
            analysis_query = f"""
            SELECT 
                COUNT(*) as total_events,
                COUNT(DISTINCT user_id) as unique_users,
                COUNT(DISTINCT session_id) as unique_sessions,
                COUNT(DISTINCT prop_recipe_id) as unique_recipes,
                COUNT(DISTINCT page_name) as unique_pages,
                COUNT(DISTINCT event_name) as unique_events,
                
                -- KST ì‹œê°„ ë²”ìœ„ í™•ì¸
                MIN(date) as min_kst_date,
                MAX(date) as max_kst_date,
                MIN(year) as min_year,
                MAX(year) as max_year,
                MIN(month) as min_month,
                MAX(month) as max_month,
                
                -- UTC vs KST ì‹œê°„ ë¹„êµ
                MIN(utc_timestamp) as min_utc,
                MAX(utc_timestamp) as max_utc
                
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE date >= '{start_date}' AND date <= '{end_date}'
            """
            
            stats = self.spark.sql(analysis_query).collect()[0]
            
            print(f"   ğŸ“ˆ ê¸°ë³¸ í†µê³„:")
            print(f"      ì´ ì´ë²¤íŠ¸: {stats.total_events:,}ê°œ")
            print(f"      ê³ ìœ  ì‚¬ìš©ì: {stats.unique_users:,}ëª…")
            print(f"      ê³ ìœ  ì„¸ì…˜: {stats.unique_sessions:,}ê°œ")
            print(f"      ê³ ìœ  ë ˆì‹œí”¼: {stats.unique_recipes:,}ê°œ")
            print(f"      ê³ ìœ  í˜ì´ì§€: {stats.unique_pages}ê°œ")
            print(f"      ê³ ìœ  ì´ë²¤íŠ¸: {stats.unique_events}ê°œ")
            
            print(f"   ğŸ‡°ğŸ‡· KST ì‹œê°„ ë²”ìœ„:")
            print(f"      KST ë‚ ì§œ: {stats.min_kst_date} ~ {stats.max_kst_date}")
            print(f"      ì—°ë„: {stats.min_year} ~ {stats.max_year}")
            print(f"      ì›”: {stats.min_month} ~ {stats.max_month}")
            
            print(f"   ğŸŒ UTC ì‹œê°„ ë²”ìœ„:")
            print(f"      UTC ì‹œê°„: {stats.min_utc} ~ {stats.max_utc}")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
            batch_size = 5000
            needed_batches = (stats.total_events + batch_size - 1) // batch_size
            estimated_time = needed_batches * 45 / 3600  # 45ì´ˆ per batch (ìµœì í™”ë¨)
            memory_usage_gb = stats.total_events * 64 / (1024**3)  # ëŒ€ëµì  ê³„ì‚°
            
            print(f"   âš¡ ì²˜ë¦¬ ì˜ˆìƒ:")
            print(f"      í•„ìš” ë°°ì¹˜: {needed_batches}ê°œ")
            print(f"      ì˜ˆìƒ ì‹œê°„: {estimated_time:.1f}ì‹œê°„")
            print(f"      ì˜ˆìƒ ë©”ëª¨ë¦¬: {memory_usage_gb:.1f}GB")
            
            return {
                'total_events': stats.total_events,
                'needed_batches': needed_batches,
                'estimated_time': estimated_time,
                'memory_usage_gb': memory_usage_gb
            }
            
        except Exception as e:
            print(f"âŒ {range_name} ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return None
            
    def create_simple_fact_table_for_range(self, start_date, end_date, range_name, is_first_range=False):
        """íŠ¹ì • ë‚ ì§œ ë²”ìœ„ì˜ ê°„ë‹¨í•œ Fact í…Œì´ë¸” ìƒì„±"""
        print(f"\nğŸ”„ {range_name} Fact í…Œì´ë¸” ìƒì„±...")
        
        try:
            # INSERT ëª¨ë“œ ê²°ì •
            insert_mode = "INSERT OVERWRITE" if is_first_range else "INSERT INTO"
            
            # KST ì‹œê°„ ì»¬ëŸ¼ í™œìš©í•œ ì¿¼ë¦¬
            fact_query = f"""
            {insert_mode} {self.catalog_name}.{self.gold_database}.fact_user_events_simple
            SELECT 
                event_id,
                user_id,
                session_id,
                anonymous_id,
                event_name,
                page_name,
                prop_recipe_id,
                
                -- KST ì‹œê°„ ì»¬ëŸ¼ í™œìš© (UTC ëŒ€ì‹  í•œêµ­ ì‹œê°„ ê¸°ì¤€)
                date as kst_date,
                year as kst_year,
                month as kst_month, 
                day as kst_day,
                hour as kst_hour,
                day_of_week as kst_day_of_week,
                
                -- ë¹„êµë¥¼ ìœ„í•œ UTC ì‹œê°„ë„ í¬í•¨
                utc_timestamp,
                
                -- ì‚¬ìš©ì ì •ë³´
                user_segment,
                cooking_style,
                ab_test_group,
                
                -- ì´ë²¤íŠ¸ ì†ì„±
                prop_list_type,
                prop_action,
                prop_search_keyword,
                prop_result_count,
                
                -- ë‹¨ìˆœí•œ ë©”íŠ¸ë¦­
                1 as event_count,
                CASE WHEN event_name IN ('auth_success', 'click_bookmark', 'create_comment') THEN 1 ELSE 0 END as conversion_flag,
                CASE 
                    WHEN event_name = 'auth_success' THEN 10
                    WHEN event_name = 'create_comment' THEN 9
                    WHEN event_name = 'click_bookmark' THEN 8
                    WHEN event_name = 'click_recipe' THEN 7
                    WHEN event_name = 'search_recipe' THEN 5
                    WHEN event_name = 'view_recipe' THEN 4
                    WHEN event_name = 'view_page' THEN 2
                    ELSE 1
                END as engagement_score,
                
                -- ë©”íƒ€ë°ì´í„°
                processed_at,
                data_source,
                pipeline_version,
                '{range_name}' as processing_batch,
                CURRENT_TIMESTAMP() as gold_processed_at
                
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE date >= '{start_date}' AND date <= '{end_date}'
            AND event_id IS NOT NULL
            """
            
            print(f"   ğŸ”„ ì‹¤í–‰ ì¤‘... ({insert_mode} ëª¨ë“œ)")
            self.spark.sql(fact_query)
            
            # ê²°ê³¼ í™•ì¸
            count_query = f"""
            SELECT COUNT(*) as cnt 
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events_simple
            WHERE processing_batch = '{range_name}'
            """
            
            range_count = self.spark.sql(count_query).collect()[0]['cnt']
            print(f"   âœ… {range_name} ì™„ë£Œ: {range_count:,}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬")
            
            return range_count
            
        except Exception as e:
            print(f"   âŒ {range_name} Fact í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return 0
            
    def create_fact_table_schema(self):
        """Fact í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        print(f"\nğŸ—ï¸ Fact í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ìƒì„±...")
        
        try:
            schema_query = f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.fact_user_events_simple (
                -- ê¸°ë³¸ ì‹ë³„ì
                event_id STRING NOT NULL,
                user_id STRING,
                session_id STRING,
                anonymous_id STRING,
                
                -- ì´ë²¤íŠ¸ ì •ë³´
                event_name STRING NOT NULL,
                page_name STRING,
                prop_recipe_id BIGINT,
                
                -- KST ì‹œê°„ ì»¬ëŸ¼ (í•œêµ­ ì‹œê°„ ê¸°ì¤€)
                kst_date DATE NOT NULL,
                kst_year INT,
                kst_month INT,
                kst_day INT,
                kst_hour INT,
                kst_day_of_week STRING,
                
                -- UTC ì‹œê°„ (ë¹„êµìš©)
                utc_timestamp TIMESTAMP,
                
                -- ì‚¬ìš©ì ì†ì„±
                user_segment STRING,
                cooking_style STRING,
                ab_test_group STRING,
                
                -- ì´ë²¤íŠ¸ ì†ì„±
                prop_list_type STRING,
                prop_action STRING,
                prop_search_keyword STRING,
                prop_result_count INT,
                
                -- ë©”íŠ¸ë¦­
                event_count INT,
                conversion_flag INT,
                engagement_score INT,
                
                -- ë©”íƒ€ë°ì´í„°
                processed_at TIMESTAMP,
                data_source STRING,
                pipeline_version STRING,
                processing_batch STRING,
                gold_processed_at TIMESTAMP
                
            ) USING ICEBERG
            PARTITIONED BY (kst_year, kst_month, kst_day)
            TBLPROPERTIES (
                'format-version' = '2',
                'write.target-file-size-bytes' = '134217728'
            )
            """
            
            self.spark.sql(schema_query)
            print(f"âœ… Fact í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ Fact í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
    def process_all_date_ranges(self):
        """ëª¨ë“  ë‚ ì§œ ë²”ìœ„ ìˆœì°¨ ì²˜ë¦¬"""
        print(f"\nğŸš€ ëª¨ë“  ë‚ ì§œ ë²”ìœ„ ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘...")
        
        total_processed = 0
        processing_times = []
        
        for i, date_range in enumerate(self.date_ranges):
            range_name = date_range['name']
            start_date = date_range['start_date']
            end_date = date_range['end_date']
            description = date_range['description']
            
            print(f"\n{'='*60}")
            print(f"ğŸ“… ì²˜ë¦¬ ì¤‘: {description}")
            print(f"   ë²”ìœ„: {start_date} ~ {end_date}")
            print(f"   ë°°ì¹˜: {range_name}")
            
            # 1. ë°ì´í„° ë¶„ì„
            analysis_result = self.analyze_date_range(start_date, end_date, range_name)
            
            if analysis_result is None:
                print(f"âš ï¸ {range_name} ê±´ë„ˆëœ€ (ë¶„ì„ ì‹¤íŒ¨)")
                continue
                
            # 2. ì²˜ë¦¬ ì‹œì‘
            start_time = time.time()
            
            is_first_range = (i == 0)
            processed_count = self.create_simple_fact_table_for_range(
                start_date, end_date, range_name, is_first_range
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # 3. ê²°ê³¼ ê¸°ë¡
            total_processed += processed_count
            processing_times.append({
                'range': range_name,
                'time': processing_time,
                'events': processed_count
            })
            
            print(f"   â±ï¸ ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„: {processing_time/60:.1f}ë¶„")
            print(f"   ğŸ“Š ëˆ„ì  ì²˜ë¦¬: {total_processed:,}ê°œ")
            
        # 4. ì „ì²´ ê²°ê³¼ ìš”ì•½
        self.summarize_processing_results(processing_times, total_processed)
        
    def summarize_processing_results(self, processing_times, total_processed):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½"""
        print(f"\nğŸ‰ ë‚ ì§œ ë²”ìœ„ ë¶„í•  ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*60}")
        
        total_time = sum(item['time'] for item in processing_times)
        
        print(f"ğŸ“Š ì „ì²´ ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   ì´ ì²˜ë¦¬ ì´ë²¤íŠ¸: {total_processed:,}ê°œ")
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {total_time/60:.1f}ë¶„ ({total_time/3600:.1f}ì‹œê°„)")
        
        print(f"\nğŸ“… ë²”ìœ„ë³„ ì²˜ë¦¬ ì‹œê°„:")
        for item in processing_times:
            efficiency = item['events'] / item['time'] if item['time'] > 0 else 0
            print(f"   {item['range']}: {item['time']/60:.1f}ë¶„ ({item['events']:,}ê°œ, {efficiency:.0f}ê°œ/ì´ˆ)")
        
        # ìµœì¢… ê²€ì¦
        self.validate_gold_table()
        
    def validate_gold_table(self):
        """Gold í…Œì´ë¸” ê²€ì¦"""
        print(f"\nğŸ” Gold í…Œì´ë¸” ê²€ì¦...")
        
        try:
            validation_query = f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT processing_batch) as unique_batches,
                COUNT(DISTINCT kst_date) as unique_kst_dates,
                COUNT(DISTINCT user_id) as unique_users,
                COUNT(DISTINCT event_name) as unique_events,
                SUM(conversion_flag) as total_conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement,
                MIN(kst_date) as min_kst_date,
                MAX(kst_date) as max_kst_date,
                MIN(utc_timestamp) as min_utc,
                MAX(utc_timestamp) as max_utc
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events_simple
            """
            
            stats = self.spark.sql(validation_query).collect()[0]
            
            print(f"âœ… Gold í…Œì´ë¸” ê²€ì¦ ê²°ê³¼:")
            print(f"   ì´ ë ˆì½”ë“œ: {stats.total_records:,}ê°œ")
            print(f"   ì²˜ë¦¬ ë°°ì¹˜: {stats.unique_batches}ê°œ")
            print(f"   ê³ ìœ  KST ë‚ ì§œ: {stats.unique_kst_dates}ê°œ")
            print(f"   ê³ ìœ  ì‚¬ìš©ì: {stats.unique_users:,}ëª…")
            print(f"   ê³ ìœ  ì´ë²¤íŠ¸: {stats.unique_events}ê°œ")
            print(f"   ì´ ì „í™˜: {stats.total_conversions:,}ê±´")
            print(f"   í‰ê·  ì°¸ì—¬ë„: {stats.avg_engagement}")
            
            print(f"\nğŸ‡°ğŸ‡· KST vs UTC ì‹œê°„ ë¹„êµ:")
            print(f"   KST ë‚ ì§œ ë²”ìœ„: {stats.min_kst_date} ~ {stats.max_kst_date}")
            print(f"   UTC ì‹œê°„ ë²”ìœ„: {stats.min_utc} ~ {stats.max_utc}")
            
            # ë°°ì¹˜ë³„ í†µê³„
            batch_stats = self.spark.sql(f"""
            SELECT 
                processing_batch,
                COUNT(*) as records,
                COUNT(DISTINCT kst_date) as dates,
                MIN(kst_date) as min_date,
                MAX(kst_date) as max_date
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events_simple
            GROUP BY processing_batch
            ORDER BY min_date
            """).collect()
            
            print(f"\nğŸ“… ë°°ì¹˜ë³„ ìƒì„¸ í†µê³„:")
            for row in batch_stats:
                print(f"   {row.processing_batch}: {row.records:,}ê°œ ({row.min_date} ~ {row.max_date}, {row.dates}ì¼)")
                
        except Exception as e:
            print(f"âŒ Gold í…Œì´ë¸” ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            
    def execute_date_range_processing(self):
        """ë‚ ì§œ ë²”ìœ„ ë¶„í•  ì²˜ë¦¬ ì „ì²´ ì‹¤í–‰"""
        print("ğŸš€ ë‚ ì§œ ë²”ìœ„ ë¶„í•  Gold Layer ì²˜ë¦¬ ì‹œì‘...")
        print("=" * 60)
        
        try:
            # 1. Spark ì„¸ì…˜ ìƒì„±
            self.create_optimized_spark_session()
            
            # 2. Gold ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            self.create_gold_database()
            
            # 3. Fact í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ìƒì„±
            self.create_fact_table_schema()
            
            # 4. ì „ì²´ ë‚ ì§œ ë²”ìœ„ ì²˜ë¦¬
            self.process_all_date_ranges()
            
            print(f"\nğŸ‰ ë‚ ì§œ ë²”ìœ„ ë¶„í•  ì²˜ë¦¬ ì™„ì „ ì„±ê³µ!")
            print(f"   âœ… KST ì‹œê°„ ì»¬ëŸ¼ í™œìš©")
            print(f"   âœ… ë©”ëª¨ë¦¬ ìµœì í™”ëœ ë¶„í•  ì²˜ë¦¬")
            print(f"   âœ… ì•ˆì „í•œ ë°°ì¹˜ë³„ ì‹¤í–‰")
            
        except Exception as e:
            print(f"âŒ ë‚ ì§œ ë²”ìœ„ ë¶„í•  ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    processor = DateRangeGoldProcessor()
    processor.execute_date_range_processing()
