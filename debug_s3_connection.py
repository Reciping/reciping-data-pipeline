#!/usr/bin/env python3
"""
S3 ì—°ê²° ë¬¸ì œ ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
Airflow DAG ì‹¤í–‰ ì „ì— S3 ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

from pyspark.sql import SparkSession
import time

def test_s3_connection():
    print("ğŸ” S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # SparkSession ìƒì„±
    spark = SparkSession.builder \
        .appName("S3ConnectionTest") \
        .config("spark.hadoop.fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.profile.ProfileCredentialsProvider") \
        .config("spark.hadoop.fs.s3a.connection.timeout", "30000") \
        .config("spark.hadoop.fs.s3a.connection.establish.timeout", "30000") \
        .config("spark.hadoop.fs.s3a.attempts.maximum", "3") \
        .getOrCreate()
    
    try:
        s3_path = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
        print(f"ğŸ“‚ í…ŒìŠ¤íŠ¸ ê²½ë¡œ: {s3_path}")
        
        # 1. ê²½ë¡œ ì¡´ì¬ í™•ì¸
        print("1ï¸âƒ£ ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸...")
        hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
        fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)
        path_obj = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(s3_path)
        
        start_time = time.time()
        path_exists = fs.exists(path_obj)
        elapsed = time.time() - start_time
        
        print(f"   ê²°ê³¼: {'ì¡´ì¬í•¨' if path_exists else 'ì¡´ì¬í•˜ì§€ ì•ŠìŒ'} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        
        if not path_exists:
            print("âŒ S3 ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            return False
        
        # 2. íŒŒì¼ ëª©ë¡ í™•ì¸
        print("2ï¸âƒ£ íŒŒì¼ ëª©ë¡ í™•ì¸...")
        start_time = time.time()
        file_status = fs.listStatus(path_obj)
        elapsed = time.time() - start_time
        
        print(f"   ì°¾ì€ íŒŒì¼ ìˆ˜: {len(file_status)} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        
        for i, file_stat in enumerate(file_status[:5]):  # ì²˜ìŒ 5ê°œë§Œ
            file_path = file_stat.getPath().toString()
            file_size = file_stat.getLen()
            print(f"   {i+1}. {file_path.split('/')[-1]} ({file_size:,} bytes)")
        
        if len(file_status) == 0:
            print("âŒ ì½ì„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            return False
        
        # 3. ì‘ì€ ìƒ˜í”Œ ë°ì´í„° ì½ê¸° í…ŒìŠ¤íŠ¸
        print("3ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„° ì½ê¸° í…ŒìŠ¤íŠ¸...")
        start_time = time.time()
        
        sample_df = spark.read \
            .option("multiline", "false") \
            .option("mode", "PERMISSIVE") \
            .json(s3_path) \
            .limit(10)
        
        sample_count = sample_df.count()
        elapsed = time.time() - start_time
        
        print(f"   ì½ì€ ìƒ˜í”Œ í–‰ ìˆ˜: {sample_count} (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        
        if sample_count > 0:
            print("âœ… S3 ì—°ê²° ë° ë°ì´í„° ì½ê¸° ì„±ê³µ!")
            
            # ìƒ˜í”Œ ë°ì´í„° êµ¬ì¡° í™•ì¸
            print("4ï¸âƒ£ ë°ì´í„° êµ¬ì¡° í™•ì¸...")
            sample_df.printSchema()
            print("\nìƒ˜í”Œ ë°ì´í„°:")
            sample_df.show(1, truncate=False)
            
            return True
        else:
            print("âŒ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return False
            
    except Exception as e:
        print(f"âŒ S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return False
        
    finally:
        spark.stop()

def test_hive_metastore():
    print("\nğŸ—„ï¸ Hive Metastore ì—°ê²° í…ŒìŠ¤íŠ¸...")
    
    spark = SparkSession.builder \
        .appName("HiveMetastoreTest") \
        .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
        .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://10.0.11.86:9083") \
        .getOrCreate()
    
    try:
        # Hive Metastore ì—°ê²° í…ŒìŠ¤íŠ¸
        databases = spark.sql("SHOW DATABASES").collect()
        print(f"âœ… Hive Metastore ì—°ê²° ì„±ê³µ! ë°ì´í„°ë² ì´ìŠ¤ ìˆ˜: {len(databases)}")
        
        for db in databases[:5]:  # ì²˜ìŒ 5ê°œë§Œ
            print(f"   - {db['databaseName']}")
            
        return True
        
    except Exception as e:
        print(f"âŒ Hive Metastore ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return False
        
    finally:
        spark.stop()

if __name__ == "__main__":
    print("ğŸ”§ ETL íŒŒì´í”„ë¼ì¸ ì‚¬ì „ ì§„ë‹¨ ì‹œì‘")
    print("=" * 50)
    
    s3_ok = test_s3_connection()
    hive_ok = test_hive_metastore()
    
    print("\n" + "=" * 50)
    print("ğŸ“‹ ì§„ë‹¨ ê²°ê³¼ ìš”ì•½:")
    print(f"   S3 ì—°ê²°: {'âœ… ì •ìƒ' if s3_ok else 'âŒ ë¬¸ì œ'}")
    print(f"   Hive Metastore: {'âœ… ì •ìƒ' if hive_ok else 'âŒ ë¬¸ì œ'}")
    
    if s3_ok and hive_ok:
        print("\nğŸ‰ ëª¨ë“  ì—°ê²°ì´ ì •ìƒì…ë‹ˆë‹¤! ETL íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ì˜¤ë¥˜ë¥¼ í•´ê²° í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
