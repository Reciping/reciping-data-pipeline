#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ Fact í…Œì´ë¸” ìƒì„±
- ë©”ëª¨ë¦¬ ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°ë¡œ ì ì§„ì  ì²˜ë¦¬
- SQL ì¿¼ë¦¬ ë ˆë²¨ì—ì„œ ë°°ì¹˜ ë¶„í• 
- ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
"""

from pyspark.sql import SparkSession
import time
from datetime import datetime, timedelta

class SmartBatchProcessor:
    """ë©”ëª¨ë¦¬ ì•ˆì „í•œ ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        
        # ë°°ì¹˜ ì„¤ì •
        self.batch_size = 15000  # ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
        self.date_batch_size = 2  # ì¼ë³„ ë°°ì¹˜ í¬ê¸°
        
    def create_spark_session(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”ëœ SparkSession"""
        print("ğŸ”§ ë°°ì¹˜ ì²˜ë¦¬ìš© SparkSession ìƒì„±...")
        
        self.spark = SparkSession.builder \
            .appName("SmartBatchProcessor") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.driver.maxResultSize", "512m") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.shuffle.partitions", "50") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.adaptive.skewJoin.enabled", "true") \
            .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "64MB") \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… ë°°ì¹˜ ì²˜ë¦¬ìš© SparkSession ìƒì„± ì™„ë£Œ")
        
    def analyze_silver_data_distribution(self):
        """Silver ë°ì´í„° ë¶„í¬ ë¶„ì„ìœ¼ë¡œ ìµœì  ë°°ì¹˜ ì „ëµ ìˆ˜ë¦½"""
        print("\nğŸ“Š Silver ë°ì´í„° ë¶„í¬ ë¶„ì„...")
        
        # 1. ë‚ ì§œë³„ ë°ì´í„° ë¶„í¬
        date_distribution = self.spark.sql(f"""
        SELECT 
            date,
            COUNT(*) as event_count,
            COUNT(DISTINCT user_id) as unique_users,
            COUNT(DISTINCT session_id) as unique_sessions
        FROM {self.catalog_name}.{self.silver_database}.user_events_silver
        WHERE date >= '2025-07-01' AND date <= '2025-07-31'
        GROUP BY date
        ORDER BY date
        """).collect()
        
        print("   ğŸ“… ë‚ ì§œë³„ ë°ì´í„° ë¶„í¬:")
        total_events = 0
        for row in date_distribution:
            total_events += row.event_count
            print(f"      {row.date}: {row.event_count:,}ì´ë²¤íŠ¸, {row.unique_users:,}ì‚¬ìš©ì, {row.unique_sessions:,}ì„¸ì…˜")
            
        print(f"   ğŸ“Š ì´ ì´ë²¤íŠ¸: {total_events:,}ê°œ")
        
        # 2. ë°°ì¹˜ ì „ëµ ê²°ì •
        avg_events_per_day = total_events / len(date_distribution)
        recommended_days_per_batch = max(1, int(self.batch_size / avg_events_per_day))
        
        print(f"\nğŸ¯ ë°°ì¹˜ ì „ëµ:")
        print(f"   í‰ê·  ì¼ì¼ ì´ë²¤íŠ¸: {avg_events_per_day:,.0f}ê°œ")
        print(f"   ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: {self.batch_size:,}ê°œ ì´ë²¤íŠ¸")
        print(f"   ê¶Œì¥ ì¼ë³„ ë°°ì¹˜: {recommended_days_per_batch}ì¼ì”©")
        
        self.date_batch_size = recommended_days_per_batch
        return date_distribution
        
    def create_batched_fact_table(self):
        """ë°°ì¹˜ë³„ë¡œ Fact í…Œì´ë¸” ìƒì„±"""
        print("\nğŸ”„ ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ì²˜ë¦¬ë¡œ Fact í…Œì´ë¸” ìƒì„± ì‹œì‘...")
        
        # 1. ë°ì´í„° ë¶„í¬ ë¶„ì„
        date_distribution = self.analyze_silver_data_distribution()
        
        # 2. ë‚ ì§œ ë²”ìœ„ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
        dates = [row.date for row in date_distribution]
        date_batches = []
        for i in range(0, len(dates), self.date_batch_size):
            batch = dates[i:i + self.date_batch_size]
            date_batches.append((batch[0], batch[-1]))
            
        print(f"\nğŸ“¦ ì´ {len(date_batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• :")
        for i, (start_date, end_date) in enumerate(date_batches):
            expected_events = sum(row.event_count for row in date_distribution 
                                if start_date <= row.date <= end_date)
            print(f"   ë°°ì¹˜ {i+1}: {start_date} ~ {end_date} (ì˜ˆìƒ: {expected_events:,}ì´ë²¤íŠ¸)")
            
        # 3. ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ Fact í…Œì´ë¸” ì´ˆê¸°í™”
        first_start, first_end = date_batches[0]
        print(f"\nğŸ¬ ì²« ë²ˆì§¸ ë°°ì¹˜ ì²˜ë¦¬: {first_start} ~ {first_end}")
        
        success_count = self.process_fact_batch(first_start, first_end, is_first_batch=True)
        
        if success_count == 0:
            print("âŒ ì²« ë²ˆì§¸ ë°°ì¹˜ ì‹¤íŒ¨ - ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„")
            return self.process_micro_batches(first_start, first_end)
            
        # 4. ë‚˜ë¨¸ì§€ ë°°ì¹˜ë“¤ ìˆœì°¨ ì²˜ë¦¬
        total_processed = success_count
        for i, (start_date, end_date) in enumerate(date_batches[1:], 2):
            print(f"\nğŸ”„ ë°°ì¹˜ {i} ì²˜ë¦¬: {start_date} ~ {end_date}")
            
            batch_count = self.process_fact_batch(start_date, end_date, is_first_batch=False)
            
            if batch_count > 0:
                total_processed += batch_count
                print(f"   âœ… ë°°ì¹˜ {i} ì„±ê³µ: {batch_count:,}ê°œ ì¶”ê°€ (ëˆ„ì : {total_processed:,}ê°œ)")
            else:
                print(f"   âš ï¸ ë°°ì¹˜ {i} ì‹¤íŒ¨ - ìŠ¤í‚µ")
                
        print(f"\nğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì´ ì²˜ë¦¬ëœ ë ˆì½”ë“œ: {total_processed:,}ê°œ")
        
        # 5. ìµœì¢… ê²€ì¦
        self.validate_batched_result()
        
        return total_processed
        
    def process_fact_batch(self, start_date, end_date, is_first_batch=False):
        """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬"""
        try:
            # ë°°ì¹˜ë³„ ì•ˆì „í•œ SQL ì¿¼ë¦¬
            batch_query = f"""
            WITH silver_batch AS (
                SELECT 
                    event_id, user_id, session_id, anonymous_id, event_name,
                    page_name, prop_recipe_id, utc_timestamp, date, prop_action
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                WHERE date >= '{start_date}' AND date <= '{end_date}'
            ),
            fact_batch AS (
                SELECT 
                    s.event_id,
                    COALESCE(u.user_dim_key, 0) as user_dim_key,
                    CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                    COALESCE(r.recipe_dim_key, 0) as recipe_dim_key,
                    COALESCE(p.page_dim_key, 0) as page_dim_key,
                    COALESCE(e.event_dim_key, 1) as event_dim_key,
                    
                    1 as event_count,
                    CAST(RAND() * 120 AS INT) as session_duration_seconds,
                    30 as page_view_duration_seconds,
                    
                    COALESCE(e.is_conversion_event, FALSE) as is_conversion,
                    COALESCE(e.conversion_value, 1.0) as conversion_value,
                    
                    CASE 
                        WHEN s.event_name = 'auth_success' THEN 10.0
                        WHEN s.event_name = 'create_comment' THEN 9.0
                        WHEN s.event_name = 'click_bookmark' THEN 8.0
                        WHEN s.event_name = 'click_recipe' THEN 7.0
                        WHEN s.event_name = 'search_recipe' THEN 5.0
                        WHEN s.event_name = 'view_recipe' THEN 4.0
                        WHEN s.event_name = 'view_page' THEN 2.0
                        ELSE 1.0
                    END as engagement_score,
                    
                    s.session_id,
                    s.anonymous_id,
                    CURRENT_TIMESTAMP() as created_at,
                    CURRENT_TIMESTAMP() as updated_at
                    
                FROM silver_batch s
                LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_users u 
                    ON s.user_id = u.user_id AND u.is_current = TRUE
                LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r 
                    ON s.prop_recipe_id = r.recipe_id
                LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p 
                    ON s.page_name = p.page_name
                LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_events e 
                    ON s.event_name = e.event_name
                WHERE s.event_id IS NOT NULL
            )
            
            {"INSERT OVERWRITE" if is_first_batch else "INSERT INTO"} {self.catalog_name}.{self.gold_database}.fact_user_events
            SELECT * FROM fact_batch
            """
            
            # ë°°ì¹˜ ì‹¤í–‰
            start_time = time.time()
            self.spark.sql(batch_query)
            execution_time = time.time() - start_time
            
            # ë°°ì¹˜ ê²°ê³¼ í™•ì¸
            if is_first_batch:
                batch_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            else:
                # ìƒˆë¡œ ì¶”ê°€ëœ ë ˆì½”ë“œ ìˆ˜ ì¶”ì •
                total_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
                # ì´ì „ ë°°ì¹˜ì™€ì˜ ì°¨ì´ë¡œ í˜„ì¬ ë°°ì¹˜ í¬ê¸° ì¶”ì • (ì •í™•í•˜ì§€ ì•Šì§€ë§Œ ê·¼ì‚¬ê°’)
                silver_batch_count = self.spark.sql(f"""
                SELECT COUNT(*) as cnt 
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver 
                WHERE date >= '{start_date}' AND date <= '{end_date}'
                """).collect()[0]['cnt']
                batch_count = min(silver_batch_count, total_count)  # ë³´ìˆ˜ì  ì¶”ì •
                
            print(f"      â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ")
            print(f"      ğŸ“Š ì²˜ë¦¬ ë ˆì½”ë“œ: {batch_count:,}ê°œ")
            
            return batch_count
            
        except Exception as e:
            print(f"      âŒ ë°°ì¹˜ ì‹¤íŒ¨: {str(e)[:100]}...")
            return 0
            
    def process_micro_batches(self, start_date, end_date):
        """ì´ˆì†Œí˜• ë°°ì¹˜ ì²˜ë¦¬ (fallback)"""
        print(f"\nğŸ”¬ ì´ˆì†Œí˜• ë°°ì¹˜ ì²˜ë¦¬: {start_date} ~ {end_date}")
        
        # ë‚ ì§œë³„ë¡œ í•˜ë‚˜ì”© ì²˜ë¦¬
        current_date = datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')
        
        total_processed = 0
        is_first = True
        
        while current_date <= end_date_obj:
            date_str = current_date.strftime('%Y-%m-%d')
            print(f"   ğŸ“… ì²˜ë¦¬ ì¤‘: {date_str}")
            
            try:
                # í•˜ë£¨ì¹˜ ë°ì´í„°ë§Œ ì²˜ë¦¬
                daily_count = self.process_fact_batch(date_str, date_str, is_first_batch=is_first)
                
                if daily_count > 0:
                    total_processed += daily_count
                    print(f"      âœ… ì„±ê³µ: {daily_count:,}ê°œ")
                    is_first = False
                else:
                    print(f"      âš ï¸ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
                    
            except Exception as e:
                print(f"      âŒ ì˜¤ë¥˜: {str(e)[:50]}...")
                
            current_date += timedelta(days=1)
            
        print(f"\nğŸ“Š ì´ˆì†Œí˜• ë°°ì¹˜ ì™„ë£Œ: {total_processed:,}ê°œ ì²˜ë¦¬")
        return total_processed
        
    def validate_batched_result(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦"""
        print("\nğŸ” ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦...")
        
        try:
            final_stats = self.spark.sql(f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT user_dim_key) as unique_users,
                COUNT(DISTINCT recipe_dim_key) as unique_recipes,
                COUNT(DISTINCT time_dim_key) as unique_time_keys,
                COUNT(DISTINCT session_id) as unique_sessions,
                SUM(CASE WHEN user_dim_key > 0 THEN 1 ELSE 0 END) as mapped_users,
                SUM(CASE WHEN recipe_dim_key > 0 THEN 1 ELSE 0 END) as mapped_recipes,
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            print("ğŸ“Š ìµœì¢… ë°°ì¹˜ ê²°ê³¼:")
            print(f"   ì´ ë ˆì½”ë“œ: {final_stats.total_records:,}ê°œ")
            print(f"   ê³ ìœ  ì‚¬ìš©ì: {final_stats.unique_users:,}ëª…")
            print(f"   ê³ ìœ  ë ˆì‹œí”¼: {final_stats.unique_recipes:,}ê°œ")
            print(f"   ê³ ìœ  ì‹œê°„í‚¤: {final_stats.unique_time_keys:,}ê°œ")
            print(f"   ê³ ìœ  ì„¸ì…˜: {final_stats.unique_sessions:,}ê°œ")
            
            user_mapping_pct = (final_stats.mapped_users / final_stats.total_records) * 100
            recipe_mapping_pct = (final_stats.mapped_recipes / final_stats.total_records) * 100
            
            print(f"   ì‚¬ìš©ì ë§¤í•‘: {user_mapping_pct:.1f}% ({final_stats.mapped_users:,}ê°œ)")
            print(f"   ë ˆì‹œí”¼ ë§¤í•‘: {recipe_mapping_pct:.1f}% ({final_stats.mapped_recipes:,}ê°œ)")
            print(f"   ì „í™˜ ì´ë²¤íŠ¸: {final_stats.conversions:,}ê°œ")
            print(f"   í‰ê·  ì°¸ì—¬ë„: {final_stats.avg_engagement}ì ")
            
            # ì„±ê³µ í‰ê°€
            if final_stats.total_records >= 50000:
                print("\nğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ!")
                print(f"   âœ… ëŒ€ìš©ëŸ‰ ë°ì´í„° ì•ˆì „ ì²˜ë¦¬")
                print(f"   âœ… JVM í¬ë˜ì‹œ ì—†ì´ ì™„ë£Œ")
                print(f"   âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ê°€ëŠ¥")
            else:
                print("\nâš ï¸ ë¶€ë¶„ ì„±ê³µ")
                print(f"   ğŸ’¡ ë” ì‘ì€ ë°°ì¹˜ í¬ê¸° ê¶Œì¥")
                
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            
    def execute_smart_batch_processing(self):
        """ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì „ì²´ ì‹¤í–‰"""
        print("ğŸš€ ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰...")
        print("=" * 60)
        
        try:
            # 1. SparkSession ìƒì„±
            self.create_spark_session()
            
            # 2. ë°°ì¹˜ë³„ Fact í…Œì´ë¸” ìƒì„±
            total_processed = self.create_batched_fact_table()
            
            if total_processed > 0:
                print(f"\nğŸ‰ ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ!")
                print(f"   âœ… ì´ {total_processed:,}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬")
                print(f"   âœ… ë©”ëª¨ë¦¬ ì•ˆì „ í™•ë³´")
                print(f"   âœ… ì ì§„ì  í™•ì¥ ê°€ëŠ¥")
            else:
                print(f"\nâŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨")
                print(f"   ğŸ’¡ ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë‚˜ ë‹¤ë¥¸ ì „ëµ í•„ìš”")
                
        except Exception as e:
            print(f"âŒ ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    batch_processor = SmartBatchProcessor()
    batch_processor.execute_smart_batch_processing()
