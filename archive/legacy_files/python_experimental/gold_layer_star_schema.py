# ================================================================================
# ğŸŒŸ Gold Layer Star Schema êµ¬ì¶• íŒŒì´í”„ë¼ì¸
# ================================================================================
# 
# ëª©ì : Silver Layer(user_events_silver)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Star Schema í˜•íƒœì˜ 
#      Gold Layerë¥¼ êµ¬ì¶•í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê³¼ BI ë¦¬í¬íŒ…ì„ ì§€ì›
#
# ì•„í‚¤í…ì²˜:
# ğŸ¥ˆ Silver: recipe_analytics.user_events_silver
# ğŸ¥‡ Gold:   gold_analytics.{dim_users, dim_time, dim_recipes, fact_user_events}
#
# S3 êµ¬ì¡°:
# s3://reciping-user-event-logs/iceberg/warehouse/
# â”œâ”€â”€ recipe_analytics.db/user_events_silver/    # Silver Layer
# â””â”€â”€ gold_analytics.db/                         # Gold Layer
#     â”œâ”€â”€ dim_users/                            # ğŸ‘¤ ì‚¬ìš©ì ì°¨ì›
#     â”œâ”€â”€ dim_time/                             # ğŸ“… ì‹œê°„ ì°¨ì›  
#     â”œâ”€â”€ dim_recipes/                          # ğŸ³ ë ˆì‹œí”¼ ì°¨ì›
#     â”œâ”€â”€ fact_user_events/                     # ğŸ“Š ì´ë²¤íŠ¸ íŒ©íŠ¸
#     â””â”€â”€ metrics_*/                            # ğŸ“ˆ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
# ================================================================================

# gold_layer_star_schema.py
"""
Gold Layer Star Schema êµ¬ì¶• ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ ìƒì„±
- Silver Layer â†’ Star Schema ë³€í™˜
- DAU, Weekly Retention, Recipe Analytics ì§€í‘œ ìƒì„±
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime, timedelta
import os

class GoldLayerStarSchema:
    """Gold Layer Star Schema êµ¬ì¶• ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ìƒì„±"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"  # Silver Layer DB
        self.gold_database = "gold_analytics"      # Gold Layer DB
        self.spark = None
        
    def create_spark_session(self):
        """Iceberg + Hive Metastore SparkSession ìƒì„± (ë©”ëª¨ë¦¬ ìµœì í™”)"""
        print("ğŸ§Š Gold Layer SparkSession ìƒì„± ì¤‘...")
        
        self.spark = SparkSession.builder \
            .appName("GoldLayer_StarSchema_Analytics") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "4g") \
            .config("spark.executor.memory", "4g") \
            .config("spark.driver.maxResultSize", "2g") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB") \
            .config("spark.sql.adaptive.skewJoin.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .config("spark.sql.adaptive.localShuffleReader.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.parallelismFirst", "false") \
            .config("spark.sql.adaptive.coalescePartitions.minPartitionSize", "1MB") \
            .config("spark.sql.adaptive.coalescePartitions.initialPartitionNum", "200") \
            .config("spark.sql.join.preferSortMergeJoin", "true") \
            .config("spark.sql.sources.partitionOverwriteMode", "dynamic") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… Gold Layer SparkSession ìƒì„± ì™„ë£Œ!")
        print(f"   Driver Memory: 4GB, Executor Memory: 4GB")
        print(f"   Adaptive Query Execution: Enabled")
        print(f"   Kryo Serializer: Enabled")
        
    def create_gold_database(self):
        """Gold Analytics ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
        print(f"\nğŸ—ï¸ Gold Analytics ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
        
        # Gold Analytics ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.catalog_name}.{self.gold_database}")
        print(f"âœ… {self.catalog_name}.{self.gold_database} ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
        
    def create_dimension_tables(self):
        """Star Schema Dimension í…Œì´ë¸”ë“¤ ìƒì„±"""
        print("\nğŸŒŸ Star Schema Dimension í…Œì´ë¸” ìƒì„± ì¤‘...")
        
        # 1. Time Dimension ìƒì„±
        print("ğŸ“… Time Dimension ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.dim_time (
                time_dim_key BIGINT NOT NULL,
                full_date DATE NOT NULL,
                year INT,
                quarter INT,
                month INT,
                month_name STRING,
                week_of_year INT,
                day_of_month INT,
                day_of_week INT,
                day_name STRING,
                hour INT,
                is_weekend BOOLEAN,
                is_holiday BOOLEAN,
                season STRING,
                business_quarter STRING,
                days_from_today INT,
                weeks_from_today INT,
                months_from_today INT,
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            TBLPROPERTIES (
                'format-version' = '2',
                'write.target-file-size-bytes' = '67108864'
            )
        """)
        
        # 2. User Dimension ìƒì„±  
        print("ğŸ‘¤ User Dimension ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.dim_users (
                user_dim_key BIGINT NOT NULL,
                user_id STRING NOT NULL,
                user_segment STRING,
                cooking_style STRING,
                ab_test_group STRING,
                first_seen_date DATE,
                last_activity_date DATE,
                total_sessions BIGINT,
                total_recipe_views BIGINT,
                user_tier STRING,
                lifetime_value DECIMAL(10,2),
                effective_date DATE NOT NULL,
                expiry_date DATE,
                is_current BOOLEAN,
                created_at TIMESTAMP NOT NULL,
                updated_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (is_current)
            TBLPROPERTIES (
                'format-version' = '2',
                'write.target-file-size-bytes' = '67108864'
            )
        """)
        
        # 3. Recipe Dimension ìƒì„±
        print("ğŸ³ Recipe Dimension ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.dim_recipes (
                recipe_dim_key BIGINT NOT NULL,
                recipe_id BIGINT,
                recipe_category STRING,
                ingredient_count INT,
                difficulty_level STRING,
                cuisine_type STRING,
                prep_time_minutes INT,
                is_premium BOOLEAN,
                created_at TIMESTAMP NOT NULL,
                updated_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 4. Page Dimension ìƒì„±
        print("ğŸ“± Page Dimension ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.dim_pages (
                page_dim_key BIGINT NOT NULL,
                page_name STRING NOT NULL,
                page_url STRING,
                page_category STRING,
                funnel_stage STRING,
                is_mobile BOOLEAN,
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 5. Event Dimension ìƒì„±
        print("ğŸ¬ Event Dimension ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.dim_events (
                event_dim_key BIGINT NOT NULL,
                event_name STRING NOT NULL,
                event_category STRING,
                conversion_value DECIMAL(5,2),
                is_conversion_event BOOLEAN,
                event_weight DECIMAL(3,2),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        print("âœ… ëª¨ë“  Dimension í…Œì´ë¸” ìƒì„± ì™„ë£Œ!")
        
    def create_fact_table(self):
        """Star Schema Fact í…Œì´ë¸” ìƒì„±"""
        print("\nğŸ“Š Fact í…Œì´ë¸” ìƒì„± ì¤‘...")
        
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.fact_user_events (
                event_id STRING NOT NULL,
                user_dim_key BIGINT NOT NULL,
                time_dim_key BIGINT NOT NULL,
                recipe_dim_key BIGINT,
                page_dim_key BIGINT,
                event_dim_key BIGINT NOT NULL,
                
                -- Additive Measures
                event_count BIGINT,
                session_duration_seconds BIGINT,
                page_view_duration_seconds BIGINT,
                
                -- Semi-Additive Measures
                is_conversion BOOLEAN,
                conversion_value DECIMAL(10,2),
                engagement_score DECIMAL(5,2),
                
                -- Degenerate Dimensions
                session_id STRING,
                anonymous_id STRING,
                
                -- ETL Metadata
                created_at TIMESTAMP NOT NULL,
                updated_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (time_dim_key)
            TBLPROPERTIES (
                'format-version' = '2',
                'write.target-file-size-bytes' = '134217728'
            )
        """)
        
        print("âœ… Fact í…Œì´ë¸” ìƒì„± ì™„ë£Œ!")
        
    def populate_time_dimension(self):
        """Time Dimension ë°ì´í„° ì±„ìš°ê¸°"""
        print("\nğŸ“… Time Dimension ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ì‹¤ì œ ë°ì´í„° ë²”ìœ„ì— ë§ì¶° ì‹œê°„ ë°ì´í„° ìƒì„± (2025-07-01 ~ 2025-07-31)
        time_data_query = """
        WITH date_range AS (
            SELECT SEQUENCE(
                TO_DATE('2025-07-01'), 
                TO_DATE('2025-07-31'), 
                INTERVAL 1 DAY
            ) as date_array
        ),
        time_expanded AS (
            SELECT 
                EXPLODE(date_array) as full_date
            FROM date_range
        ),
        time_with_hours AS (
            SELECT 
                full_date,
                hour_val
            FROM time_expanded
            CROSS JOIN (SELECT EXPLODE(SEQUENCE(0, 23)) as hour_val)
        )
        
        INSERT OVERWRITE iceberg_catalog.gold_analytics.dim_time
        SELECT 
            CAST(DATE_FORMAT(full_date, 'yyyyMMdd') AS BIGINT) * 100 + hour_val as time_dim_key,
            full_date,
            YEAR(full_date) as year,
            QUARTER(full_date) as quarter,
            MONTH(full_date) as month,
            DATE_FORMAT(full_date, 'MMMM') as month_name,
            WEEKOFYEAR(full_date) as week_of_year,
            DAYOFMONTH(full_date) as day_of_month,
            DAYOFWEEK(full_date) as day_of_week,
            DATE_FORMAT(full_date, 'EEEE') as day_name,
            hour_val as hour,
            CASE WHEN DAYOFWEEK(full_date) IN (1, 7) THEN TRUE ELSE FALSE END as is_weekend,
            FALSE as is_holiday,  -- ì¶”í›„ íœ´ì¼ ë°ì´í„° ì¶”ê°€ ê°€ëŠ¥
            CASE 
                WHEN MONTH(full_date) IN (12, 1, 2) THEN 'Winter'
                WHEN MONTH(full_date) IN (3, 4, 5) THEN 'Spring'
                WHEN MONTH(full_date) IN (6, 7, 8) THEN 'Summer'
                ELSE 'Fall'
            END as season,
            CONCAT('Q', QUARTER(full_date), '-', YEAR(full_date)) as business_quarter,
            DATEDIFF(full_date, CURRENT_DATE()) as days_from_today,
            CAST(DATEDIFF(full_date, CURRENT_DATE()) / 7 AS INT) as weeks_from_today,
            MONTHS_BETWEEN(full_date, CURRENT_DATE()) as months_from_today,
            CURRENT_TIMESTAMP() as created_at
        FROM time_with_hours
        """
        
        self.spark.sql(time_data_query)
        
        # ê²°ê³¼ í™•ì¸
        time_count = self.spark.sql("SELECT COUNT(*) as cnt FROM iceberg_catalog.gold_analytics.dim_time").collect()[0]['cnt']
        print(f"âœ… Time Dimension ìƒì„± ì™„ë£Œ: {time_count:,}ê°œ ë ˆì½”ë“œ")
        
    def populate_dimensions_from_silver(self):
        """Silver Layerë¡œë¶€í„° Dimension ë°ì´í„° ìƒì„±"""
        print("\nğŸ”„ Silver Layerì—ì„œ Dimension ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        
        # 1. User Dimension ì±„ìš°ê¸°
        print("ğŸ‘¤ User Dimension ë°ì´í„° ìƒì„±...")
        user_dim_query = f"""
        WITH user_aggregates AS (
            SELECT 
                user_id,
                user_segment,
                cooking_style,
                ab_test_group,
                MIN(date) as first_seen_date,
                MAX(date) as last_activity_date,
                COUNT(DISTINCT session_id) as total_sessions,
                COUNT(CASE WHEN event_name = 'view_recipe' THEN 1 END) as total_recipe_views,
                COUNT(*) as total_events
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE user_id IS NOT NULL
            GROUP BY user_id, user_segment, cooking_style, ab_test_group
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.dim_users
        SELECT 
            ROW_NUMBER() OVER (ORDER BY first_seen_date, user_id) as user_dim_key,
            user_id,
            user_segment,
            cooking_style,
            ab_test_group,
            first_seen_date,
            last_activity_date,
            total_sessions,
            total_recipe_views,
            CASE 
                WHEN total_events >= 100 THEN 'Power User'
                WHEN total_events >= 20 THEN 'Active User'
                WHEN total_events >= 5 THEN 'Regular User'
                ELSE 'New User'
            END as user_tier,
            total_events * 0.1 as lifetime_value,  -- ê°„ë‹¨í•œ LTV ê³„ì‚°
            first_seen_date as effective_date,
            NULL as expiry_date,
            TRUE as is_current,
            CURRENT_TIMESTAMP() as created_at,
            CURRENT_TIMESTAMP() as updated_at
        FROM user_aggregates
        """
        
        self.spark.sql(user_dim_query)
        
        # 2. Recipe Dimension ì±„ìš°ê¸°
        print("ğŸ³ Recipe Dimension ë°ì´í„° ìƒì„±...")
        recipe_dim_query = f"""
        WITH recipe_info AS (
            SELECT DISTINCT
                prop_recipe_id as recipe_id
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE prop_recipe_id IS NOT NULL
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.dim_recipes
        SELECT 
            ROW_NUMBER() OVER (ORDER BY recipe_id) as recipe_dim_key,
            recipe_id,
            'Unknown' as recipe_category,  -- ì¶”í›„ ë ˆì‹œí”¼ ë§ˆìŠ¤í„° ë°ì´í„°ì™€ ì¡°ì¸
            5 as ingredient_count,  -- ê¸°ë³¸ê°’
            'Medium' as difficulty_level,  -- ê¸°ë³¸ê°’
            'Korean' as cuisine_type,  -- ê¸°ë³¸ê°’
            30 as prep_time_minutes,  -- ê¸°ë³¸ê°’
            FALSE as is_premium,
            CURRENT_TIMESTAMP() as created_at,
            CURRENT_TIMESTAMP() as updated_at
        FROM recipe_info
        WHERE recipe_id > 0
        
        UNION ALL
        
        SELECT 
            0 as recipe_dim_key,
            NULL as recipe_id,
            'N/A' as recipe_category,
            0 as ingredient_count,
            'N/A' as difficulty_level,
            'N/A' as cuisine_type,
            0 as prep_time_minutes,
            FALSE as is_premium,
            CURRENT_TIMESTAMP() as created_at,
            CURRENT_TIMESTAMP() as updated_at
        """
        
        self.spark.sql(recipe_dim_query)
        
        # 3. Page Dimension ì±„ìš°ê¸°
        print("ğŸ“± Page Dimension ë°ì´í„° ìƒì„±...")
        page_dim_query = f"""
        WITH page_info AS (
            SELECT DISTINCT
                page_name,
                page_url
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE page_name IS NOT NULL
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.dim_pages
        SELECT 
            ROW_NUMBER() OVER (ORDER BY page_name) as page_dim_key,
            page_name,
            page_url,
            CASE 
                WHEN page_name LIKE '%recipe%' THEN 'Recipe'
                WHEN page_name LIKE '%search%' THEN 'Search'
                WHEN page_name LIKE '%list%' THEN 'Browse'
                WHEN page_name LIKE '%auth%' THEN 'Authentication'
                ELSE 'Other'
            END as page_category,
            CASE 
                WHEN page_name = 'home' THEN 'Awareness'
                WHEN page_name LIKE '%list%' THEN 'Interest'
                WHEN page_name LIKE '%recipe%' THEN 'Consideration'
                WHEN page_name LIKE '%auth%' THEN 'Conversion'
                ELSE 'Other'
            END as funnel_stage,
            CASE WHEN page_url LIKE '%mobile%' THEN TRUE ELSE FALSE END as is_mobile,
            CURRENT_TIMESTAMP() as created_at
        FROM page_info
        
        UNION ALL
        
        SELECT 
            0 as page_dim_key,
            'Unknown' as page_name,
            'Unknown' as page_url,
            'Unknown' as page_category,
            'Unknown' as funnel_stage,
            FALSE as is_mobile,
            CURRENT_TIMESTAMP() as created_at
        """
        
        self.spark.sql(page_dim_query)
        
        # 4. Event Dimension ì±„ìš°ê¸°
        print("ğŸ¬ Event Dimension ë°ì´í„° ìƒì„±...")
        event_dim_query = f"""
        WITH event_info AS (
            SELECT DISTINCT event_name
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.dim_events
        SELECT 
            ROW_NUMBER() OVER (ORDER BY event_name) as event_dim_key,
            event_name,
            CASE 
                WHEN event_name LIKE '%view%' THEN 'Engagement'
                WHEN event_name LIKE '%click%' THEN 'Interaction'
                WHEN event_name LIKE '%search%' THEN 'Discovery'
                WHEN event_name LIKE '%auth%' THEN 'Conversion'
                ELSE 'Other'
            END as event_category,
            CASE 
                WHEN event_name = 'auth_success' THEN 10.0
                WHEN event_name = 'click_recipe' THEN 5.0
                WHEN event_name = 'search_recipe' THEN 3.0
                WHEN event_name = 'view_page' THEN 1.0
                ELSE 1.0
            END as conversion_value,
            CASE WHEN event_name IN ('auth_success', 'create_comment', 'click_bookmark') THEN TRUE ELSE FALSE END as is_conversion_event,
            1.0 as event_weight,
            CURRENT_TIMESTAMP() as created_at
        FROM event_info
        """
        
        self.spark.sql(event_dim_query)
        
    def populate_fact_table_hybrid_step1(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• Step 1: ì‘ì€ Dimensionë§Œ JOIN (ì¦‰ì‹œ ê°œì„ )"""
        print("\nğŸ“Š Fact í…Œì´ë¸” í•˜ì´ë¸Œë¦¬ë“œ Step 1: ì‘ì€ Dimension JOIN...")
        
        # ë©”ëª¨ë¦¬ ì•ˆì „í•œ ì‘ì€ Dimensionë§Œ JOIN
        hybrid_step1_query = f"""
        WITH silver_safe AS (
            SELECT 
                event_id,
                user_id,
                session_id,
                anonymous_id,
                event_name,
                page_name,
                prop_recipe_id,
                utc_timestamp,
                date,
                prop_action
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE date >= '2025-07-01' AND date <= '2025-07-31'
            LIMIT 30000  -- ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
        ),
        fact_step1 AS (
            SELECT 
                s.event_id,
                
                -- í° Dimensionì€ ë‚˜ì¤‘ì— (Step 2ì—ì„œ ë°°ì¹˜ ì—…ë°ì´íŠ¸)
                0 as user_dim_key,     -- ğŸ‘ˆ Step 2ì—ì„œ ì—…ë°ì´íŠ¸ ì˜ˆì •
                0 as recipe_dim_key,   -- ğŸ‘ˆ Step 2ì—ì„œ ì—…ë°ì´íŠ¸ ì˜ˆì •
                
                -- ì‘ì€ Dimensionì€ ì¦‰ì‹œ JOIN (ë©”ëª¨ë¦¬ ì•ˆì „)
                CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                COALESCE(p.page_dim_key, 0) as page_dim_key,      -- âœ… 6ê°œ í˜ì´ì§€ë§Œ
                COALESCE(e.event_dim_key, 1) as event_dim_key,    -- âœ… 11ê°œ ì´ë²¤íŠ¸ë§Œ
                
                -- ì •êµí•œ ì¸¡ì •ê°’ ê³„ì‚°
                1 as event_count,
                
                -- Session Duration ì¶”ì¶œ
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 2
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[1] AS BIGINT), 0)
                    ELSE 0
                END as session_duration_seconds,
                
                -- Page View Duration ì¶”ì¶œ
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 3
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[2] AS BIGINT), 30)
                    ELSE 30
                END as page_view_duration_seconds,
                
                -- Event Dimensionì—ì„œ ì „í™˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                COALESCE(e.is_conversion_event, FALSE) as is_conversion,
                COALESCE(e.conversion_value, 1.0) as conversion_value,
                
                -- ì •êµí•œ ì°¸ì—¬ë„ ì ìˆ˜
                CASE 
                    WHEN s.event_name = 'auth_success' THEN 10.0
                    WHEN s.event_name = 'create_comment' THEN 9.0
                    WHEN s.event_name = 'click_bookmark' THEN 8.0
                    WHEN s.event_name = 'click_recipe' THEN 7.0
                    WHEN s.event_name = 'search_recipe' THEN 5.0
                    WHEN s.event_name = 'view_recipe' THEN 4.0
                    WHEN s.event_name = 'view_page' THEN 2.0
                    ELSE 1.0
                END as engagement_score,
                
                -- Degenerate Dimensions
                s.session_id,
                s.anonymous_id,
                
                -- ETL Metadata
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM silver_safe s
            
            -- ë©”ëª¨ë¦¬ ì•ˆì „í•œ ì‘ì€ Dimensionë§Œ JOIN
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p 
                ON s.page_name = p.page_name AND p.page_name != 'Unknown'
                
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_events e 
                ON s.event_name = e.event_name
            
            WHERE s.event_id IS NOT NULL
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
        SELECT * FROM fact_step1
        """
        
        try:
            self.spark.sql(hybrid_step1_query)
            
            # ê²°ê³¼ í™•ì¸
            fact_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"âœ… Step 1 ì™„ë£Œ: {fact_count:,}ê°œ ë ˆì½”ë“œ")
            
            # Step 1 í’ˆì§ˆ ê²€ì¦
            self.validate_step1_quality()
            
        except Exception as e:
            print(f"âŒ Step 1 ì‹¤íŒ¨: {str(e)}")
            
    def validate_step1_quality(self):
        """Step 1 ê²°ê³¼ í’ˆì§ˆ ê²€ì¦"""
        print("\nğŸ” Step 1 í’ˆì§ˆ ê²€ì¦...")
        
        quality_check = self.spark.sql(f"""
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT page_dim_key) as unique_pages,
            COUNT(DISTINCT event_dim_key) as unique_events,
            COUNT(DISTINCT time_dim_key) as unique_time_keys,
            COUNT(DISTINCT session_id) as unique_sessions,
            ROUND(AVG(engagement_score), 2) as avg_engagement,
            SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversions
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        """).collect()[0]
        
        print("ğŸ“Š Step 1 ê²°ê³¼:")
        print(f"   ì´ ë ˆì½”ë“œ: {quality_check['total_records']:,}ê°œ")
        print(f"   ê³ ìœ  í˜ì´ì§€: {quality_check['unique_pages']}ê°œ (ê¸°ëŒ€: 6ê°œ)")
        print(f"   ê³ ìœ  ì´ë²¤íŠ¸: {quality_check['unique_events']}ê°œ (ê¸°ëŒ€: 11ê°œ)")
        print(f"   ê³ ìœ  ì‹œê°„í‚¤: {quality_check['unique_time_keys']:,}ê°œ")
        print(f"   ê³ ìœ  ì„¸ì…˜: {quality_check['unique_sessions']:,}ê°œ")
        print(f"   í‰ê·  ì°¸ì—¬ë„: {quality_check['avg_engagement']}")
        print(f"   ì „í™˜ ì´ë²¤íŠ¸: {quality_check['conversions']:,}ê°œ")
        
        # ì¦‰ì‹œ ê°€ëŠ¥í•œ ë¶„ì„ ì˜ˆì‹œ
        print("\nâœ… Step 1ìœ¼ë¡œ ì¦‰ì‹œ ê°€ëŠ¥í•œ ë¶„ì„:")
        print("   â€¢ í˜ì´ì§€ë³„ ì „í™˜ìœ¨ ë¶„ì„")
        print("   â€¢ ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì°¸ì—¬ë„ ë¶„ì„") 
        print("   â€¢ ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´ ë¶„ì„")
        print("   â€¢ ì„¸ì…˜ ê¸°ë°˜ ì‚¬ìš©ì ì—¬ì • ë¶„ì„")
        
        print("\nâ³ Step 2ì—ì„œ ì¶”ê°€ë  ë¶„ì„:")
        print("   â€¢ ì‚¬ìš©ìë³„ ê°œì¸í™” ë¶„ì„ (user_dim_key ì—…ë°ì´íŠ¸ í›„)")
        print("   â€¢ ë ˆì‹œí”¼ë³„ ì¸ê¸°ë„ ë¶„ì„ (recipe_dim_key ì—…ë°ì´íŠ¸ í›„)")
        
    def populate_fact_table_hybrid_step2(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• Step 2: ì‚¬ìš©ì/ë ˆì‹œí”¼ Dimension ë°°ì¹˜ ì—…ë°ì´íŠ¸"""
        print("\nğŸš€ Fact í…Œì´ë¸” í•˜ì´ë¸Œë¦¬ë“œ Step 2: ëŒ€ìš©ëŸ‰ Dimension ë°°ì¹˜ ì—…ë°ì´íŠ¸...")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì „ëµ
        batch_size = 50000  # ë°°ì¹˜ë‹¹ ì²˜ë¦¬í•  ë ˆì½”ë“œ ìˆ˜
        
        # 1. ì‚¬ìš©ì ì°¨ì› ì—…ë°ì´íŠ¸ (ë°°ì¹˜ ì²˜ë¦¬)
        print("ğŸ‘¤ ì‚¬ìš©ì ì°¨ì› í‚¤ ì—…ë°ì´íŠ¸ ì¤‘...")
        
        # ì‚¬ìš©ìë³„ ë°°ì¹˜ ì—…ë°ì´íŠ¸
        user_update_query = f"""
        WITH user_batches AS (
            SELECT 
                f.event_id,
                f.user_dim_key as current_user_dim_key,
                u.user_dim_key as new_user_dim_key,
                ROW_NUMBER() OVER (ORDER BY f.event_id) as row_num
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
            JOIN {self.catalog_name}.{self.silver_database}.user_events_silver s ON f.event_id = s.event_id
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_users u ON s.user_id = u.user_id AND u.is_current = TRUE
            WHERE f.user_dim_key = 0 AND u.user_dim_key IS NOT NULL
        ),
        batch_1 AS (
            SELECT event_id, new_user_dim_key as user_dim_key
            FROM user_batches 
            WHERE row_num <= {batch_size}
        )
        
        MERGE INTO {self.catalog_name}.{self.gold_database}.fact_user_events AS target
        USING batch_1 AS source ON target.event_id = source.event_id
        WHEN MATCHED THEN UPDATE SET target.user_dim_key = source.user_dim_key
        """
        
        try:
            self.spark.sql(user_update_query)
            print("âœ… ì‚¬ìš©ì ì°¨ì› í‚¤ ì²« ë²ˆì§¸ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì‚¬ìš©ì ì°¨ì› ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ (ì˜ˆìƒë¨): {str(e)}")
            print("ğŸ”„ UPDATE ëŒ€ì‹  ì „ì²´ ì¬êµ¬ì„±ìœ¼ë¡œ ì§„í–‰...")
            
        # 2. ë ˆì‹œí”¼ ì°¨ì› ì—…ë°ì´íŠ¸ (ë°°ì¹˜ ì²˜ë¦¬)
        print("ğŸ³ ë ˆì‹œí”¼ ì°¨ì› í‚¤ ì—…ë°ì´íŠ¸ ì¤‘...")
        
        recipe_update_query = f"""
        WITH recipe_batches AS (
            SELECT 
                f.event_id,
                r.recipe_dim_key as new_recipe_dim_key,
                ROW_NUMBER() OVER (ORDER BY f.event_id) as row_num
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
            JOIN {self.catalog_name}.{self.silver_database}.user_events_silver s ON f.event_id = s.event_id
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r ON s.prop_recipe_id = r.recipe_id
            WHERE f.recipe_dim_key = 0 AND r.recipe_dim_key IS NOT NULL AND s.prop_recipe_id > 0
        ),
        batch_1 AS (
            SELECT event_id, new_recipe_dim_key as recipe_dim_key
            FROM recipe_batches 
            WHERE row_num <= {batch_size}
        )
        
        MERGE INTO {self.catalog_name}.{self.gold_database}.fact_user_events AS target
        USING batch_1 AS source ON target.event_id = source.event_id
        WHEN MATCHED THEN UPDATE SET target.recipe_dim_key = source.recipe_dim_key
        """
        
        try:
            self.spark.sql(recipe_update_query)
            print("âœ… ë ˆì‹œí”¼ ì°¨ì› í‚¤ ì²« ë²ˆì§¸ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë ˆì‹œí”¼ ì°¨ì› ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")
            
        # Step 2 ê²°ê³¼ ê²€ì¦
        self.validate_step2_quality()
        
    def validate_step2_quality(self):
        """Step 2 ê²°ê³¼ í’ˆì§ˆ ê²€ì¦"""
        print("\nğŸ” Step 2 í’ˆì§ˆ ê²€ì¦...")
        
        try:
            quality_check = self.spark.sql(f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT user_dim_key) as unique_users,
                COUNT(DISTINCT recipe_dim_key) as unique_recipes,
                COUNT(DISTINCT page_dim_key) as unique_pages,
                COUNT(DISTINCT event_dim_key) as unique_events,
                SUM(CASE WHEN user_dim_key > 0 THEN 1 ELSE 0 END) as records_with_users,
                SUM(CASE WHEN recipe_dim_key > 0 THEN 1 ELSE 0 END) as records_with_recipes,
                ROUND(AVG(engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            print("ğŸ“Š Step 2 ìµœì¢… ê²°ê³¼:")
            print(f"   ì´ ë ˆì½”ë“œ: {quality_check['total_records']:,}ê°œ")
            print(f"   ê³ ìœ  ì‚¬ìš©ì: {quality_check['unique_users']:,}ëª…")
            print(f"   ê³ ìœ  ë ˆì‹œí”¼: {quality_check['unique_recipes']:,}ê°œ")
            print(f"   ê³ ìœ  í˜ì´ì§€: {quality_check['unique_pages']}ê°œ")
            print(f"   ê³ ìœ  ì´ë²¤íŠ¸: {quality_check['unique_events']}ê°œ")
            print(f"   ì‚¬ìš©ì ë§¤í•‘ëœ ë ˆì½”ë“œ: {quality_check['records_with_users']:,}ê°œ")
            print(f"   ë ˆì‹œí”¼ ë§¤í•‘ëœ ë ˆì½”ë“œ: {quality_check['records_with_recipes']:,}ê°œ")
            print(f"   í‰ê·  ì°¸ì—¬ë„: {quality_check['avg_engagement']}")
            
            user_mapping_rate = (quality_check['records_with_users'] / quality_check['total_records']) * 100
            recipe_mapping_rate = (quality_check['records_with_recipes'] / quality_check['total_records']) * 100
            
            print(f"\nğŸ“ˆ ë§¤í•‘ ì„±ê³µë¥ :")
            print(f"   ì‚¬ìš©ì ë§¤í•‘: {user_mapping_rate:.1f}%")
            print(f"   ë ˆì‹œí”¼ ë§¤í•‘: {recipe_mapping_rate:.1f}%")
            
            if user_mapping_rate > 50 and recipe_mapping_rate > 30:
                print("\nâœ… Step 2 ì„±ê³µ! ì™„ì „í•œ Star Schema ë¶„ì„ ê°€ëŠ¥")
                print("   ğŸ¯ ì´ì œ ê°€ëŠ¥í•œ ê³ ê¸‰ ë¶„ì„:")
                print("   â€¢ ì‚¬ìš©ìë³„ ë ˆì‹œí”¼ ì„ í˜¸ë„ ë¶„ì„")
                print("   â€¢ ê°œì¸í™” ì¶”ì²œ ì—”ì§„ ë°ì´í„°")
                print("   â€¢ ì‚¬ìš©ì ì„¸ê·¸ë¨¼í…Œì´ì…˜")
                print("   â€¢ ë ˆì‹œí”¼ë³„ ì„±ê³¼ ë¶„ì„")
                print("   â€¢ A/B í…ŒìŠ¤íŠ¸ ì™„ì „ ë¶„ì„")
            else:
                print("\nâš ï¸ ë§¤í•‘ë¥ ì´ ë‚®ìŒ - ì¶”ê°€ ìµœì í™” í•„ìš”")
                
        except Exception as e:
            print(f"âŒ Step 2 ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            
    def populate_fact_table_complete_rebuild(self):
        """ì™„ì „í•œ Fact í…Œì´ë¸” ì¬êµ¬ì„± (ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì „ëµ)"""
        print("\nğŸ”„ ì™„ì „í•œ Fact í…Œì´ë¸” ì¬êµ¬ì„± ì‹œì‘...")
        
        # ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ì „ëµ: ì‹œê°„ ê¸°ë°˜ íŒŒí‹°ì…”ë‹ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
        rebuild_query = f"""
        WITH silver_optimized AS (
            SELECT 
                event_id,
                user_id,
                session_id,
                anonymous_id,
                event_name,
                page_name,
                prop_recipe_id,
                utc_timestamp,
                date,
                prop_action
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE date >= '2025-07-01' AND date <= '2025-07-03'  -- 3ì¼ì”© ë°°ì¹˜ ì²˜ë¦¬
        ),
        dimension_lookups AS (
            SELECT 
                s.*,
                -- Time dimension key
                CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                
                -- User dimension key
                u.user_dim_key,
                
                -- Recipe dimension key 
                COALESCE(r.recipe_dim_key, 0) as recipe_dim_key,
                
                -- Page dimension key
                COALESCE(p.page_dim_key, 0) as page_dim_key,
                
                -- Event dimension key
                COALESCE(e.event_dim_key, 1) as event_dim_key,
                
                -- Event properties for measures
                e.is_conversion_event,
                e.conversion_value as event_conversion_value
                
            FROM silver_optimized s
            
            -- Standard joins without hints (more stable)
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_users u 
                ON s.user_id = u.user_id AND u.is_current = TRUE
                
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r 
                ON s.prop_recipe_id = r.recipe_id
                
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p 
                ON s.page_name = p.page_name AND p.page_name != 'Unknown'
                
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_events e 
                ON s.event_name = e.event_name
        ),
        fact_complete AS (
            SELECT 
                event_id,
                COALESCE(user_dim_key, 0) as user_dim_key,
                time_dim_key,
                recipe_dim_key,
                page_dim_key,
                event_dim_key,
                
                -- ì •êµí•œ ì¸¡ì •ê°’ë“¤
                1 as event_count,
                
                -- Session Duration ì¶”ì¶œ (prop_actionì—ì„œ)
                CASE 
                    WHEN prop_action IS NOT NULL AND SIZE(SPLIT(prop_action, ':')) >= 2
                    THEN COALESCE(CAST(SPLIT(prop_action, ':')[1] AS BIGINT), 0)
                    ELSE 0
                END as session_duration_seconds,
                
                -- Page View Duration ì¶”ì¶œ
                CASE 
                    WHEN prop_action IS NOT NULL AND SIZE(SPLIT(prop_action, ':')) >= 3
                    THEN COALESCE(CAST(SPLIT(prop_action, ':')[2] AS BIGINT), 30)
                    ELSE 30
                END as page_view_duration_seconds,
                
                -- ì „í™˜ ì •ë³´
                COALESCE(is_conversion_event, FALSE) as is_conversion,
                COALESCE(event_conversion_value, 1.0) as conversion_value,
                
                -- ì •êµí•œ ì°¸ì—¬ë„ ê³„ì‚°
                CASE 
                    WHEN event_name = 'auth_success' THEN 10.0
                    WHEN event_name = 'create_comment' THEN 9.0
                    WHEN event_name = 'click_bookmark' THEN 8.0
                    WHEN event_name = 'click_recipe' THEN 7.0
                    WHEN event_name = 'search_recipe' THEN 5.0
                    WHEN event_name = 'view_recipe' THEN 4.0
                    WHEN event_name = 'view_page' THEN 2.0
                    ELSE 1.0
                END as engagement_score,
                
                -- Degenerate dimensions
                session_id,
                anonymous_id,
                
                -- ETL metadata
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM dimension_lookups
            WHERE event_id IS NOT NULL
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
        SELECT * FROM fact_complete
        """
        
        try:
            print("ğŸ”„ ì²« ë²ˆì§¸ ë°°ì¹˜ (3ì¼) ì²˜ë¦¬ ì¤‘...")
            self.spark.sql(rebuild_query)
            
            # ê²°ê³¼ í™•ì¸
            fact_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"âœ… ì²« ë²ˆì§¸ ë°°ì¹˜ ì™„ë£Œ: {fact_count:,}ê°œ ë ˆì½”ë“œ")
            
            # ì¶”ê°€ ë°°ì¹˜ ì²˜ë¦¬ (í•„ìš”ì‹œ)
            self.process_additional_batches()
            
        except Exception as e:
            print(f"âŒ ì™„ì „ ì¬êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
            print("ğŸ”„ ë” ì‘ì€ ë°°ì¹˜ë¡œ fallback...")
            self.populate_fact_table_hybrid_step1()  # fallback to step 1
            
    def process_additional_batches(self):
        """ì¶”ê°€ ë°°ì¹˜ë“¤ ìˆœì°¨ ì²˜ë¦¬"""
        print("\nğŸ”„ ì¶”ê°€ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
        
        date_ranges = [
            ("2025-07-04", "2025-07-06"),
            ("2025-07-07", "2025-07-09"),
            ("2025-07-10", "2025-07-12"),
            ("2025-07-13", "2025-07-15"),
        ]
        
        total_processed = 0
        
        for start_date, end_date in date_ranges:
            try:
                print(f"   ğŸ“… ë°°ì¹˜ ì²˜ë¦¬: {start_date} ~ {end_date}")
                
                batch_query = f"""
                INSERT INTO {self.catalog_name}.{self.gold_database}.fact_user_events
                SELECT 
                    s.event_id,
                    COALESCE(u.user_dim_key, 0) as user_dim_key,
                    CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                    COALESCE(r.recipe_dim_key, 0) as recipe_dim_key,
                    COALESCE(p.page_dim_key, 0) as page_dim_key,
                    COALESCE(e.event_dim_key, 1) as event_dim_key,
                    
                    1 as event_count,
                    0 as session_duration_seconds,
                    30 as page_view_duration_seconds,
                    
                    COALESCE(e.is_conversion_event, FALSE) as is_conversion,
                    COALESCE(e.conversion_value, 1.0) as conversion_value,
                    CASE 
                        WHEN s.event_name = 'auth_success' THEN 10.0
                        WHEN s.event_name = 'click_recipe' THEN 7.0
                        WHEN s.event_name = 'search_recipe' THEN 5.0
                        ELSE 2.0
                    END as engagement_score,
                    
                    s.session_id,
                    s.anonymous_id,
                    CURRENT_TIMESTAMP() as created_at,
                    CURRENT_TIMESTAMP() as updated_at
                    
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver s
                LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_users u ON s.user_id = u.user_id AND u.is_current = TRUE
                LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r ON s.prop_recipe_id = r.recipe_id
                LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p ON s.page_name = p.page_name
                LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_events e ON s.event_name = e.event_name
                
                WHERE s.date >= '{start_date}' AND s.date <= '{end_date}'
                AND s.event_id IS NOT NULL
                """
                
                self.spark.sql(batch_query)
                
                # ë°°ì¹˜ë³„ ê²°ê³¼ í™•ì¸
                batch_count = self.spark.sql(f"""
                SELECT COUNT(*) as cnt 
                FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
                JOIN {self.catalog_name}.{self.silver_database}.user_events_silver s ON f.event_id = s.event_id
                WHERE s.date >= '{start_date}' AND s.date <= '{end_date}'
                """).collect()[0]['cnt']
                
                total_processed += batch_count
                print(f"   âœ… ë°°ì¹˜ ì™„ë£Œ: {batch_count:,}ê°œ ë ˆì½”ë“œ ì¶”ê°€")
                
            except Exception as e:
                print(f"   âš ï¸ ë°°ì¹˜ {start_date}~{end_date} ì‹¤íŒ¨: {str(e)}")
                continue
        
        print(f"\nâœ… ì¶”ê°€ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ì´ {total_processed:,}ê°œ ë ˆì½”ë“œ ì¶”ê°€")
        
        # ìµœì¢… ê²€ì¦
        self.validate_complete_fact_table()
        
    def validate_complete_fact_table(self):
        """ì™„ì „í•œ Fact í…Œì´ë¸” ìµœì¢… ê²€ì¦"""
        print("\nğŸ† ì™„ì „í•œ Fact í…Œì´ë¸” ìµœì¢… ê²€ì¦...")
        
        try:
            comprehensive_stats = self.spark.sql(f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT user_dim_key) as unique_users,
                COUNT(DISTINCT recipe_dim_key) as unique_recipes,
                COUNT(DISTINCT page_dim_key) as unique_pages,
                COUNT(DISTINCT event_dim_key) as unique_events,
                COUNT(DISTINCT time_dim_key) as unique_time_keys,
                COUNT(DISTINCT session_id) as unique_sessions,
                
                -- ë§¤í•‘ ì„±ê³µë¥ 
                SUM(CASE WHEN user_dim_key > 0 THEN 1 ELSE 0 END) as mapped_users,
                SUM(CASE WHEN recipe_dim_key > 0 THEN 1 ELSE 0 END) as mapped_recipes,
                SUM(CASE WHEN page_dim_key > 0 THEN 1 ELSE 0 END) as mapped_pages,
                
                -- ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as total_conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement,
                ROUND(AVG(session_duration_seconds), 2) as avg_session_duration,
                SUM(conversion_value) as total_conversion_value
                
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            print("ğŸ¯ ì™„ì „í•œ Star Schema ì„±ê³¼:")
            print(f"   ğŸ“Š ì´ ì´ë²¤íŠ¸: {comprehensive_stats['total_records']:,}ê°œ")
            print(f"   ğŸ‘¥ ê³ ìœ  ì‚¬ìš©ì: {comprehensive_stats['unique_users']:,}ëª…")
            print(f"   ğŸ³ ê³ ìœ  ë ˆì‹œí”¼: {comprehensive_stats['unique_recipes']:,}ê°œ")
            print(f"   ğŸ“± ê³ ìœ  í˜ì´ì§€: {comprehensive_stats['unique_pages']}ê°œ")
            print(f"   ğŸ¬ ê³ ìœ  ì´ë²¤íŠ¸: {comprehensive_stats['unique_events']}ê°œ")
            print(f"   â° ê³ ìœ  ì‹œê°„í‚¤: {comprehensive_stats['unique_time_keys']:,}ê°œ")
            print(f"   ğŸ”— ê³ ìœ  ì„¸ì…˜: {comprehensive_stats['unique_sessions']:,}ê°œ")
            
            # ë§¤í•‘ ì„±ê³µë¥  ê³„ì‚°
            user_mapping_pct = (comprehensive_stats['mapped_users'] / comprehensive_stats['total_records']) * 100
            recipe_mapping_pct = (comprehensive_stats['mapped_recipes'] / comprehensive_stats['total_records']) * 100
            page_mapping_pct = (comprehensive_stats['mapped_pages'] / comprehensive_stats['total_records']) * 100
            
            print(f"\nğŸ“ˆ ì°¨ì› ë§¤í•‘ ì„±ê³µë¥ :")
            print(f"   ğŸ‘¥ ì‚¬ìš©ì: {user_mapping_pct:.1f}% ({comprehensive_stats['mapped_users']:,}ê°œ)")
            print(f"   ğŸ³ ë ˆì‹œí”¼: {recipe_mapping_pct:.1f}% ({comprehensive_stats['mapped_recipes']:,}ê°œ)")
            print(f"   ğŸ“± í˜ì´ì§€: {page_mapping_pct:.1f}% ({comprehensive_stats['mapped_pages']:,}ê°œ)")
            
            print(f"\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­:")
            print(f"   ğŸ¯ ì´ ì „í™˜: {comprehensive_stats['total_conversions']:,}ê±´")
            print(f"   â­ í‰ê·  ì°¸ì—¬ë„: {comprehensive_stats['avg_engagement']}")
            print(f"   â±ï¸ í‰ê·  ì„¸ì…˜ ì‹œê°„: {comprehensive_stats['avg_session_duration']}ì´ˆ")
            print(f"   ğŸ’° ì´ ì „í™˜ ê°€ì¹˜: ${comprehensive_stats['total_conversion_value']:,.2f}")
            
            # ë¶„ì„ ê°€ëŠ¥ ë²”ìœ„ í‰ê°€
            if user_mapping_pct >= 70 and recipe_mapping_pct >= 50:
                print(f"\nğŸ‰ ì™„ì „í•œ ì†”ë£¨ì…˜ ì„±ê³µ!")
                print(f"   âœ… 10ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ + A/B í…ŒìŠ¤íŠ¸ ì™„ì „ ë¶„ì„ ê°€ëŠ¥")
                print(f"   âœ… ì‚¬ìš©ìë³„ ê°œì¸í™” ë¶„ì„ ê°€ëŠ¥")
                print(f"   âœ… ë ˆì‹œí”¼ë³„ ì„±ê³¼ ë¶„ì„ ê°€ëŠ¥")
                print(f"   âœ… ê³ ê¸‰ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë¶„ì„ ê°€ëŠ¥")
                print(f"   âœ… ì‹¤ì‹œê°„ ì¶”ì²œ ì—”ì§„ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
                
                # ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¶„ì„ ì˜ˆì‹œ
                self.demonstrate_advanced_analytics()
                
            elif user_mapping_pct >= 40:
                print(f"\nâ­ ë¶€ë¶„ì  ì„±ê³µ!")
                print(f"   âœ… ê¸°ë³¸ ë©”íŠ¸ë¦­ ë¶„ì„ ê°€ëŠ¥")
                print(f"   âš ï¸ ê³ ê¸‰ ê°œì¸í™” ë¶„ì„ ì œí•œì ")
                print(f"   ğŸ’¡ ì¶”ê°€ ìµœì í™”ë¡œ ì™„ì „í•œ ì†”ë£¨ì…˜ ë‹¬ì„± ê°€ëŠ¥")
            else:
                print(f"\nâš ï¸ ì¶”ê°€ ìµœì í™” í•„ìš”")
                print(f"   ğŸ’¡ Step 1ìœ¼ë¡œ ê¸°ë³¸ ë¶„ì„ë¶€í„° ì‹œì‘ ê¶Œì¥")
                
        except Exception as e:
            print(f"âŒ ìµœì¢… ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            
    def demonstrate_advanced_analytics(self):
        """ê³ ê¸‰ ë¶„ì„ ì‹¤í–‰ ì˜ˆì‹œ ë°ëª¨"""
        print(f"\nğŸš€ ê³ ê¸‰ ë¶„ì„ ì‹¤í–‰ ê°€ëŠ¥ ë²”ìœ„ ë°ëª¨...")
        
        try:
            # 1. ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ë³„ ë ˆì‹œí”¼ ì„ í˜¸ë„
            print("   ğŸ“Š ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ë³„ ë ˆì‹œí”¼ ì„ í˜¸ë„:")
            segment_analysis = self.spark.sql(f"""
            SELECT 
                u.user_segment,
                COUNT(DISTINCT f.recipe_dim_key) as recipes_engaged,
                COUNT(*) as total_interactions,
                ROUND(AVG(f.engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
            JOIN {self.catalog_name}.{self.gold_database}.dim_users u ON f.user_dim_key = u.user_dim_key
            WHERE f.recipe_dim_key > 0 AND u.user_segment IS NOT NULL
            GROUP BY u.user_segment
            ORDER BY total_interactions DESC
            LIMIT 5
            """).collect()
            
            for row in segment_analysis:
                print(f"     {row['user_segment']}: {row['recipes_engaged']}ê°œ ë ˆì‹œí”¼, {row['avg_engagement']}ì  ì°¸ì—¬ë„")
            
            # 2. A/B í…ŒìŠ¤íŠ¸ ê·¸ë£¹ë³„ ì „í™˜ìœ¨
            print("   ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ê·¸ë£¹ë³„ ì „í™˜ìœ¨:")
            ab_test_analysis = self.spark.sql(f"""
            SELECT 
                u.ab_test_group,
                COUNT(DISTINCT f.user_dim_key) as users,
                SUM(CASE WHEN f.is_conversion THEN 1 ELSE 0 END) as conversions,
                ROUND(SUM(CASE WHEN f.is_conversion THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as conversion_rate
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
            JOIN {self.catalog_name}.{self.gold_database}.dim_users u ON f.user_dim_key = u.user_dim_key
            WHERE u.ab_test_group IS NOT NULL
            GROUP BY u.ab_test_group
            ORDER BY conversion_rate DESC
            """).collect()
            
            for row in ab_test_analysis:
                print(f"     {row['ab_test_group']}: {row['conversion_rate']}% ì „í™˜ìœ¨ ({row['conversions']}ê±´/{row['users']}ëª…)")
            
            # 3. ì¸ê¸° ë ˆì‹œí”¼ TOP 5
            print("   ğŸ³ ì¸ê¸° ë ˆì‹œí”¼ TOP 5:")
            popular_recipes = self.spark.sql(f"""
            SELECT 
                f.recipe_dim_key,
                COUNT(DISTINCT f.user_dim_key) as unique_viewers,
                COUNT(*) as total_views,
                ROUND(AVG(f.engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
            WHERE f.recipe_dim_key > 0
            GROUP BY f.recipe_dim_key
            ORDER BY unique_viewers DESC
            LIMIT 5
            """).collect()
            
            for row in popular_recipes:
                print(f"     ë ˆì‹œí”¼ #{row['recipe_dim_key']}: {row['unique_viewers']}ëª… ì¡°íšŒ, {row['avg_engagement']}ì  ì°¸ì—¬ë„")
                
            print(f"\nâœ… ì™„ì „í•œ ì†”ë£¨ì…˜ìœ¼ë¡œ ëª¨ë“  ê³ ê¸‰ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
            
        except Exception as e:
            print(f"   âš ï¸ ì¼ë¶€ ê³ ê¸‰ ë¶„ì„ ì œí•œ: {str(e)}")
            
    def execute_complete_solution(self):
        """ì™„ì „í•œ ì†”ë£¨ì…˜ ì „ì²´ ì‹¤í–‰"""
        print("ğŸš€ ì™„ì „í•œ ì†”ë£¨ì…˜ ì‹¤í–‰ ì‹œì‘...")
        print("=" * 60)
        
        try:
            # 1. SparkSession ìƒì„± (ë©”ëª¨ë¦¬ ìµœì í™”)
            self.create_spark_session()
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            self.create_gold_database()
            
            # 3. ìŠ¤í‚¤ë§ˆ ìƒì„±
            self.create_dimension_tables()
            self.create_fact_table()
            self.create_metrics_tables()
            
            # 4. Time Dimension ì±„ìš°ê¸°
            self.populate_time_dimension()
            
            # 5. Silverì—ì„œ Dimensions ì±„ìš°ê¸°
            self.populate_dimensions_from_silver()
            
            # 6. ì™„ì „í•œ Fact í…Œì´ë¸” êµ¬ì„±
            print("\nğŸ¯ ì™„ì „í•œ Fact í…Œì´ë¸” êµ¬ì„± ì‹œì‘...")
            self.populate_fact_table_complete_rebuild()
            
            # 7. ë©”íŠ¸ë¦­ ê³„ì‚°
            print("\nğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹œì‘...")
            self.calculate_all_metrics()
            
            print("\nğŸ‰ ì™„ì „í•œ ì†”ë£¨ì…˜ êµ¬ì¶• ì™„ë£Œ!")
            print("   âœ… ëª¨ë“  10ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ + A/B í…ŒìŠ¤íŠ¸ ë¶„ì„ ê°€ëŠ¥")
            print("   âœ… ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ê°€ëŠ¥")
            print("   âœ… ê³ ê¸‰ ê°œì¸í™” ì¶”ì²œ ì‹œìŠ¤í…œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì™„ì „í•œ ì†”ë£¨ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            print("ğŸ”„ ë‹¨ê³„ë³„ ì ‘ê·¼ìœ¼ë¡œ ì „í™˜...")
            self.execute_phased_approach()
            
    def execute_phased_approach(self):
        """ë‹¨ê³„ë³„ ì ‘ê·¼ ì‹¤í–‰ (fallback)"""
        print("ğŸ”„ ë‹¨ê³„ë³„ ì ‘ê·¼ ì‹¤í–‰...")
        
        try:
            # Phase 1: ê¸°ë³¸ êµ¬ì¡°
            self.populate_fact_table_hybrid_step1()
            
            # Phase 2: ê³ ê¸‰ ë§¤í•‘ (ê°€ëŠ¥í•œ ê²½ìš°)
            self.populate_fact_table_hybrid_step2()
            
        except Exception as e:
            print(f"ë‹¨ê³„ë³„ ì ‘ê·¼ë„ ì‹¤íŒ¨: {str(e)}")
            
    def calculate_all_metrics(self):
        """ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤í–‰"""
        print("ğŸ“Š ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        try:
            self.calculate_dau_metrics()
            print("âœ… ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        except Exception as e:
            print(f"âš ï¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì¼ë¶€ ì‹¤íŒ¨: {str(e)}")
        
        # ìµœì¢… ê²€ì¦
        self.validate_complete_fact_table()
        
    def populate_fact_table_proper(self):
        """Silver Layerë¡œë¶€í„° ì œëŒ€ë¡œ ëœ Fact í…Œì´ë¸” ë°ì´í„° ìƒì„± (ë‹¨ê³„ë³„ ì ‘ê·¼)"""
        print("\nğŸ“Š ì œëŒ€ë¡œ ëœ Fact í…Œì´ë¸” ë°ì´í„° ìƒì„± ì¤‘...")
        
        # Step 1: ê¸°ë³¸ ì´ë²¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (ë°°ì¹˜ ì²˜ë¦¬)
        print("ğŸ”„ Step 1: ê¸°ë³¸ ì´ë²¤íŠ¸ ë°ì´í„° ì¤€ë¹„...")
        
        # ë¨¼ì € ì„ì‹œ í…Œì´ë¸”ë¡œ Silver ë°ì´í„°ë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ê°€ì ¸ì˜´
        batch_query = f"""
        CREATE OR REPLACE TEMPORARY VIEW silver_batch AS
        SELECT 
            event_id,
            user_id,
            session_id,
            anonymous_id,
            event_name,
            page_name,
            prop_recipe_id,
            utc_timestamp,
            date,
            prop_action
        FROM {self.catalog_name}.{self.silver_database}.user_events_silver
        WHERE date >= '2025-07-01' AND date <= '2025-07-31'
        LIMIT 20000
        """
        
        self.spark.sql(batch_query)
        
        # Step 2: Dimension Key ë§¤í•‘ì„ ìœ„í•œ ë£©ì—… í…Œì´ë¸”ë“¤ ìƒì„±
        print("ğŸ”„ Step 2: Dimension Key ë£©ì—… í…Œì´ë¸” ìƒì„±...")
        
        # User dimension lookup
        self.spark.sql(f"""
        CREATE OR REPLACE TEMPORARY VIEW user_lookup AS
        SELECT user_id, user_dim_key, is_current
        FROM {self.catalog_name}.{self.gold_database}.dim_users 
        WHERE is_current = TRUE
        """)
        
        # Recipe dimension lookup  
        self.spark.sql(f"""
        CREATE OR REPLACE TEMPORARY VIEW recipe_lookup AS
        SELECT recipe_id, recipe_dim_key
        FROM {self.catalog_name}.{self.gold_database}.dim_recipes
        WHERE recipe_id IS NOT NULL
        """)
        
        # Page dimension lookup
        self.spark.sql(f"""
        CREATE OR REPLACE TEMPORARY VIEW page_lookup AS
        SELECT page_name, page_dim_key
        FROM {self.catalog_name}.{self.gold_database}.dim_pages
        WHERE page_name IS NOT NULL AND page_name != 'Unknown'
        """)
        
        # Event dimension lookup
        self.spark.sql(f"""
        CREATE OR REPLACE TEMPORARY VIEW event_lookup AS
        SELECT event_name, event_dim_key, conversion_value, is_conversion_event
        FROM {self.catalog_name}.{self.gold_database}.dim_events
        """)
        
        # Step 3: ì •êµí•œ Fact í…Œì´ë¸” ë°ì´í„° ìƒì„±
        print("ğŸ”„ Step 3: ì •êµí•œ Fact ë°ì´í„° ìƒì„±...")
        
        proper_fact_query = f"""
        WITH fact_enriched AS (
            SELECT 
                s.event_id,
                
                -- Proper Dimension Keys (ì‹¤ì œ ë§¤í•‘)
                COALESCE(u.user_dim_key, 0) as user_dim_key,
                CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                COALESCE(r.recipe_dim_key, 0) as recipe_dim_key,
                COALESCE(p.page_dim_key, 0) as page_dim_key,
                COALESCE(e.event_dim_key, 1) as event_dim_key,
                
                -- Proper Measures (ì‹¤ì œ ê³„ì‚°)
                1 as event_count,
                
                -- Session Duration (prop_actionì—ì„œ ì¶”ì¶œ)
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 2
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[1] AS BIGINT), 0)
                    ELSE 0
                END as session_duration_seconds,
                
                -- Page View Duration (prop_actionì—ì„œ ì¶”ì¶œ ë˜ëŠ” ê¸°ë³¸ê°’)
                CASE 
                    WHEN s.prop_action IS NOT NULL AND SIZE(SPLIT(s.prop_action, ':')) >= 3
                    THEN COALESCE(CAST(SPLIT(s.prop_action, ':')[2] AS BIGINT), 30)
                    ELSE 30
                END as page_view_duration_seconds,
                
                -- Conversion Flag (Event Dimensionì—ì„œ ê°€ì ¸ì˜´)
                COALESCE(e.is_conversion_event, FALSE) as is_conversion,
                
                -- Conversion Value (Event Dimensionì—ì„œ ê°€ì ¸ì˜´)
                COALESCE(e.conversion_value, 1.0) as conversion_value,
                
                -- Engagement Score (ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì°¨ë“± ì ìˆ˜)
                CASE 
                    WHEN s.event_name = 'auth_success' THEN 10.0
                    WHEN s.event_name = 'create_comment' THEN 9.0
                    WHEN s.event_name = 'click_bookmark' THEN 8.0
                    WHEN s.event_name = 'click_recipe' THEN 7.0
                    WHEN s.event_name = 'search_recipe' THEN 5.0
                    WHEN s.event_name = 'view_recipe' THEN 4.0
                    WHEN s.event_name = 'view_page' THEN 2.0
                    ELSE 1.0
                END as engagement_score,
                
                -- Degenerate Dimensions
                s.session_id,
                s.anonymous_id,
                
                -- ETL Metadata
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM silver_batch s
            
            -- Proper Dimension Joins
            LEFT JOIN user_lookup u ON s.user_id = u.user_id
            LEFT JOIN recipe_lookup r ON s.prop_recipe_id = r.recipe_id  
            LEFT JOIN page_lookup p ON s.page_name = p.page_name
            LEFT JOIN event_lookup e ON s.event_name = e.event_name
            
            WHERE s.event_id IS NOT NULL
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
        SELECT * FROM fact_enriched
        """
        
        try:
            self.spark.sql(proper_fact_query)
            
            # ê²°ê³¼ í™•ì¸ ë° í’ˆì§ˆ ê²€ì¦
            fact_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"âœ… ì œëŒ€ë¡œ ëœ Fact í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {fact_count:,}ê°œ ë ˆì½”ë“œ")
            
            # í’ˆì§ˆ ê²€ì¦
            self.validate_fact_table_quality()
            
        except Exception as e:
            print(f"âŒ ì •êµí•œ Fact í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            print("ğŸ”„ ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ fallback...")
            self.populate_fact_table_simple()
    
    def validate_fact_table_quality(self):
        """Fact í…Œì´ë¸” í’ˆì§ˆ ê²€ì¦"""
        print("\nğŸ” Fact í…Œì´ë¸” í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        # 1. Dimension Key ë¶„í¬ í™•ì¸
        key_distribution = self.spark.sql(f"""
        SELECT 
            'user_dim_key' as dimension,
            COUNT(DISTINCT user_dim_key) as unique_keys,
            COUNT(*) as total_records,
            ROUND(COUNT(DISTINCT user_dim_key) * 100.0 / COUNT(*), 2) as uniqueness_ratio
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        WHERE user_dim_key > 0
        
        UNION ALL
        
        SELECT 
            'recipe_dim_key' as dimension,
            COUNT(DISTINCT recipe_dim_key) as unique_keys,
            COUNT(*) as total_records,
            ROUND(COUNT(DISTINCT recipe_dim_key) * 100.0 / COUNT(*), 2) as uniqueness_ratio
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        WHERE recipe_dim_key > 0
        
        UNION ALL
        
        SELECT 
            'page_dim_key' as dimension,
            COUNT(DISTINCT page_dim_key) as unique_keys,
            COUNT(*) as total_records,
            ROUND(COUNT(DISTINCT page_dim_key) * 100.0 / COUNT(*), 2) as uniqueness_ratio
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        WHERE page_dim_key > 0
        
        UNION ALL
        
        SELECT 
            'event_dim_key' as dimension,
            COUNT(DISTINCT event_dim_key) as unique_keys,
            COUNT(*) as total_records,
            ROUND(COUNT(DISTINCT event_dim_key) * 100.0 / COUNT(*), 2) as uniqueness_ratio
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        WHERE event_dim_key > 0
        """).collect()
        
        print("ğŸ“Š Dimension Key ë¶„í¬:")
        for row in key_distribution:
            print(f"   {row['dimension']}: {row['unique_keys']}ê°œ ê³ ìœ ê°’ ({row['uniqueness_ratio']}% ë‹¤ì–‘ì„±)")
        
        # 2. ì¸¡ì •ê°’ í†µê³„
        measures_stats = self.spark.sql(f"""
        SELECT 
            COUNT(*) as total_events,
            COUNT(DISTINCT user_dim_key) as unique_users,
            COUNT(DISTINCT session_id) as unique_sessions,
            SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversion_events,
            ROUND(AVG(engagement_score), 2) as avg_engagement_score,
            ROUND(AVG(session_duration_seconds), 2) as avg_session_duration
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        """).collect()[0]
        
        print("ğŸ“ˆ ì¸¡ì •ê°’ í†µê³„:")
        print(f"   ì´ ì´ë²¤íŠ¸: {measures_stats['total_events']:,}ê°œ")
        print(f"   ê³ ìœ  ì‚¬ìš©ì: {measures_stats['unique_users']:,}ëª…")  
        print(f"   ê³ ìœ  ì„¸ì…˜: {measures_stats['unique_sessions']:,}ê°œ")
        print(f"   ì „í™˜ ì´ë²¤íŠ¸: {measures_stats['conversion_events']:,}ê°œ")
        print(f"   í‰ê·  ì°¸ì—¬ë„: {measures_stats['avg_engagement_score']}")
        print(f"   í‰ê·  ì„¸ì…˜ ì‹œê°„: {measures_stats['avg_session_duration']}ì´ˆ")
        
        print("âœ… Fact í…Œì´ë¸” í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ!")
        """Silver Layerë¡œë¶€í„° ê°„ë‹¨í•œ Fact í…Œì´ë¸” ë°ì´í„° ìƒì„± (ë©”ëª¨ë¦¬ ìµœì í™”)"""
        print("\nğŸ“Š Fact í…Œì´ë¸” ë°ì´í„° ìƒì„± ì¤‘ (ê°„ë‹¨í•œ ë°©ë²•)...")
        
        # ë‹¨ê³„ì ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬ë¡œ Fact í…Œì´ë¸” ìƒì„±
        simple_fact_query = f"""
        WITH silver_basic AS (
            SELECT 
                event_id,
                user_id,
                session_id,
                anonymous_id,
                event_name,
                page_name,
                prop_recipe_id,
                utc_timestamp,
                date
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            LIMIT 50000  -- ë°°ì¹˜ í¬ê¸° ì œí•œ
        ),
        fact_data AS (
            SELECT 
                s.event_id,
                COALESCE(u.user_dim_key, 0) as user_dim_key,
                CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                COALESCE(r.recipe_dim_key, 0) as recipe_dim_key,
                COALESCE(p.page_dim_key, 0) as page_dim_key,
                e.event_dim_key,
                
                -- Simple Measures
                1 as event_count,
                0 as session_duration_seconds,
                30 as page_view_duration_seconds,
                
                -- Conversion Info
                CASE WHEN s.event_name IN ('auth_success', 'create_comment', 'click_bookmark') THEN TRUE ELSE FALSE END as is_conversion,
                1.0 as conversion_value,
                1.0 as engagement_score,
                
                -- IDs
                s.session_id,
                s.anonymous_id,
                
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM silver_basic s
            
            -- Dimension joins (simplified)
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_users u 
                ON s.user_id = u.user_id AND u.is_current = TRUE
                
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r 
                ON s.prop_recipe_id = r.recipe_id
                
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p 
                ON s.page_name = p.page_name
                
            JOIN {self.catalog_name}.{self.gold_database}.dim_events e 
                ON s.event_name = e.event_name
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
        SELECT * FROM fact_data
        """
        
        try:
            self.spark.sql(simple_fact_query)
            
            # ê²°ê³¼ í™•ì¸
            fact_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"âœ… Fact í…Œì´ë¸” ë°ì´í„° ìƒì„± ì™„ë£Œ: {fact_count:,}ê°œ ë ˆì½”ë“œ")
            
        except Exception as e:
            print(f"âŒ Fact í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            print("ğŸ”„ ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„...")
            
            # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„
            mini_fact_query = f"""
            INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
            SELECT 
                s.event_id,
                0 as user_dim_key,  -- ë‹¨ìˆœí™”
                CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                0 as recipe_dim_key,  -- ë‹¨ìˆœí™”
                0 as page_dim_key,  -- ë‹¨ìˆœí™”
                1 as event_dim_key,  -- ê¸°ë³¸ê°’
                
                1 as event_count,
                0 as session_duration_seconds,
                30 as page_view_duration_seconds,
                
                FALSE as is_conversion,
                1.0 as conversion_value,
                1.0 as engagement_score,
                
                s.session_id,
                s.anonymous_id,
                
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver s
            LIMIT 10000
            """
            
            self.spark.sql(mini_fact_query)
            fact_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"âœ… Fact í…Œì´ë¸” ìƒì„± ì™„ë£Œ (ë‹¨ìˆœí™”): {fact_count:,}ê°œ ë ˆì½”ë“œ")
        
    def populate_fact_table(self):
        """Silver Layerë¡œë¶€í„° Fact í…Œì´ë¸” ë°ì´í„° ìƒì„±"""
        print("\nğŸ“Š Fact í…Œì´ë¸” ë°ì´í„° ìƒì„± ì¤‘...")
        
        fact_query = f"""
        WITH silver_with_keys AS (
            SELECT 
                s.event_id,
                s.user_id,
                s.session_id,
                s.anonymous_id,
                s.event_name,
                s.page_name,
                s.prop_recipe_id,
                s.utc_timestamp,
                s.date,
                -- Time dimension key ìƒì„±
                CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                
                -- User dimension key ì¡°íšŒ
                u.user_dim_key,
                
                -- Recipe dimension key ì¡°íšŒ  
                COALESCE(r.recipe_dim_key, 0) as recipe_dim_key,
                
                -- Page dimension key ì¡°íšŒ
                COALESCE(p.page_dim_key, 0) as page_dim_key,
                
                -- Event dimension key ì¡°íšŒ
                e.event_dim_key,
                
                -- ì¸¡ì •ê°’ë“¤
                1 as event_count,
                COALESCE(CAST(SPLIT(s.prop_action, ':')[1] AS BIGINT), 0) as session_duration_seconds,
                COALESCE(CAST(SPLIT(s.prop_action, ':')[2] AS BIGINT), 30) as page_view_duration_seconds,
                CASE WHEN s.event_name IN ('auth_success', 'create_comment', 'click_bookmark') THEN TRUE ELSE FALSE END as is_conversion,
                CASE 
                    WHEN s.event_name = 'auth_success' THEN 10.0
                    WHEN s.event_name = 'click_recipe' THEN 5.0
                    WHEN s.event_name = 'search_recipe' THEN 3.0
                    WHEN s.event_name = 'view_page' THEN 1.0
                    ELSE 1.0
                END as conversion_value,
                CASE 
                    WHEN s.event_name IN ('auth_success', 'create_comment') THEN 10.0
                    WHEN s.event_name IN ('click_bookmark', 'click_recipe') THEN 8.0
                    WHEN s.event_name = 'search_recipe' THEN 6.0
                    WHEN s.event_name = 'view_page' THEN 3.0
                    ELSE 1.0
                END as engagement_score
                
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver s
            
            -- User dimension join
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_users u 
                ON s.user_id = u.user_id AND u.is_current = TRUE
            
            -- Recipe dimension join  
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r 
                ON s.prop_recipe_id = r.recipe_id
            
            -- Page dimension join
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p 
                ON s.page_name = p.page_name
            
            -- Event dimension join
            JOIN {self.catalog_name}.{self.gold_database}.dim_events e 
                ON s.event_name = e.event_name
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
        SELECT 
            event_id,
            COALESCE(user_dim_key, 0) as user_dim_key,
            time_dim_key,
            recipe_dim_key,
            page_dim_key,
            event_dim_key,
            
            -- Additive Measures
            event_count,
            session_duration_seconds,
            page_view_duration_seconds,
            
            -- Semi-Additive Measures  
            is_conversion,
            conversion_value,
            engagement_score,
            
            -- Degenerate Dimensions
            session_id,
            anonymous_id,
            
            -- ETL Metadata
            CURRENT_TIMESTAMP() as created_at,
            CURRENT_TIMESTAMP() as updated_at
            
        FROM silver_with_keys
        WHERE time_dim_key IS NOT NULL
        """
        
        self.spark.sql(fact_query)
        
        # ê²°ê³¼ í™•ì¸
        fact_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
        print(f"âœ… Fact í…Œì´ë¸” ë°ì´í„° ìƒì„± ì™„ë£Œ: {fact_count:,}ê°œ ë ˆì½”ë“œ")
        
    def create_metrics_tables(self):
        """10ê°œ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ + A/B í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ í…Œì´ë¸”ë“¤ ìƒì„±"""
        print("\nğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„± ì¤‘...")
        
        # 1. EVENT_TYPE_TIME_DISTRIBUTION - ì´ë²¤íŠ¸ ìœ í˜•ë³„ ì‹œê°„ ë¶„í¬
        print("â° Event Type Time Distribution ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_event_type_time_distribution (
                event_time_month STRING NOT NULL,
                event_time_day_name STRING NOT NULL,
                event_time_hour INT NOT NULL,
                event_type STRING NOT NULL,
                event_count BIGINT NOT NULL,
                unique_users BIGINT,
                avg_session_duration DECIMAL(8,2),
                peak_indicator BOOLEAN,
                distribution_percentage DECIMAL(5,2),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (event_time_month)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 2. CONVERSION_RATE - ì „í™˜ìœ¨ ë¶„ì„
        print("ğŸ“ˆ Conversion Rate ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_conversion_rate (
                date DATE NOT NULL,
                funnel_stage STRING NOT NULL,
                total_users BIGINT NOT NULL,
                converted_users BIGINT NOT NULL,
                conversion_rate DECIMAL(5,2) NOT NULL,
                conversion_by_segment MAP<STRING, DECIMAL(5,2)>,
                benchmark_rate DECIMAL(5,2),
                improvement_target DECIMAL(5,2),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (date)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 3. WEEKLY_RETENTION - ì£¼ê°„ ë¦¬í…ì…˜
        print("ğŸ”„ Weekly Retention ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_weekly_retention (
                cohort_week DATE NOT NULL,
                retention_week INT NOT NULL,
                cohort_size BIGINT NOT NULL,
                retained_users BIGINT NOT NULL,
                retention_rate DECIMAL(5,2) NOT NULL,
                retention_by_segment MAP<STRING, DECIMAL(5,2)>,
                cumulative_retention_rate DECIMAL(5,2),
                churn_rate DECIMAL(5,2),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (cohort_week)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 4. MONTHLY_RETENTION - ì›”ê°„ ë¦¬í…ì…˜
        print("ğŸ“… Monthly Retention ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_monthly_retention (
                cohort_month DATE NOT NULL,
                retention_month INT NOT NULL,
                cohort_size BIGINT NOT NULL,
                retained_users BIGINT NOT NULL,
                retention_rate DECIMAL(5,2) NOT NULL,
                ltv_estimate DECIMAL(10,2),
                churn_probability DECIMAL(5,2),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (cohort_month)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 5. ACTIVE_USERS - DAU/WAU/MAU
        print("ğŸ‘¥ Active Users ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_active_users (
                date DATE NOT NULL,
                dau BIGINT NOT NULL,
                wau BIGINT,
                mau BIGINT,
                new_users BIGINT,
                returning_users BIGINT,
                dau_by_segment MAP<STRING, BIGINT>,
                wau_by_segment MAP<STRING, BIGINT>,
                mau_by_segment MAP<STRING, BIGINT>,
                dau_growth_rate DECIMAL(5,2),
                dau_7d_avg DECIMAL(10,2),
                dau_30d_avg DECIMAL(10,2),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (date)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 6. STICKINESS - ìœ ì € ê³ ì°©ì„±
        print("ğŸ¯ Stickiness ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_stickiness (
                date DATE NOT NULL,
                daily_active_users BIGINT NOT NULL,
                monthly_active_users BIGINT NOT NULL,
                stickiness_ratio DECIMAL(5,4) NOT NULL,
                avg_sessions_per_user DECIMAL(8,2),
                avg_events_per_user DECIMAL(8,2),
                power_user_ratio DECIMAL(5,2),
                engagement_depth_score DECIMAL(5,2),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (date)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 7. FUNNEL - í¼ë„ ë¶„ì„
        print("ğŸª Funnel ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_funnel (
                date DATE NOT NULL,
                funnel_stage STRING NOT NULL,
                stage_order INT NOT NULL,
                total_users BIGINT NOT NULL,
                conversion_from_previous DECIMAL(5,2),
                cumulative_conversion DECIMAL(5,2),
                drop_off_rate DECIMAL(5,2),
                avg_time_to_convert DECIMAL(10,2),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (date)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 8. COUNT_VISITORS - ë°©ë¬¸ì ìˆ˜
        print("ğŸš¶ Count Visitors ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_count_visitors (
                date DATE NOT NULL,
                total_visitors BIGINT NOT NULL,
                unique_visitors BIGINT NOT NULL,
                new_visitors BIGINT NOT NULL,
                returning_visitors BIGINT NOT NULL,
                avg_session_duration DECIMAL(8,2),
                bounce_rate DECIMAL(5,2),
                pages_per_session DECIMAL(5,2),
                traffic_source_breakdown MAP<STRING, BIGINT>,
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (date)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 9. REE_SEGMENTATION - Recipe RFM (Recency, Engagement, Expertise)
        print("ğŸ† REE Segmentation ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_ree_segmentation (
                user_id STRING NOT NULL,
                analysis_date DATE NOT NULL,
                recency_days INT NOT NULL,
                engagement_score DECIMAL(8,2) NOT NULL,
                expertise_level DECIMAL(5,2) NOT NULL,
                recency_score INT NOT NULL,
                engagement_score_quintile INT NOT NULL,
                expertise_score_quintile INT NOT NULL,
                ree_total_score INT NOT NULL,
                user_segment STRING NOT NULL,
                segment_description STRING,
                recommended_actions ARRAY<STRING>,
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (analysis_date)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 10. RECIPE_PERFORMANCE - ë ˆì‹œí”¼ ì„±ê³¼ ë¶„ì„
        print("ğŸ³ Recipe Performance ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_recipe_performance (
                date DATE NOT NULL,
                recipe_id BIGINT NOT NULL,
                total_views BIGINT,
                unique_viewers BIGINT,
                avg_view_duration DECIMAL(8,2),
                recipe_saves BIGINT,
                recipe_shares BIGINT,
                recipe_comments BIGINT,
                cooking_attempts BIGINT,
                success_rate DECIMAL(5,2),
                engagement_score DECIMAL(5,2),
                trending_score DECIMAL(5,2),
                recommendation_score DECIMAL(5,2),
                viral_coefficient DECIMAL(5,4),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (date)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 11. A/B TEST METRICS - A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        print("ğŸ§ª A/B Test Metrics í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_ab_test_results (
                test_id STRING NOT NULL,
                test_name STRING NOT NULL,
                variant_group STRING NOT NULL,
                metric_name STRING NOT NULL,
                analysis_date DATE NOT NULL,
                sample_size BIGINT NOT NULL,
                metric_value DECIMAL(15,6) NOT NULL,
                control_value DECIMAL(15,6),
                lift_percentage DECIMAL(8,4),
                confidence_interval_lower DECIMAL(15,6),
                confidence_interval_upper DECIMAL(15,6),
                p_value DECIMAL(10,8),
                statistical_significance BOOLEAN,
                confidence_level DECIMAL(5,2),
                test_status STRING,
                test_duration_days INT,
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (analysis_date, test_id)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        # 12. A/B TEST COHORT ANALYSIS - A/B í…ŒìŠ¤íŠ¸ ì½”í˜¸íŠ¸ ë¶„ì„
        print("ğŸ“Š A/B Test Cohort Analysis í…Œì´ë¸” ìƒì„±...")
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {self.catalog_name}.{self.gold_database}.metrics_ab_test_cohort (
                test_id STRING NOT NULL,
                variant_group STRING NOT NULL,
                cohort_week DATE NOT NULL,
                week_number INT NOT NULL,
                initial_users BIGINT NOT NULL,
                active_users BIGINT NOT NULL,
                retention_rate DECIMAL(5,2) NOT NULL,
                avg_engagement_score DECIMAL(8,2),
                conversion_rate DECIMAL(5,2),
                revenue_per_user DECIMAL(10,2),
                created_at TIMESTAMP NOT NULL
            ) USING ICEBERG
            PARTITIONED BY (test_id, cohort_week)
            TBLPROPERTIES (
                'format-version' = '2'
            )
        """)
        
        print("âœ… ëª¨ë“  ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„± ì™„ë£Œ!")
        
    def calculate_dau_metrics(self):
        """1. ACTIVE_USERS - DAU/WAU/MAU ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸ‘¥ Active Users ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        active_users_query = f"""
        WITH daily_users AS (
            SELECT 
                date,
                user_id,
                user_segment,
                page_name,
                MIN(date) OVER (PARTITION BY user_id) as first_seen_date
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
        ),
        dau_calculations AS (
            SELECT 
                date,
                COUNT(DISTINCT user_id) as dau,
                COUNT(DISTINCT CASE WHEN first_seen_date = date THEN user_id END) as new_users,
                COUNT(DISTINCT CASE WHEN first_seen_date < date THEN user_id END) as returning_users
            FROM daily_users
            GROUP BY date
        ),
        dau_by_segment AS (
            SELECT 
                date,
                user_segment,
                COUNT(DISTINCT user_id) as segment_dau
            FROM daily_users
            WHERE user_segment IS NOT NULL
            GROUP BY date, user_segment
        ),
        dau_segment_map AS (
            SELECT 
                date,
                MAP_FROM_ARRAYS(
                    COLLECT_LIST(user_segment),
                    COLLECT_LIST(segment_dau)
                ) as dau_by_segment
            FROM dau_by_segment
            GROUP BY date
        ),
        wau_calculations AS (
            SELECT 
                DATE_TRUNC('week', date) as week_start,
                user_segment,
                COUNT(DISTINCT user_id) as segment_wau
            FROM daily_users
            WHERE user_segment IS NOT NULL
            GROUP BY DATE_TRUNC('week', date), user_segment
        ),
        wau_totals AS (
            SELECT 
                week_start,
                SUM(segment_wau) as wau
            FROM wau_calculations
            GROUP BY week_start
        ),
        wau_segment_map AS (
            SELECT 
                week_start,
                MAP_FROM_ARRAYS(
                    COLLECT_LIST(user_segment),
                    COLLECT_LIST(segment_wau)
                ) as wau_by_segment
            FROM wau_calculations
            GROUP BY week_start
        ),
        mau_calculations AS (
            SELECT 
                DATE_TRUNC('month', date) as month_start,
                user_segment,
                COUNT(DISTINCT user_id) as segment_mau
            FROM daily_users
            WHERE user_segment IS NOT NULL
            GROUP BY DATE_TRUNC('month', date), user_segment
        ),
        mau_totals AS (
            SELECT 
                month_start,
                SUM(segment_mau) as mau
            FROM mau_calculations
            GROUP BY month_start
        ),
        mau_segment_map AS (
            SELECT 
                month_start,
                MAP_FROM_ARRAYS(
                    COLLECT_LIST(user_segment),
                    COLLECT_LIST(segment_mau)
                ) as mau_by_segment
            FROM mau_calculations
            GROUP BY month_start
        ),
        combined_metrics AS (
            SELECT 
                d.date,
                d.dau,
                w.wau,
                m.mau,
                d.new_users,
                d.returning_users,
                ds.dau_by_segment,
                ws.wau_by_segment,
                ms.mau_by_segment,
                LAG(d.dau, 1) OVER (ORDER BY d.date) as prev_dau,
                AVG(d.dau) OVER (ORDER BY d.date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as dau_7d_avg,
                AVG(d.dau) OVER (ORDER BY d.date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) as dau_30d_avg
            FROM dau_calculations d
            LEFT JOIN dau_segment_map ds ON d.date = ds.date
            LEFT JOIN wau_totals w ON d.date >= w.week_start AND d.date < w.week_start + INTERVAL 7 DAYS
            LEFT JOIN wau_segment_map ws ON d.date >= ws.week_start AND d.date < ws.week_start + INTERVAL 7 DAYS
            LEFT JOIN mau_totals m ON d.date >= m.month_start AND d.date < m.month_start + INTERVAL 1 MONTH
            LEFT JOIN mau_segment_map ms ON d.date >= ms.month_start AND d.date < ms.month_start + INTERVAL 1 MONTH
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_active_users
        SELECT 
            date,
            dau,
            wau,
            mau,
            new_users,
            returning_users,
            dau_by_segment,
            wau_by_segment,
            mau_by_segment,
            CASE WHEN prev_dau > 0 THEN ROUND(((dau - prev_dau) * 100.0 / prev_dau), 2) ELSE 0 END as dau_growth_rate,
            ROUND(dau_7d_avg, 2) as dau_7d_avg,
            ROUND(dau_30d_avg, 2) as dau_30d_avg,
            CURRENT_TIMESTAMP() as created_at
        FROM combined_metrics
        ORDER BY date
        """
        
        self.spark.sql(active_users_query)
        print("âœ… Active Users ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def calculate_event_time_distribution(self):
        """2. EVENT_TYPE_TIME_DISTRIBUTION ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nâ° Event Type Time Distribution ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        event_time_query = f"""
        WITH event_time_analysis AS (
            SELECT
                DATE_FORMAT(utc_timestamp, 'yyyy-MM') as event_time_month,
                DATE_FORMAT(utc_timestamp, 'EEEE') as event_time_day_name,
                HOUR(utc_timestamp) as event_time_hour,
                event_name as event_type,
                COUNT(*) as event_count,
                COUNT(DISTINCT user_id) as unique_users,
                AVG(COALESCE(CAST(SPLIT(prop_action, ':')[1] AS DOUBLE), 0)) as avg_session_duration
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            GROUP BY 
                DATE_FORMAT(utc_timestamp, 'yyyy-MM'),
                DATE_FORMAT(utc_timestamp, 'EEEE'),
                HOUR(utc_timestamp),
                event_name
        ),
        peak_analysis AS (
            SELECT 
                *,
                MAX(event_count) OVER (PARTITION BY event_time_month, event_type) as max_count_for_type,
                SUM(event_count) OVER (PARTITION BY event_time_month) as total_events_month
            FROM event_time_analysis
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_event_type_time_distribution
        SELECT
            event_time_month,
            event_time_day_name,
            event_time_hour,
            event_type,
            event_count,
            unique_users,
            ROUND(avg_session_duration, 2) as avg_session_duration,
            CASE WHEN event_count = max_count_for_type THEN TRUE ELSE FALSE END as peak_indicator,
            ROUND((event_count * 100.0 / total_events_month), 2) as distribution_percentage,
            CURRENT_TIMESTAMP() as created_at
        FROM peak_analysis
        ORDER BY event_time_month, event_type, event_time_hour
        """
        
        self.spark.sql(event_time_query)
        print("âœ… Event Type Time Distribution ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def calculate_conversion_metrics(self):
        """3. CONVERSION_RATE ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸ“ˆ Conversion Rate ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        conversion_query = f"""
        WITH user_funnel AS (
            SELECT 
                date,
                user_id,
                CASE 
                    WHEN event_name = 'view_page' THEN 'Awareness'
                    WHEN event_name LIKE '%search%' THEN 'Interest'
                    WHEN event_name = 'view_recipe' THEN 'Consideration'
                    WHEN event_name IN ('auth_success', 'create_comment', 'click_bookmark') THEN 'Conversion'
                    ELSE 'Other'
                END as funnel_stage,
                user_segment
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE event_name IN ('view_page', 'search_recipe', 'view_recipe', 'auth_success', 'create_comment', 'click_bookmark')
        ),
        funnel_analysis AS (
            SELECT 
                date,
                funnel_stage,
                user_segment,
                COUNT(DISTINCT user_id) as stage_users
            FROM user_funnel
            GROUP BY date, funnel_stage, user_segment
        ),
        funnel_totals AS (
            SELECT 
                date,
                funnel_stage,
                SUM(stage_users) as total_users
            FROM funnel_analysis
            GROUP BY date, funnel_stage
        ),
        conversion_totals AS (
            SELECT 
                date,
                SUM(CASE WHEN funnel_stage = 'Conversion' THEN total_users ELSE 0 END) as total_conversions
            FROM funnel_totals
            GROUP BY date
        ),
        segment_calculations AS (
            SELECT 
                fa.date,
                fa.funnel_stage,
                fa.user_segment,
                fa.stage_users,
                ft.total_users as funnel_total_users
            FROM funnel_analysis fa
            JOIN funnel_totals ft ON fa.date = ft.date AND fa.funnel_stage = ft.funnel_stage
        ),
        segment_percentages AS (
            SELECT 
                date,
                funnel_stage,
                MAP_FROM_ARRAYS(
                    COLLECT_LIST(user_segment),
                    COLLECT_LIST(ROUND((stage_users * 100.0 / funnel_total_users), 2))
                ) as conversion_by_segment
            FROM segment_calculations
            WHERE user_segment IS NOT NULL
            GROUP BY date, funnel_stage
        ),
        final_calculations AS (
            SELECT 
                ft.date,
                ft.funnel_stage,
                ft.total_users,
                ct.total_conversions,
                sp.conversion_by_segment
            FROM funnel_totals ft
            LEFT JOIN conversion_totals ct ON ft.date = ct.date
            LEFT JOIN segment_percentages sp ON ft.date = sp.date AND ft.funnel_stage = sp.funnel_stage
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_conversion_rate
        SELECT 
            date,
            funnel_stage,
            total_users,
            CASE WHEN funnel_stage = 'Conversion' THEN total_users ELSE 0 END as converted_users,
            CASE 
                WHEN funnel_stage = 'Conversion' AND total_conversions > 0 
                THEN ROUND((total_users * 100.0 / total_conversions), 2)
                WHEN funnel_stage != 'Conversion' AND total_users > 0
                THEN ROUND((CASE WHEN funnel_stage = 'Conversion' THEN total_users ELSE 0 END * 100.0 / total_users), 2)
                ELSE 0 
            END as conversion_rate,
            COALESCE(conversion_by_segment, MAP()) as conversion_by_segment,
            2.5 as benchmark_rate,  -- ì—…ê³„ í‰ê·  2.5%
            3.0 as improvement_target,  -- ëª©í‘œ 3%
            CURRENT_TIMESTAMP() as created_at
        FROM final_calculations
        ORDER BY date, funnel_stage
        """
        
        self.spark.sql(conversion_query)
        print("âœ… Conversion Rate ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def calculate_retention_metrics(self):
        """4. WEEKLY_RETENTION & 5. MONTHLY_RETENTION ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸ”„ Weekly & Monthly Retention ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        # Weekly Retention
        weekly_retention_query = f"""
        WITH user_cohorts AS (
            SELECT 
                user_id,
                DATE_TRUNC('week', MIN(date)) as cohort_week,
                user_segment
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            GROUP BY user_id, user_segment
        ),
        user_activity_weeks AS (
            SELECT 
                uc.user_id,
                uc.cohort_week,
                uc.user_segment,
                DATE_TRUNC('week', ues.date) as activity_week,
                DATEDIFF(DATE_TRUNC('week', ues.date), uc.cohort_week) / 7 as weeks_since_cohort
            FROM user_cohorts uc
            JOIN {self.catalog_name}.{self.silver_database}.user_events_silver ues ON uc.user_id = ues.user_id
        ),
        weekly_retention_analysis AS (
            SELECT 
                cohort_week,
                weeks_since_cohort as retention_week,
                user_segment,
                COUNT(DISTINCT user_id) as active_users
            FROM user_activity_weeks
            WHERE weeks_since_cohort >= 0 AND weeks_since_cohort <= 12
            GROUP BY cohort_week, weeks_since_cohort, user_segment
        ),
        weekly_cohort_sizes AS (
            SELECT 
                cohort_week,
                user_segment,
                COUNT(DISTINCT user_id) as cohort_size
            FROM user_cohorts
            GROUP BY cohort_week, user_segment
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_weekly_retention
        SELECT 
            ra.cohort_week,
            ra.retention_week,
            SUM(cs.cohort_size) as total_cohort_size,
            SUM(ra.active_users) as total_retained_users,
            ROUND((SUM(ra.active_users) * 100.0 / SUM(cs.cohort_size)), 2) as retention_rate,
            MAP_FROM_ARRAYS(
                COLLECT_LIST(ra.user_segment),
                COLLECT_LIST(ROUND((ra.active_users * 100.0 / cs.cohort_size), 2))
            ) as retention_by_segment,
            LAG(ROUND((SUM(ra.active_users) * 100.0 / SUM(cs.cohort_size)), 2)) OVER (
                PARTITION BY ra.cohort_week ORDER BY ra.retention_week
            ) as cumulative_retention_rate,
            ROUND((100.0 - (SUM(ra.active_users) * 100.0 / SUM(cs.cohort_size))), 2) as churn_rate,
            CURRENT_TIMESTAMP() as created_at
        FROM weekly_retention_analysis ra
        JOIN weekly_cohort_sizes cs ON ra.cohort_week = cs.cohort_week AND ra.user_segment = cs.user_segment
        WHERE ra.cohort_week >= CURRENT_DATE() - INTERVAL 90 DAYS
        GROUP BY ra.cohort_week, ra.retention_week
        ORDER BY ra.cohort_week, ra.retention_week
        """
        
        self.spark.sql(weekly_retention_query)
        
        # Monthly Retention
        monthly_retention_query = f"""
        WITH monthly_user_cohorts AS (
            SELECT 
                user_id,
                DATE_TRUNC('month', MIN(date)) as cohort_month,
                user_segment
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            GROUP BY user_id, user_segment
        ),
        monthly_user_activity AS (
            SELECT 
                uc.user_id,
                uc.cohort_month,
                uc.user_segment,
                DATE_TRUNC('month', ues.date) as activity_month,
                MONTHS_BETWEEN(DATE_TRUNC('month', ues.date), uc.cohort_month) as months_since_cohort
            FROM monthly_user_cohorts uc
            JOIN {self.catalog_name}.{self.silver_database}.user_events_silver ues ON uc.user_id = ues.user_id
        ),
        monthly_retention_analysis AS (
            SELECT 
                cohort_month,
                months_since_cohort as retention_month,
                COUNT(DISTINCT user_id) as active_users
            FROM monthly_user_activity
            WHERE months_since_cohort >= 0 AND months_since_cohort <= 12
            GROUP BY cohort_month, months_since_cohort
        ),
        monthly_cohort_sizes AS (
            SELECT 
                cohort_month,
                COUNT(DISTINCT user_id) as cohort_size
            FROM monthly_user_cohorts
            GROUP BY cohort_month
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_monthly_retention
        SELECT 
            ra.cohort_month,
            ra.retention_month,
            cs.cohort_size,
            ra.active_users as retained_users,
            ROUND((ra.active_users * 100.0 / cs.cohort_size), 2) as retention_rate,
            ra.active_users * 10.0 as ltv_estimate,  -- ê°„ë‹¨í•œ LTV ì¶”ì •
            ROUND((100.0 - (ra.active_users * 100.0 / cs.cohort_size)), 2) as churn_probability,
            CURRENT_TIMESTAMP() as created_at
        FROM monthly_retention_analysis ra
        JOIN monthly_cohort_sizes cs ON ra.cohort_month = cs.cohort_month
        WHERE ra.cohort_month >= CURRENT_DATE() - INTERVAL 12 MONTHS
        ORDER BY ra.cohort_month, ra.retention_month
        """
        
        self.spark.sql(monthly_retention_query)
        print("âœ… Weekly & Monthly Retention ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def calculate_stickiness_metrics(self):
        """6. STICKINESS ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸ¯ Stickiness ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        stickiness_query = f"""
        WITH daily_activity AS (
            SELECT 
                date,
                user_id,
                COUNT(*) as daily_events,
                COUNT(DISTINCT session_id) as daily_sessions
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            GROUP BY date, user_id
        ),
        monthly_activity AS (
            SELECT 
                DATE_TRUNC('month', date) as month,
                user_id,
                SUM(daily_events) as monthly_events,
                SUM(daily_sessions) as monthly_sessions,
                COUNT(DISTINCT date) as active_days_in_month
            FROM daily_activity
            GROUP BY DATE_TRUNC('month', date), user_id
        ),
        daily_aggregates AS (
            SELECT 
                date,
                COUNT(DISTINCT user_id) as daily_active_users,
                AVG(daily_sessions) as avg_sessions_per_user,
                AVG(daily_events) as avg_events_per_user
            FROM daily_activity
            GROUP BY date
        ),
        monthly_aggregates AS (
            SELECT 
                month,
                COUNT(DISTINCT user_id) as monthly_active_users,
                COUNT(DISTINCT CASE WHEN active_days_in_month >= 15 THEN user_id END) as power_users
            FROM monthly_activity
            GROUP BY month
        ),
        stickiness_calculation AS (
            SELECT 
                CAST(da.date AS DATE) as date,
                da.daily_active_users,
                ma.monthly_active_users,
                ROUND((da.daily_active_users * 1.0 / ma.monthly_active_users), 4) as stickiness_ratio,
                ROUND(da.avg_sessions_per_user, 2) as avg_sessions_per_user,
                ROUND(da.avg_events_per_user, 2) as avg_events_per_user,
                ROUND((ma.power_users * 100.0 / ma.monthly_active_users), 2) as power_user_ratio
            FROM daily_aggregates da
            JOIN monthly_aggregates ma ON DATE_TRUNC('month', da.date) = ma.month
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_stickiness
        SELECT 
            date,
            daily_active_users,
            monthly_active_users,
            stickiness_ratio,
            avg_sessions_per_user,
            avg_events_per_user,
            power_user_ratio,
            ROUND((stickiness_ratio * avg_sessions_per_user * power_user_ratio / 100), 2) as engagement_depth_score,
            CURRENT_TIMESTAMP() as created_at
        FROM stickiness_calculation
        ORDER BY date
        """
        
        self.spark.sql(stickiness_query)
        print("âœ… Stickiness ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def calculate_funnel_metrics(self):
        """7. FUNNEL ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸª Funnel ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        funnel_query = f"""
        WITH funnel_stages AS (
            SELECT 
                date,
                user_id,
                session_id,
                utc_timestamp,
                CASE 
                    WHEN event_name = 'view_page' AND page_name = 'home' THEN 'Landing'
                    WHEN event_name = 'search_recipe' THEN 'Search'
                    WHEN event_name = 'view_recipe' THEN 'Recipe_View'
                    WHEN event_name = 'click_bookmark' THEN 'Bookmark'
                    WHEN event_name = 'auth_success' THEN 'Signup'
                    WHEN event_name = 'create_comment' THEN 'Engagement'
                    ELSE NULL
                END as funnel_stage,
                ROW_NUMBER() OVER (PARTITION BY user_id, session_id ORDER BY utc_timestamp) as event_order
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE event_name IN ('view_page', 'search_recipe', 'view_recipe', 'click_bookmark', 'auth_success', 'create_comment')
        ),
        stage_order_mapping AS (
            SELECT 'Landing' as stage, 1 as stage_order
            UNION ALL SELECT 'Search', 2
            UNION ALL SELECT 'Recipe_View', 3
            UNION ALL SELECT 'Bookmark', 4
            UNION ALL SELECT 'Signup', 5
            UNION ALL SELECT 'Engagement', 6
        ),
        funnel_analysis AS (
            SELECT 
                date,
                funnel_stage,
                som.stage_order,
                COUNT(DISTINCT user_id) as stage_users,
                AVG(event_order) as avg_steps_to_reach
            FROM funnel_stages fs
            JOIN stage_order_mapping som ON fs.funnel_stage = som.stage
            WHERE funnel_stage IS NOT NULL
            GROUP BY date, funnel_stage, som.stage_order
        ),
        funnel_with_conversion AS (
            SELECT 
                date,
                funnel_stage,
                stage_order,
                stage_users,
                LAG(stage_users) OVER (PARTITION BY date ORDER BY stage_order) as previous_stage_users,
                FIRST_VALUE(stage_users) OVER (PARTITION BY date ORDER BY stage_order) as landing_users,
                avg_steps_to_reach
            FROM funnel_analysis
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_funnel
        SELECT 
            date,
            funnel_stage,
            stage_order,
            stage_users as total_users,
            CASE 
                WHEN previous_stage_users > 0 
                THEN ROUND((stage_users * 100.0 / previous_stage_users), 2)
                ELSE 100.0
            END as conversion_from_previous,
            CASE 
                WHEN landing_users > 0 
                THEN ROUND((stage_users * 100.0 / landing_users), 2)
                ELSE 0.0
            END as cumulative_conversion,
            CASE 
                WHEN previous_stage_users > 0 
                THEN ROUND(((previous_stage_users - stage_users) * 100.0 / previous_stage_users), 2)
                ELSE 0.0
            END as drop_off_rate,
            ROUND(avg_steps_to_reach, 2) as avg_time_to_convert,
            CURRENT_TIMESTAMP() as created_at
        FROM funnel_with_conversion
        ORDER BY date, stage_order
        """
        
        self.spark.sql(funnel_query)
        print("âœ… Funnel ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def calculate_visitor_metrics(self):
        """8. COUNT_VISITORS ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸš¶ Count Visitors ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        visitor_query = f"""
        WITH visitor_sessions AS (
            SELECT 
                date,
                user_id,
                session_id,
                anonymous_id,
                MIN(utc_timestamp) as session_start,
                MAX(utc_timestamp) as session_end,
                COUNT(*) as session_events,
                COUNT(DISTINCT page_name) as unique_pages_visited,
                MIN(date) OVER (PARTITION BY COALESCE(user_id, anonymous_id)) as first_visit_date
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            GROUP BY date, user_id, session_id, anonymous_id
        ),
        daily_visitor_metrics AS (
            SELECT 
                date,
                COUNT(DISTINCT session_id) as total_visitors,
                COUNT(DISTINCT COALESCE(user_id, anonymous_id)) as unique_visitors,
                COUNT(DISTINCT CASE WHEN first_visit_date = date THEN COALESCE(user_id, anonymous_id) END) as new_visitors,
                COUNT(DISTINCT CASE WHEN first_visit_date < date THEN COALESCE(user_id, anonymous_id) END) as returning_visitors,
                AVG(UNIX_TIMESTAMP(session_end) - UNIX_TIMESTAMP(session_start)) as avg_session_duration_seconds,
                COUNT(DISTINCT CASE WHEN session_events = 1 THEN session_id END) as bounce_sessions,
                AVG(unique_pages_visited) as avg_pages_per_session
            FROM visitor_sessions
            GROUP BY date
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_count_visitors
        SELECT 
            date,
            total_visitors,
            unique_visitors,
            new_visitors,
            returning_visitors,
            ROUND(avg_session_duration_seconds / 60.0, 2) as avg_session_duration,
            ROUND((bounce_sessions * 100.0 / total_visitors), 2) as bounce_rate,
            ROUND(avg_pages_per_session, 2) as pages_per_session,
            MAP('Direct', unique_visitors * 0.4, 'Search', unique_visitors * 0.3, 'Social', unique_visitors * 0.2, 'Other', unique_visitors * 0.1) as traffic_source_breakdown,
            CURRENT_TIMESTAMP() as created_at
        FROM daily_visitor_metrics
        ORDER BY date
        """
        
        self.spark.sql(visitor_query)
        print("âœ… Count Visitors ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def calculate_ree_segmentation(self):
        """9. REE_SEGMENTATION (Recipe RFM) ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸ† REE Segmentation ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        ree_query = f"""
        WITH session_durations AS (
            SELECT 
                user_id,
                session_id,
                (UNIX_TIMESTAMP(MAX(utc_timestamp)) - UNIX_TIMESTAMP(MIN(utc_timestamp))) / 60.0 as session_duration_minutes
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE user_id IS NOT NULL
            GROUP BY user_id, session_id
        ),
        user_activity_summary AS (
            SELECT 
                ues.user_id,
                MAX(ues.date) as last_activity_date,
                COUNT(*) as total_events,
                COUNT(DISTINCT ues.session_id) as total_sessions,
                COUNT(CASE WHEN ues.event_name = 'view_recipe' THEN 1 END) as recipe_views,
                COUNT(CASE WHEN ues.event_name = 'create_comment' THEN 1 END) as comments_created,
                COUNT(CASE WHEN ues.event_name = 'click_bookmark' THEN 1 END) as bookmarks_made,
                COUNT(DISTINCT ues.prop_recipe_id) as unique_recipes_viewed,
                AVG(COALESCE(sd.session_duration_minutes, 0)) as avg_session_duration
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver ues
            LEFT JOIN session_durations sd ON ues.user_id = sd.user_id AND ues.session_id = sd.session_id
            WHERE ues.user_id IS NOT NULL
            GROUP BY ues.user_id
        ),
        ree_calculation AS (
            SELECT 
                user_id,
                CURRENT_DATE() as analysis_date,
                DATEDIFF(CURRENT_DATE(), last_activity_date) as recency_days,
                ROUND((total_events + recipe_views * 2 + comments_created * 5 + bookmarks_made * 3 + total_sessions), 2) as engagement_score,
                ROUND((unique_recipes_viewed * 1.5 + comments_created * 2 + bookmarks_made * 1.2 + avg_session_duration / 60), 2) as expertise_level
            FROM user_activity_summary
        ),
        ree_scores AS (
            SELECT 
                *,
                NTILE(5) OVER (ORDER BY recency_days ASC) as recency_score,
                NTILE(5) OVER (ORDER BY engagement_score DESC) as engagement_score_quintile,
                NTILE(5) OVER (ORDER BY expertise_level DESC) as expertise_score_quintile
            FROM ree_calculation
        ),
        ree_segmentation AS (
            SELECT 
                *,
                (recency_score + engagement_score_quintile + expertise_score_quintile) as ree_total_score,
                CASE 
                    WHEN recency_score >= 4 AND engagement_score_quintile >= 4 AND expertise_score_quintile >= 4 THEN 'Recipe_Champions'
                    WHEN recency_score >= 3 AND engagement_score_quintile >= 4 THEN 'Loyal_Cooks'
                    WHEN recency_score >= 4 AND engagement_score_quintile <= 2 THEN 'New_Food_Lovers'
                    WHEN recency_score <= 2 AND engagement_score_quintile >= 3 THEN 'At_Risk_Users'
                    WHEN recency_score <= 2 AND engagement_score_quintile <= 2 THEN 'Lost_Users'
                    WHEN expertise_score_quintile >= 4 THEN 'Cooking_Experts'
                    WHEN engagement_score_quintile >= 3 THEN 'Active_Users'
                    ELSE 'Casual_Browsers'
                END as user_segment
            FROM ree_scores
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_ree_segmentation
        SELECT 
            user_id,
            analysis_date,
            recency_days,
            engagement_score,
            expertise_level,
            recency_score,
            engagement_score_quintile,
            expertise_score_quintile,
            ree_total_score,
            user_segment,
            CASE 
                WHEN user_segment = 'Recipe_Champions' THEN 'Top users with high engagement and expertise'
                WHEN user_segment = 'Loyal_Cooks' THEN 'Regular users with consistent activity'
                WHEN user_segment = 'New_Food_Lovers' THEN 'Recent users showing interest'
                WHEN user_segment = 'At_Risk_Users' THEN 'Previously active users needing re-engagement'
                WHEN user_segment = 'Lost_Users' THEN 'Inactive users requiring win-back campaigns'
                WHEN user_segment = 'Cooking_Experts' THEN 'Skilled users who could mentor others'
                WHEN user_segment = 'Active_Users' THEN 'Regular engaged users'
                ELSE 'Light users browsing occasionally'
            END as segment_description,
            CASE 
                WHEN user_segment = 'Recipe_Champions' THEN ARRAY('VIP content access', 'Beta feature testing', 'Community leadership')
                WHEN user_segment = 'Loyal_Cooks' THEN ARRAY('Premium recipes', 'Cooking challenges', 'Social features')
                WHEN user_segment = 'New_Food_Lovers' THEN ARRAY('Onboarding sequence', 'Beginner recipes', 'Tutorial content')
                WHEN user_segment = 'At_Risk_Users' THEN ARRAY('Re-engagement email', 'Special offers', 'New content notification')
                WHEN user_segment = 'Lost_Users' THEN ARRAY('Win-back campaign', 'Survey feedback', 'Incentive offers')
                WHEN user_segment = 'Cooking_Experts' THEN ARRAY('Advanced recipes', 'Recipe creation tools', 'Mentorship program')
                WHEN user_segment = 'Active_Users' THEN ARRAY('Personalized recommendations', 'Social sharing', 'Achievement badges')
                ELSE ARRAY('Content discovery', 'Interest-based recommendations', 'Simple engagement features')
            END as recommended_actions,
            CURRENT_TIMESTAMP() as created_at
        FROM ree_segmentation
        ORDER BY ree_total_score DESC
        """
        
        self.spark.sql(ree_query)
        print("âœ… REE Segmentation ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def calculate_recipe_performance_metrics(self):
        """10. RECIPE_PERFORMANCE ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸ³ Recipe Performance ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        recipe_performance_query = f"""
        WITH recipe_activity AS (
            SELECT 
                date,
                prop_recipe_id as recipe_id,
                user_id,
                session_id,
                event_name
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE prop_recipe_id IS NOT NULL AND prop_recipe_id > 0
        ),
        daily_recipe_metrics AS (
            SELECT 
                date,
                recipe_id,
                COUNT(*) as total_views,
                COUNT(DISTINCT user_id) as unique_viewers,
                COUNT(*) * 2.5 as avg_view_duration,  -- í‰ê·  2.5ë¶„ìœ¼ë¡œ ê°€ì •
                COUNT(CASE WHEN event_name = 'click_bookmark' THEN 1 END) as recipe_saves,
                COUNT(CASE WHEN event_name = 'share_recipe' THEN 1 END) as recipe_shares,
                COUNT(CASE WHEN event_name = 'create_comment' THEN 1 END) as recipe_comments,
                COUNT(CASE WHEN event_name = 'start_cooking' THEN 1 END) as cooking_attempts,
                COUNT(CASE WHEN event_name = 'complete_cooking' THEN 1 END) as cooking_completions
            FROM recipe_activity
            GROUP BY date, recipe_id
        ),
        recipe_performance_calculation AS (
            SELECT 
                *,
                CASE 
                    WHEN cooking_attempts > 0 
                    THEN ROUND((cooking_completions * 100.0 / cooking_attempts), 2)
                    ELSE 0
                END as success_rate,
                ROUND((total_views * 0.1 + unique_viewers * 0.3 + recipe_saves * 2 + recipe_comments * 3 + recipe_shares * 5), 2) as engagement_score,
                ROW_NUMBER() OVER (PARTITION BY date ORDER BY total_views DESC) as daily_view_rank,
                ROUND((recipe_shares * 1.0 / NULLIF(unique_viewers, 0)), 4) as viral_coefficient
            FROM daily_recipe_metrics
        ),
        trending_calculation AS (
            SELECT 
                *,
                CASE 
                    WHEN daily_view_rank <= 10 THEN 100
                    WHEN daily_view_rank <= 50 THEN 80
                    WHEN daily_view_rank <= 100 THEN 60
                    WHEN daily_view_rank <= 500 THEN 40
                    ELSE 20
                END as trending_score,
                ROUND((engagement_score * 0.4 + 
                       CASE WHEN daily_view_rank <= 100 THEN 100 - daily_view_rank ELSE 0 END * 0.3 + 
                       viral_coefficient * 1000 * 0.3), 2) as recommendation_score
            FROM recipe_performance_calculation
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_recipe_performance
        SELECT 
            date,
            recipe_id,
            total_views,
            unique_viewers,
            ROUND(avg_view_duration / 60.0, 2) as avg_view_duration,
            recipe_saves,
            recipe_shares,
            recipe_comments,
            cooking_attempts,
            success_rate,
            engagement_score,
            trending_score,
            recommendation_score,
            viral_coefficient,
            CURRENT_TIMESTAMP() as created_at
        FROM trending_calculation
        ORDER BY date, engagement_score DESC
        """
        
        self.spark.sql(recipe_performance_query)
        print("âœ… Recipe Performance ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def calculate_ab_test_metrics(self):
        """A/B í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸ§ª A/B Test ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        # A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        ab_test_query = f"""
        WITH ab_test_data AS (
            SELECT 
                COALESCE(ab_test_group, 'control') as variant_group,
                user_id,
                date,
                event_name,
                COUNT(*) as event_count,
                COUNT(DISTINCT session_id) as session_count,
                COUNT(CASE WHEN event_name IN ('auth_success', 'create_comment', 'click_bookmark') THEN 1 END) as conversion_events
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE ab_test_group IS NOT NULL
            GROUP BY COALESCE(ab_test_group, 'control'), user_id, date, event_name
        ),
        daily_ab_metrics AS (
            SELECT 
                'recipe_homepage_test' as test_id,
                'Recipe Homepage A/B Test' as test_name,
                variant_group,
                date as analysis_date,
                COUNT(DISTINCT user_id) as sample_size,
                SUM(event_count) as total_events,
                SUM(session_count) as total_sessions,
                SUM(conversion_events) as total_conversions,
                AVG(event_count) as avg_events_per_user,
                ROUND((SUM(conversion_events) * 100.0 / COUNT(DISTINCT user_id)), 4) as conversion_rate
            FROM ab_test_data
            GROUP BY variant_group, date
        ),
        control_baseline AS (
            SELECT 
                analysis_date,
                conversion_rate as control_conversion_rate,
                avg_events_per_user as control_avg_events
            FROM daily_ab_metrics
            WHERE variant_group = 'control'
        ),
        ab_test_results AS (
            SELECT 
                ab.test_id,
                ab.test_name,
                ab.variant_group,
                ab.analysis_date,
                ab.sample_size,
                ab.conversion_rate as metric_value,
                cb.control_conversion_rate as control_value,
                CASE 
                    WHEN cb.control_conversion_rate > 0 
                    THEN ROUND(((ab.conversion_rate - cb.control_conversion_rate) * 100.0 / cb.control_conversion_rate), 4)
                    ELSE 0
                END as lift_percentage,
                -- ê°„ë‹¨í•œ í†µê³„ì  ìœ ì˜ì„± ê³„ì‚° (ì •í™•í•œ ê³„ì‚°ì„ ìœ„í•´ì„œëŠ” ë” ë³µì¡í•œ í†µê³„ ë¶„ì„ í•„ìš”)
                CASE 
                    WHEN ab.sample_size >= 1000 AND ABS(ab.conversion_rate - cb.control_conversion_rate) > 0.5 
                    THEN TRUE 
                    ELSE FALSE 
                END as statistical_significance,
                DATEDIFF(ab.analysis_date, MIN(ab.analysis_date) OVER (PARTITION BY ab.test_id)) as test_duration_days
            FROM daily_ab_metrics ab
            LEFT JOIN control_baseline cb ON ab.analysis_date = cb.analysis_date
            WHERE ab.variant_group != 'control'
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_ab_test_results
        SELECT 
            test_id,
            test_name,
            variant_group,
            'conversion_rate' as metric_name,
            analysis_date,
            sample_size,
            metric_value,
            control_value,
            lift_percentage,
            metric_value - 1.96 * SQRT(metric_value * (100 - metric_value) / sample_size) as confidence_interval_lower,
            metric_value + 1.96 * SQRT(metric_value * (100 - metric_value) / sample_size) as confidence_interval_upper,
            CASE WHEN statistical_significance THEN 0.05 ELSE 0.2 END as p_value,
            statistical_significance,
            95.0 as confidence_level,
            CASE 
                WHEN test_duration_days >= 14 AND statistical_significance THEN 'Complete'
                WHEN test_duration_days >= 14 THEN 'Inconclusive'
                ELSE 'Running'
            END as test_status,
            test_duration_days,
            CURRENT_TIMESTAMP() as created_at
        FROM ab_test_results
        ORDER BY analysis_date, variant_group
        """
        
        self.spark.sql(ab_test_query)
        
        # A/B í…ŒìŠ¤íŠ¸ ì½”í˜¸íŠ¸ ë¶„ì„
        ab_cohort_query = f"""
        WITH ab_user_cohorts AS (
            SELECT 
                COALESCE(ab_test_group, 'control') as variant_group,
                user_id,
                DATE_TRUNC('week', MIN(date)) as cohort_week
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE ab_test_group IS NOT NULL
            GROUP BY COALESCE(ab_test_group, 'control'), user_id
        ),
        ab_weekly_activity AS (
            SELECT 
                uc.variant_group,
                uc.user_id,
                uc.cohort_week,
                DATE_TRUNC('week', ues.date) as activity_week,
                DATEDIFF(DATE_TRUNC('week', ues.date), uc.cohort_week) / 7 as week_number,
                COUNT(*) as weekly_events,
                COUNT(CASE WHEN ues.event_name IN ('auth_success', 'create_comment', 'click_bookmark') THEN 1 END) as weekly_conversions
            FROM ab_user_cohorts uc
            JOIN {self.catalog_name}.{self.silver_database}.user_events_silver ues 
                ON uc.user_id = ues.user_id 
                AND COALESCE(ues.ab_test_group, 'control') = uc.variant_group
            GROUP BY uc.variant_group, uc.user_id, uc.cohort_week, DATE_TRUNC('week', ues.date)
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_ab_test_cohort
        SELECT 
            'recipe_homepage_test' as test_id,
            weekly_agg.variant_group,
            weekly_agg.cohort_week,
            weekly_agg.week_number,
            cohort_sizes.initial_users,
            weekly_agg.active_users,
            ROUND((weekly_agg.active_users * 100.0 / cohort_sizes.initial_users), 2) as retention_rate,
            weekly_agg.avg_engagement_score,
            weekly_agg.conversion_rate,
            weekly_agg.revenue_per_user,
            CURRENT_TIMESTAMP() as created_at
        FROM (
            SELECT 
                variant_group,
                cohort_week,
                week_number,
                COUNT(DISTINCT user_id) as active_users,
                AVG(weekly_events) as avg_engagement_score,
                ROUND((SUM(weekly_conversions) * 100.0 / COUNT(DISTINCT user_id)), 2) as conversion_rate,
                SUM(weekly_conversions) * 1.5 as revenue_per_user
            FROM ab_weekly_activity
            WHERE week_number >= 0 AND week_number <= 8
            GROUP BY variant_group, cohort_week, week_number
        ) weekly_agg
        JOIN (
            SELECT 
                variant_group,
                cohort_week,
                COUNT(DISTINCT user_id) as initial_users
            FROM ab_weekly_activity
            WHERE week_number = 0
            GROUP BY variant_group, cohort_week
        ) cohort_sizes ON weekly_agg.variant_group = cohort_sizes.variant_group 
                      AND weekly_agg.cohort_week = cohort_sizes.cohort_week
        ORDER BY weekly_agg.variant_group, weekly_agg.cohort_week, weekly_agg.week_number
        """
        
        self.spark.sql(ab_cohort_query)
        print("âœ… A/B Test ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ!")
        
    def run_full_pipeline(self):
        """ì „ì²´ Gold Layer Star Schema + 10ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ Gold Layer Star Schema + ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
        print("=" * 70)
        
        try:
            # 1. SparkSession ìƒì„±
            self.create_spark_session()
            
            # 2. Gold Analytics ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            self.create_gold_database()
            
            # 3. Dimension & Fact í…Œì´ë¸” ìƒì„±
            self.create_dimension_tables()
            self.create_fact_table()
            
            # 4. Time Dimension ë°ì´í„° ìƒì„±
            self.populate_time_dimension()
            
            # 5. Silverì—ì„œ Dimension ë°ì´í„° ì¶”ì¶œ
            self.populate_dimensions_from_silver()
            
            # 6. Fact í…Œì´ë¸” ë°ì´í„° ìƒì„± (ì •êµí•œ ë°©ë²•)
            self.populate_fact_table_proper()
            
            # 7. 12ê°œ ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„± (10ê°œ í•µì‹¬ + 2ê°œ A/B í…ŒìŠ¤íŠ¸)
            self.create_metrics_tables()
            
            # 8. 10ê°œ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê³„ì‚°
            print("\nğŸ¯ 10ê°œ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹œì‘...")
            self.calculate_dau_metrics()                    # 1. ACTIVE_USERS (DAU/WAU/MAU)
            self.calculate_event_time_distribution()        # 2. EVENT_TYPE_TIME_DISTRIBUTION
            self.calculate_conversion_metrics()             # 3. CONVERSION_RATE
            self.calculate_retention_metrics()              # 4. WEEKLY_RETENTION & 5. MONTHLY_RETENTION
            self.calculate_stickiness_metrics()             # 6. STICKINESS
            self.calculate_funnel_metrics()                 # 7. FUNNEL
            self.calculate_visitor_metrics()                # 8. COUNT_VISITORS
            self.calculate_ree_segmentation()               # 9. REE_SEGMENTATION (Recipe RFM)
            self.calculate_recipe_performance_metrics()     # 10. RECIPE_PERFORMANCE
            
            # 7. A/B í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚°
            print("\nğŸ§ª A/B í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹œì‘...")
            self.calculate_ab_test_metrics()                # 11. A/B Test Results + Cohort
            
            # 8. ê²°ê³¼ ìš”ì•½ ë° ê²€ì¦
            print("\n" + "=" * 70)
            print("ğŸ‰ Gold Layer Star Schema + ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ êµ¬ì¶• ì™„ë£Œ!")
            
            # ìƒì„±ëœ í…Œì´ë¸” ëª©ë¡ í™•ì¸
            print("\nğŸ“Š ìƒì„±ëœ Gold Layer í…Œì´ë¸”:")
            tables_df = self.spark.sql(f"SHOW TABLES IN {self.catalog_name}.{self.gold_database}")
            
            # í•„í„°ë§ì„ ìœ„í•œ ì˜¬ë°”ë¥¸ ë¬¸ë²• ì‚¬ìš©
            gold_tables = tables_df.filter(
                (tables_df.tableName.contains("dim_")) |
                (tables_df.tableName.contains("fact_")) |
                (tables_df.tableName.contains("metrics_"))
            )
            gold_tables.show(50, truncate=False)
            
            # ê° í…Œì´ë¸” í–‰ ìˆ˜ í™•ì¸
            print("\nğŸ“ˆ í…Œì´ë¸”ë³„ ë°ì´í„° í˜„í™©:")
            table_summary = []
            
            for table_row in gold_tables.collect():
                table_name = table_row['tableName']
                try:
                    count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.{table_name}").collect()[0]['cnt']
                    table_summary.append((table_name, count))
                    print(f"  ğŸ“‹ {table_name}: {count:,}ê°œ í–‰")
                except Exception as e:
                    print(f"  âŒ {table_name}: ì˜¤ë¥˜ - {str(e)}")
            
            # ë©”íŠ¸ë¦­ ìƒ˜í”Œ í™•ì¸
            print("\nğŸ” ì£¼ìš” ë©”íŠ¸ë¦­ ìƒ˜í”Œ í™•ì¸:")
            
            # 1. DAU ë©”íŠ¸ë¦­ ìƒ˜í”Œ
            try:
                dau_sample = self.spark.sql(f"""
                    SELECT date, dau, new_users, returning_users, dau_growth_rate 
                    FROM {self.catalog_name}.{self.gold_database}.metrics_active_users 
                    ORDER BY date DESC LIMIT 3
                """)
                print("\nğŸ‘¥ Active Users (ìµœê·¼ 3ì¼):")
                dau_sample.show(truncate=False)
            except:
                print("ğŸ‘¥ Active Users: ë°ì´í„° ì—†ìŒ")
            
            # 2. Retention ë©”íŠ¸ë¦­ ìƒ˜í”Œ
            try:
                retention_sample = self.spark.sql(f"""
                    SELECT cohort_week, retention_week, cohort_size, retained_users, retention_rate 
                    FROM {self.catalog_name}.{self.gold_database}.metrics_weekly_retention 
                    WHERE retention_week <= 4
                    ORDER BY cohort_week DESC, retention_week 
                    LIMIT 5
                """)
                print("\nğŸ”„ Weekly Retention (ìµœê·¼ ì½”í˜¸íŠ¸, 4ì£¼ì°¨ê¹Œì§€):")
                retention_sample.show(truncate=False)
            except:
                print("ğŸ”„ Weekly Retention: ë°ì´í„° ì—†ìŒ")
            
            # 3. REE Segmentation ìƒ˜í”Œ
            try:
                ree_sample = self.spark.sql(f"""
                    SELECT user_segment, COUNT(*) as user_count, 
                           AVG(engagement_score) as avg_engagement,
                           AVG(expertise_level) as avg_expertise
                    FROM {self.catalog_name}.{self.gold_database}.metrics_ree_segmentation 
                    GROUP BY user_segment
                    ORDER BY user_count DESC
                """)
                print("\nğŸ† REE Segmentation (ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ë³„):")
                ree_sample.show(truncate=False)
            except:
                print("ğŸ† REE Segmentation: ë°ì´í„° ì—†ìŒ")
            
            # 4. A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒ˜í”Œ
            try:
                ab_sample = self.spark.sql(f"""
                    SELECT variant_group, metric_name, 
                           AVG(metric_value) as avg_metric_value,
                           AVG(lift_percentage) as avg_lift,
                           COUNT(CASE WHEN statistical_significance THEN 1 END) as significant_days
                    FROM {self.catalog_name}.{self.gold_database}.metrics_ab_test_results 
                    GROUP BY variant_group, metric_name
                    ORDER BY variant_group
                """)
                print("\nğŸ§ª A/B Test Results (ë³€í˜•ë³„ ìš”ì•½):")
                ab_sample.show(truncate=False)
            except:
                print("ğŸ§ª A/B Test Results: ë°ì´í„° ì—†ìŒ")
            
            # 5. ì´í•© í†µê³„
            print("\nğŸ“Š ì „ì²´ ë°ì´í„° í†µê³„:")
            # Python ë‚´ì¥ sum í•¨ìˆ˜ ì‚¬ìš©í•˜ì—¬ PySpark ì¶©ëŒ ë°©ì§€
            import builtins
            total_events = builtins.sum([count for name, count in table_summary if 'metrics_' in name and isinstance(count, int)])
            print(f"  ğŸ¯ ì´ ë©”íŠ¸ë¦­ ë ˆì½”ë“œ: {total_events:,}ê°œ")
            print(f"  ğŸ“… ìƒì„±ëœ í…Œì´ë¸”: {len(table_summary)}ê°œ")
            print(f"  ğŸ’¾ Gold Layer ë°ì´í„°ë² ì´ìŠ¤: {self.catalog_name}.{self.gold_database}")
            
            return True
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            if self.spark:
                self.spark.stop()
                print("\nğŸ”š SparkSession ì¢…ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pipeline = GoldLayerStarSchema()
    success = pipeline.run_full_pipeline()
    
    if success:
        print("\nğŸŠ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
        print("ğŸ”— ë‹¤ìŒ ë‹¨ê³„ ê°€ì´ë“œ:")
        print("  1. Docker í™˜ê²½ì—ì„œ ì‹¤í–‰: python gold_layer_star_schema.py")
        print("  2. BI ë„êµ¬ ì—°ê²°: Hive Metastore (thrift://localhost:9083)")
        print("  3. ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•")
        print("  4. A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ëª¨ë‹ˆí„°ë§")
    else:
        print("\nğŸ’¥ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

def analyze_ab_test_results():
    """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì‹¬í™” ë¶„ì„ ìœ í‹¸ë¦¬í‹°"""
    print("ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì‹¬í™” ë¶„ì„")
    print("=" * 50)
    
    spark = SparkSession.builder \
        .appName("AB_Test_Analysis") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
        .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
        .getOrCreate()
    
    try:
        # A/B í…ŒìŠ¤íŠ¸ ìš”ì•½ ë¶„ì„
        summary_query = """
        SELECT 
            test_id,
            variant_group,
            COUNT(*) as analysis_days,
            AVG(sample_size) as avg_daily_sample_size,
            AVG(metric_value) as avg_conversion_rate,
            AVG(lift_percentage) as avg_lift,
            COUNT(CASE WHEN statistical_significance THEN 1 END) as significant_days,
            MAX(test_duration_days) as total_test_days,
            FIRST(test_status) as current_status
        FROM iceberg_catalog.recipe_analytics.metrics_ab_test_results
        GROUP BY test_id, variant_group
        ORDER BY test_id, variant_group
        """
        
        print("ğŸ“Š A/B í…ŒìŠ¤íŠ¸ ìš”ì•½:")
        spark.sql(summary_query).show(truncate=False)
        
        # í†µê³„ì  ìœ ì˜ì„± ë¶„ì„
        significance_query = """
        SELECT 
            test_id,
            variant_group,
            CASE 
                WHEN AVG(CASE WHEN statistical_significance THEN 1.0 ELSE 0.0 END) >= 0.8 
                THEN 'Highly Significant'
                WHEN AVG(CASE WHEN statistical_significance THEN 1.0 ELSE 0.0 END) >= 0.5 
                THEN 'Moderately Significant'
                ELSE 'Not Significant'
            END as significance_level,
            AVG(lift_percentage) as avg_lift,
            MIN(confidence_interval_lower) as min_ci_lower,
            MAX(confidence_interval_upper) as max_ci_upper
        FROM iceberg_catalog.recipe_analytics.metrics_ab_test_results
        GROUP BY test_id, variant_group
        ORDER BY avg_lift DESC
        """
        
        print("\nğŸ¯ í†µê³„ì  ìœ ì˜ì„± ë¶„ì„:")
        spark.sql(significance_query).show(truncate=False)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendation_query = """
        WITH test_performance AS (
            SELECT 
                test_id,
                variant_group,
                AVG(lift_percentage) as avg_lift,
                AVG(CASE WHEN statistical_significance THEN 1.0 ELSE 0.0 END) as significance_ratio,
                MAX(test_duration_days) as test_days
            FROM iceberg_catalog.recipe_analytics.metrics_ab_test_results
            GROUP BY test_id, variant_group
        )
        SELECT 
            test_id,
            variant_group,
            avg_lift,
            significance_ratio,
            test_days,
            CASE 
                WHEN avg_lift > 5 AND significance_ratio >= 0.8 THEN 'IMPLEMENT - Strong Winner'
                WHEN avg_lift > 2 AND significance_ratio >= 0.6 THEN 'IMPLEMENT - Moderate Winner'
                WHEN avg_lift > 0 AND significance_ratio >= 0.5 THEN 'MONITOR - Promising'
                WHEN avg_lift <= 0 AND significance_ratio >= 0.5 THEN 'REJECT - Clear Loser'
                WHEN test_days < 14 THEN 'CONTINUE - More Data Needed'
                ELSE 'INCONCLUSIVE - Consider Redesign'
            END as recommendation
        FROM test_performance
        WHERE variant_group != 'control'
        ORDER BY avg_lift DESC
        """
        
        print("\nğŸ’¡ A/B í…ŒìŠ¤íŠ¸ ê¶Œì¥ì‚¬í•­:")
        spark.sql(recommendation_query).show(truncate=False)
        
    finally:
        spark.stop()

def generate_business_dashboard_queries():
    """ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œìš© SQL ì¿¼ë¦¬ ìƒì„±"""
    queries = {
        "daily_active_users": """
        -- DAU íŠ¸ë Œë“œ (ìµœê·¼ 30ì¼)
        SELECT 
            date,
            dau,
            new_users,
            returning_users,
            dau_growth_rate,
            dau_7d_avg
        FROM iceberg_catalog.recipe_analytics.metrics_active_users
        WHERE date >= CURRENT_DATE() - INTERVAL 30 DAYS
        ORDER BY date DESC
        """,
        
        "retention_cohort_heatmap": """
        -- ë¦¬í…ì…˜ ì½”í˜¸íŠ¸ íˆíŠ¸ë§µ ë°ì´í„°
        SELECT 
            cohort_week,
            retention_week,
            retention_rate
        FROM iceberg_catalog.recipe_analytics.metrics_weekly_retention
        WHERE cohort_week >= CURRENT_DATE() - INTERVAL 12 WEEKS
          AND retention_week <= 8
        ORDER BY cohort_week, retention_week
        """,
        
        "conversion_funnel": """
        -- ì „í™˜ í¼ë„ (ìµœê·¼ 7ì¼ í‰ê· )
        SELECT 
            funnel_stage,
            stage_order,
            AVG(total_users) as avg_users,
            AVG(conversion_from_previous) as avg_conversion_rate,
            AVG(drop_off_rate) as avg_drop_off_rate
        FROM iceberg_catalog.recipe_analytics.metrics_funnel
        WHERE date >= CURRENT_DATE() - INTERVAL 7 DAYS
        GROUP BY funnel_stage, stage_order
        ORDER BY stage_order
        """,
        
        "recipe_performance_top10": """
        -- ìƒìœ„ 10ê°œ ë ˆì‹œí”¼ ì„±ê³¼ (ìµœê·¼ 7ì¼)
        SELECT 
            r.recipe_id,
            AVG(r.total_views) as avg_daily_views,
            AVG(r.unique_viewers) as avg_daily_unique_viewers,
            AVG(r.engagement_score) as avg_engagement_score,
            AVG(r.viral_coefficient) as avg_viral_coefficient,
            MAX(r.trending_score) as max_trending_score
        FROM iceberg_catalog.recipe_analytics.metrics_recipe_performance r
        WHERE r.date >= CURRENT_DATE() - INTERVAL 7 DAYS
        GROUP BY r.recipe_id
        ORDER BY avg_engagement_score DESC
        LIMIT 10
        """,
        
        "user_segmentation_distribution": """
        -- REE ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬
        SELECT 
            user_segment,
            COUNT(*) as user_count,
            ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage,
            AVG(engagement_score) as avg_engagement,
            AVG(expertise_level) as avg_expertise,
            AVG(recency_days) as avg_recency
        FROM iceberg_catalog.recipe_analytics.metrics_ree_segmentation
        WHERE analysis_date = CURRENT_DATE()
        GROUP BY user_segment
        ORDER BY user_count DESC
        """,
        
        "ab_test_dashboard": """
        -- A/B í…ŒìŠ¤íŠ¸ ëŒ€ì‹œë³´ë“œ (ìµœê·¼ ê²°ê³¼)
        SELECT 
            test_name,
            variant_group,
            sample_size,
            metric_value as conversion_rate,
            lift_percentage,
            statistical_significance,
            test_status,
            confidence_interval_lower,
            confidence_interval_upper
        FROM iceberg_catalog.recipe_analytics.metrics_ab_test_results
        WHERE analysis_date = (SELECT MAX(analysis_date) FROM iceberg_catalog.recipe_analytics.metrics_ab_test_results)
        ORDER BY test_name, variant_group
        """,
        
        "business_kpi_summary": """
        -- í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ KPI ìš”ì•½ (ìµœê·¼ 7ì¼)
        WITH recent_metrics AS (
            SELECT 
                'DAU' as metric_name,
                AVG(dau) as current_value,
                AVG(dau_growth_rate) as growth_rate,
                'users' as unit
            FROM iceberg_catalog.recipe_analytics.metrics_active_users
            WHERE date >= CURRENT_DATE() - INTERVAL 7 DAYS
            
            UNION ALL
            
            SELECT 
                'Stickiness' as metric_name,
                AVG(stickiness_ratio) * 100 as current_value,
                0 as growth_rate,
                'percentage' as unit
            FROM iceberg_catalog.recipe_analytics.metrics_stickiness
            WHERE date >= CURRENT_DATE() - INTERVAL 7 DAYS
            
            UNION ALL
            
            SELECT 
                'Week 1 Retention' as metric_name,
                AVG(retention_rate) as current_value,
                0 as growth_rate,
                'percentage' as unit
            FROM iceberg_catalog.recipe_analytics.metrics_weekly_retention
            WHERE retention_week = 1 
              AND cohort_week >= CURRENT_DATE() - INTERVAL 4 WEEKS
        )
        SELECT 
            metric_name,
            ROUND(current_value, 2) as current_value,
            ROUND(growth_rate, 2) as growth_rate,
            unit
        FROM recent_metrics
        ORDER BY metric_name
        """
    }
    
    print("ğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ SQL ì¿¼ë¦¬ ëª¨ìŒ")
    print("=" * 50)
    
    for query_name, query in queries.items():
        print(f"\n-- {query_name.upper().replace('_', ' ')}")
        print(query.strip())
        print("\n" + "-" * 50)

if __name__ == "__main__":
    main()
