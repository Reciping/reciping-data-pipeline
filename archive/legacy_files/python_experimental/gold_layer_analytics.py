# gold_layer_analytics.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import os

def main():
    """
    Silver Layerì—ì„œ Gold Layer ë¹„ì¦ˆë‹ˆìŠ¤ ì§‘ê³„ í…Œì´ë¸”ì„ ìƒì„±í•˜ëŠ” 
    ê³ ê¸‰ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (Iceberg + Hive Metastore).
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['HADOOP_USER_NAME'] = 'root'
        os.environ['USER'] = 'root'
        os.environ['HOME'] = '/tmp'
        os.environ['JAVA_OPTS'] = '-Duser.name=root'
        os.environ['IVY_HOME'] = '/tmp/.ivy2'
        os.makedirs('/tmp/.ivy2', exist_ok=True)

        # -----------------------------------------------------------------------------
        # 1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„± (Iceberg + Hive Metastore ì„¤ì •)
        # -----------------------------------------------------------------------------
        print("ğŸ”§ SparkSession with Iceberg for Gold Layer ìƒì„±...")
        
        spark = SparkSession.builder \
            .appName("Gold_Layer_Analytics_Pipeline") \
            .master("local[*]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.jars.ivy", "/tmp/.ivy2") \
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.3") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
            .config("spark.sql.catalog.spark_catalog.type", "hive") \
            .config("spark.sql.catalog.spark_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.spark_catalog.warehouse", "s3a://reciping-user-event-logs/warehouse") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "")) \
            .config("spark.hadoop.fs.s3a.region", "ap-northeast-2") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.path.style.access", "false") \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSession with Iceberg for Gold Layer ìƒì„± ì™„ë£Œ!")

        # -----------------------------------------------------------------------------
        # 2. Silver ë°ì´í„° ë¡œë“œ
        # -----------------------------------------------------------------------------
        print("\nğŸ“Š Silver Layer ë°ì´í„° ë¡œë“œ...")
        df_silver = spark.table("silver_db.cleaned_events")
        silver_count = df_silver.count()
        print(f"âœ… Silverì—ì„œ {silver_count:,}í–‰ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        # -----------------------------------------------------------------------------
        # 3. ğŸ¥‡ Gold Layer ì§‘ê³„ í…Œì´ë¸”ë“¤ ìƒì„±
        # -----------------------------------------------------------------------------
        
        # --- 3.1. ì¼ë³„ ì´ë²¤íŠ¸ ìš”ì•½ í…Œì´ë¸” ---
        print("\nğŸ¥‡ Gold Layer - ì¼ë³„ ì´ë²¤íŠ¸ ìš”ì•½ í…Œì´ë¸” ìƒì„±...")
        
        daily_events_summary = df_silver.groupBy(
            "year", "month", "day", "event_name"
        ).agg(
            count("*").alias("event_count"),
            countDistinct("user_id").alias("unique_users"),
            countDistinct("session_id").alias("unique_sessions"),
            countDistinct("anonymous_id").alias("unique_anonymous_users"),
            min("event_timestamp").alias("first_event_time"),
            max("event_timestamp").alias("last_event_time"),
            current_timestamp().alias("aggregation_timestamp")
        ).withColumn(
            "event_date", 
            to_date(concat_ws("-", col("year"), 
                             lpad(col("month"), 2, "0"), 
                             lpad(col("day"), 2, "0")))
        )
        
        # ì¼ë³„ ìš”ì•½ í…Œì´ë¸” ì €ì¥
        daily_events_summary.writeTo("gold_db.daily_events_summary") \
            .partitionedBy("year", "month") \
            .tableProperty("format-version", "2") \
            .createOrReplace()
        
        print("âœ… ì¼ë³„ ì´ë²¤íŠ¸ ìš”ì•½ í…Œì´ë¸” 'gold_db.daily_events_summary' ìƒì„± ì™„ë£Œ")

        # --- 3.2. ì‚¬ìš©ìë³„ í–‰ë™ í”„ë¡œíŒŒì¼ í…Œì´ë¸” ---
        print("\nğŸ¥‡ Gold Layer - ì‚¬ìš©ìë³„ í–‰ë™ í”„ë¡œíŒŒì¼ í…Œì´ë¸” ìƒì„±...")
        
        user_behavior_profiles = df_silver.filter(col("user_id").isNotNull()).groupBy("user_id").agg(
            # ê¸°ë³¸ í†µê³„
            count("*").alias("total_events"),
            countDistinct("session_id").alias("total_sessions"),
            countDistinct("event_name").alias("unique_event_types"),
            
            # ì‹œê°„ ì •ë³´
            min("event_timestamp").alias("first_seen"),
            max("event_timestamp").alias("last_seen"),
            
            # ì´ë²¤íŠ¸ë³„ ì¹´ìš´íŠ¸
            sum(when(col("event_name") == "view_page", 1).otherwise(0)).alias("page_views"),
            sum(when(col("event_name") == "search_recipe", 1).otherwise(0)).alias("recipe_searches"),
            sum(when(col("event_name") == "click_recipe", 1).otherwise(0)).alias("recipe_clicks"),
            sum(when(col("event_name") == "view_recipe_list", 1).otherwise(0)).alias("list_views"),
            
            # ì‚¬ìš©ì ì†ì„± (ìµœì‹  ê°’)
            last("user_segment", True).alias("latest_user_segment"),
            last("activity_level", True).alias("latest_activity_level"),
            last("cooking_style", True).alias("latest_cooking_style"),
            last("ab_test_group", True).alias("latest_ab_test_group"),
            
            # ì§‘ê³„ ì‹œì 
            current_timestamp().alias("profile_updated_at")
        ).withColumn(
            "avg_events_per_session",
            round(col("total_events") / col("total_sessions"), 2)
        ).withColumn(
            "days_active",
            datediff(col("last_seen"), col("first_seen")) + 1
        )
        
        # ì‚¬ìš©ì í”„ë¡œíŒŒì¼ í…Œì´ë¸” ì €ì¥
        user_behavior_profiles.writeTo("gold_db.user_behavior_profiles") \
            .tableProperty("format-version", "2") \
            .createOrReplace()
        
        print("âœ… ì‚¬ìš©ìë³„ í–‰ë™ í”„ë¡œíŒŒì¼ í…Œì´ë¸” 'gold_db.user_behavior_profiles' ìƒì„± ì™„ë£Œ")

        # --- 3.3. ë ˆì‹œí”¼ ì¸ê¸°ë„ ë¶„ì„ í…Œì´ë¸” ---
        print("\nğŸ¥‡ Gold Layer - ë ˆì‹œí”¼ ì¸ê¸°ë„ ë¶„ì„ í…Œì´ë¸” ìƒì„±...")
        
        recipe_popularity = df_silver.filter(
            col("prop_recipe_id").isNotNull()
        ).groupBy("prop_recipe_id").agg(
            # ê¸°ë³¸ í†µê³„
            count("*").alias("total_interactions"),
            countDistinct("user_id").alias("unique_users"),
            countDistinct("session_id").alias("unique_sessions"),
            
            # ì´ë²¤íŠ¸ë³„ ë¶„ì„
            sum(when(col("event_name") == "click_recipe", 1).otherwise(0)).alias("clicks"),
            sum(when(col("event_name") == "view_recipe", 1).otherwise(0)).alias("views"),
            sum(when(col("event_name") == "bookmark_recipe", 1).otherwise(0)).alias("bookmarks"),
            sum(when(col("event_name") == "share_recipe", 1).otherwise(0)).alias("shares"),
            
            # ì‹œê°„ ì •ë³´
            min("event_timestamp").alias("first_interaction"),
            max("event_timestamp").alias("last_interaction"),
            
            # ì¹´í…Œê³ ë¦¬ ì •ë³´ (ìµœì‹  ê°’)
            last("prop_category", True).alias("recipe_category"),
            
            # í‰ê·  ìˆœìœ„ (ê²€ìƒ‰ ê²°ê³¼ì—ì„œ)
            avg("prop_rank").alias("avg_search_rank"),
            
            # ì§‘ê³„ ì‹œì 
            current_timestamp().alias("analysis_updated_at")
        ).withColumn(
            "click_through_rate",
            round(col("clicks") / col("total_interactions"), 4)
        ).withColumn(
            "engagement_score",
            col("clicks") * 1.0 + col("views") * 0.5 + col("bookmarks") * 2.0 + col("shares") * 3.0
        )
        
        # ë ˆì‹œí”¼ ì¸ê¸°ë„ í…Œì´ë¸” ì €ì¥
        recipe_popularity.writeTo("gold_db.recipe_popularity_analysis") \
            .tableProperty("format-version", "2") \
            .createOrReplace()
        
        print("âœ… ë ˆì‹œí”¼ ì¸ê¸°ë„ ë¶„ì„ í…Œì´ë¸” 'gold_db.recipe_popularity_analysis' ìƒì„± ì™„ë£Œ")

        # --- 3.4. ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„ í…Œì´ë¸” ---
        print("\nğŸ¥‡ Gold Layer - ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„ í…Œì´ë¸” ìƒì„±...")
        
        search_trends = df_silver.filter(
            (col("event_name") == "search_recipe") & 
            col("prop_search_keyword").isNotNull()
        ).groupBy(
            "year", "month", "day", "prop_search_keyword"
        ).agg(
            count("*").alias("search_count"),
            countDistinct("user_id").alias("unique_searchers"),
            countDistinct("session_id").alias("unique_sessions"),
            avg("prop_result_count").alias("avg_results_returned"),
            
            # ê²€ìƒ‰ íƒ€ì…ë³„ ë¶„ì„
            sum(when(col("prop_search_type") == "ingredient", 1).otherwise(0)).alias("ingredient_searches"),
            sum(when(col("prop_search_type") == "name", 1).otherwise(0)).alias("name_searches"),
            sum(when(col("prop_search_type") == "category", 1).otherwise(0)).alias("category_searches"),
            
            # ì‹œê°„ ì •ë³´
            min("event_timestamp").alias("first_search_time"),
            max("event_timestamp").alias("last_search_time"),
            
            current_timestamp().alias("trend_updated_at")
        ).withColumn(
            "search_date", 
            to_date(concat_ws("-", col("year"), 
                             lpad(col("month"), 2, "0"), 
                             lpad(col("day"), 2, "0")))
        )
        
        # ê²€ìƒ‰ íŠ¸ë Œë“œ í…Œì´ë¸” ì €ì¥
        search_trends.writeTo("gold_db.search_trends_analysis") \
            .partitionedBy("year", "month") \
            .tableProperty("format-version", "2") \
            .createOrReplace()
        
        print("âœ… ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„ í…Œì´ë¸” 'gold_db.search_trends_analysis' ìƒì„± ì™„ë£Œ")

        # --- 3.5. A/B í…ŒìŠ¤íŠ¸ ì„±ê³¼ ë¶„ì„ í…Œì´ë¸” ---
        print("\nğŸ¥‡ Gold Layer - A/B í…ŒìŠ¤íŠ¸ ì„±ê³¼ ë¶„ì„ í…Œì´ë¸” ìƒì„±...")
        
        ab_test_analysis = df_silver.filter(
            col("ab_test_group").isNotNull()
        ).groupBy(
            "ab_test_scenario", "ab_test_group", "event_name"
        ).agg(
            count("*").alias("event_count"),
            countDistinct("user_id").alias("unique_users"),
            countDistinct("session_id").alias("unique_sessions"),
            
            # ì „í™˜ìœ¨ ê³„ì‚°ì„ ìœ„í•œ ê¸°ë³¸ ë°ì´í„°
            sum(when(col("event_name") == "click_recipe", 1).otherwise(0)).alias("recipe_clicks"),
            sum(when(col("event_name") == "bookmark_recipe", 1).otherwise(0)).alias("bookmarks"),
            sum(when(col("event_name") == "purchase", 1).otherwise(0)).alias("purchases"),
            
            # ì‹œê°„ ì •ë³´
            min("event_timestamp").alias("analysis_period_start"),
            max("event_timestamp").alias("analysis_period_end"),
            
            current_timestamp().alias("ab_analysis_updated_at")
        )
        
        # A/B í…ŒìŠ¤íŠ¸ ë¶„ì„ í…Œì´ë¸” ì €ì¥
        ab_test_analysis.writeTo("gold_db.ab_test_performance") \
            .tableProperty("format-version", "2") \
            .createOrReplace()
        
        print("âœ… A/B í…ŒìŠ¤íŠ¸ ì„±ê³¼ ë¶„ì„ í…Œì´ë¸” 'gold_db.ab_test_performance' ìƒì„± ì™„ë£Œ")

        # -----------------------------------------------------------------------------
        # 4. ğŸ“Š Gold Layer í…Œì´ë¸” ê²€ì¦ ë° ìƒ˜í”Œ ì¡°íšŒ
        # -----------------------------------------------------------------------------
        print("\nğŸ“Š Gold Layer í…Œì´ë¸” ê²€ì¦...")
        
        # ìƒì„±ëœ Gold í…Œì´ë¸” ëª©ë¡
        print("\nğŸ—ƒï¸ ìƒì„±ëœ Gold Layer í…Œì´ë¸”:")
        spark.sql("SHOW TABLES IN gold_db").show()
        
        # ê° í…Œì´ë¸” ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        print("\nğŸ“‹ ì¼ë³„ ì´ë²¤íŠ¸ ìš”ì•½ (ìƒìœ„ 10í–‰):")
        spark.sql("SELECT * FROM gold_db.daily_events_summary ORDER BY event_date DESC, event_count DESC LIMIT 10").show()
        
        print("\nğŸ‘¤ ì‚¬ìš©ì í–‰ë™ í”„ë¡œíŒŒì¼ (ìƒìœ„ 5í–‰):")
        spark.sql("SELECT user_id, total_events, total_sessions, page_views, recipe_searches, latest_user_segment FROM gold_db.user_behavior_profiles ORDER BY total_events DESC LIMIT 5").show()
        
        print("\nğŸ³ ë ˆì‹œí”¼ ì¸ê¸°ë„ (ìƒìœ„ 10í–‰):")
        spark.sql("SELECT prop_recipe_id, total_interactions, unique_users, clicks, engagement_score FROM gold_db.recipe_popularity_analysis ORDER BY engagement_score DESC LIMIT 10").show()
        
        print("\nğŸ” ê²€ìƒ‰ íŠ¸ë Œë“œ (ìƒìœ„ 10í–‰):")
        spark.sql("SELECT prop_search_keyword, search_count, unique_searchers, search_date FROM gold_db.search_trends_analysis ORDER BY search_count DESC LIMIT 10").show()

        # -----------------------------------------------------------------------------
        # 5. ğŸ’ ê³ ê¸‰ ë¶„ì„ ì¿¼ë¦¬ ì˜ˆì œ ì‹¤í–‰
        # -----------------------------------------------------------------------------
        print("\nğŸ’ ê³ ê¸‰ ë¶„ì„ ì¿¼ë¦¬ ì‹¤í–‰...")
        
        # ì˜ˆì œ 1: ê°€ì¥ í™œì„± ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
        print("\nğŸ“ˆ ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ë³„ í™œë™ ë¶„ì„:")
        spark.sql("""
            SELECT 
                latest_user_segment,
                COUNT(*) as user_count,
                AVG(total_events) as avg_events_per_user,
                AVG(total_sessions) as avg_sessions_per_user,
                AVG(days_active) as avg_days_active
            FROM gold_db.user_behavior_profiles
            WHERE latest_user_segment IS NOT NULL
            GROUP BY latest_user_segment
            ORDER BY avg_events_per_user DESC
        """).show()
        
        # ì˜ˆì œ 2: ì¼ë³„ íŠ¸ë Œë“œ ë¶„ì„
        print("\nğŸ“… ìµœê·¼ ì¼ë³„ í™œë™ íŠ¸ë Œë“œ:")
        spark.sql("""
            SELECT 
                event_date,
                SUM(event_count) as total_events,
                SUM(unique_users) as total_unique_users,
                SUM(unique_sessions) as total_sessions
            FROM gold_db.daily_events_summary
            GROUP BY event_date
            ORDER BY event_date DESC
            LIMIT 7
        """).show()

        # -----------------------------------------------------------------------------
        # 6. ê²€ì¦ ë° ìš”ì•½
        # -----------------------------------------------------------------------------
        total_gold_tables = spark.sql("SHOW TABLES IN gold_db").count()
        
        print("\nğŸ“ˆ Gold Layer êµ¬ì¶• ì™„ë£Œ ìš”ì•½:")
        print(f"ğŸ¥ˆ Silver ì…ë ¥ ë°ì´í„°: {silver_count:,}í–‰")
        print(f"ğŸ¥‡ ìƒì„±ëœ Gold í…Œì´ë¸” ìˆ˜: {total_gold_tables}")
        print(f"ğŸ“Š ì£¼ìš” ì§‘ê³„ í…Œì´ë¸”:")
        print(f"   - daily_events_summary: ì¼ë³„ ì´ë²¤íŠ¸ ìš”ì•½")
        print(f"   - user_behavior_profiles: ì‚¬ìš©ì í–‰ë™ í”„ë¡œíŒŒì¼")
        print(f"   - recipe_popularity_analysis: ë ˆì‹œí”¼ ì¸ê¸°ë„ ë¶„ì„")
        print(f"   - search_trends_analysis: ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„")
        print(f"   - ab_test_performance: A/B í…ŒìŠ¤íŠ¸ ì„±ê³¼ ë¶„ì„")
        print(f"ğŸ—ï¸ ë©”íƒ€ìŠ¤í† ì–´: Hive Metastore")
        print(f"ğŸ’¾ ì €ì¥ì†Œ: Iceberg Tables on S3")

        # -----------------------------------------------------------------------------
        # 7. ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        spark.stop()
        print("âœ… Gold Layer êµ¬ì¶•ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"âŒ Gold Layer êµ¬ì¶• ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        try:
            spark.stop()
        except:
            pass

if __name__ == "__main__":
    main()
