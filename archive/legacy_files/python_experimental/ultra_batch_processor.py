#!/usr/bin/env python3
"""
ê·¹ì†Œí˜• ë°°ì¹˜ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ í¬ë˜ì‹œ ì™„ì „ í•´ê²°
- 5,000ê°œ ì´ë²¤íŠ¸ì”© ì²˜ë¦¬
- JOIN ì—†ëŠ” ë‹¨ìˆœ ì¿¼ë¦¬ ìš°ì„ 
- ì ì§„ì  ì°¨ì› ë§¤í•‘
"""

from pyspark.sql import SparkSession

class UltraBatchProcessor:
    """ê·¹ì†Œí˜• ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        self.batch_size = 5000  # ë§¤ìš° ì‘ì€ ë°°ì¹˜
        
    def create_minimal_spark_session(self):
        """ìµœì†Œí•œì˜ SparkSession"""
        print("ğŸ”§ ê·¹ì†Œí˜• ë°°ì¹˜ìš© SparkSession...")
        
        self.spark = SparkSession.builder \
            .appName("UltraBatch") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "1g") \
            .config("spark.executor.memory", "1g") \
            .config("spark.sql.shuffle.partitions", "20") \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… ê·¹ì†Œí˜• SparkSession ìƒì„± ì™„ë£Œ")
        
    def create_simple_fact_table_no_joins(self):
        """JOIN ì—†ëŠ” ë‹¨ìˆœ Fact í…Œì´ë¸” ë¨¼ì € ìƒì„±"""
        print("\nğŸ”„ JOIN ì—†ëŠ” ë‹¨ìˆœ Fact í…Œì´ë¸” ìƒì„±...")
        
        # Step 1: JOIN ì—†ì´ ê¸°ë³¸ êµ¬ì¡°ë§Œ ìƒì„±
        simple_query = f"""
        WITH silver_simple AS (
            SELECT 
                event_id,
                user_id,
                session_id, 
                anonymous_id,
                event_name,
                page_name,
                prop_recipe_id,
                utc_timestamp,
                date,
                prop_action
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE date = '2025-07-01'
            LIMIT {self.batch_size}
        ),
        fact_simple AS (
            SELECT 
                event_id,
                
                -- ì°¨ì› í‚¤ëŠ” ì¼ë‹¨ 0 ë˜ëŠ” ê¸°ë³¸ê°’
                0 as user_dim_key,
                CAST(DATE_FORMAT(utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(utc_timestamp) as time_dim_key,
                0 as recipe_dim_key,
                0 as page_dim_key,
                1 as event_dim_key,
                
                -- ë‹¨ìˆœ ì¸¡ì •ê°’
                1 as event_count,
                60 as session_duration_seconds,
                30 as page_view_duration_seconds,
                
                -- ë‹¨ìˆœ ì „í™˜ ë¡œì§
                CASE WHEN event_name IN ('auth_success', 'click_bookmark') THEN TRUE ELSE FALSE END as is_conversion,
                1.0 as conversion_value,
                2.0 as engagement_score,
                
                -- Degenerate dimensions
                session_id,
                anonymous_id,
                
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM silver_simple
            WHERE event_id IS NOT NULL
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
        SELECT * FROM fact_simple
        """
        
        try:
            print(f"   ğŸ”„ {self.batch_size:,}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì¤‘...")
            self.spark.sql(simple_query)
            
            # ê²°ê³¼ í™•ì¸
            count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"   âœ… ë‹¨ìˆœ Fact í…Œì´ë¸” ìƒì„± ì„±ê³µ: {count:,}ê°œ")
            
            return count
            
        except Exception as e:
            print(f"   âŒ ë‹¨ìˆœ Fact í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return 0
            
    def add_more_batches_incrementally(self):
        """ì ì§„ì ìœ¼ë¡œ ë” ë§ì€ ë°°ì¹˜ ì¶”ê°€"""
        print("\nğŸ”„ ì ì§„ì  ë°°ì¹˜ í™•ì¥...")
        
        dates = ['2025-07-02', '2025-07-03', '2025-07-04', '2025-07-05']
        total_added = 0
        
        for date in dates:
            try:
                print(f"   ğŸ“… {date} ì²˜ë¦¬ ì¤‘...")
                
                incremental_query = f"""
                INSERT INTO {self.catalog_name}.{self.gold_database}.fact_user_events
                SELECT 
                    event_id,
                    0 as user_dim_key,
                    CAST(DATE_FORMAT(utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(utc_timestamp) as time_dim_key,
                    0 as recipe_dim_key,
                    0 as page_dim_key,
                    1 as event_dim_key,
                    1 as event_count,
                    60 as session_duration_seconds,
                    30 as page_view_duration_seconds,
                    CASE WHEN event_name IN ('auth_success', 'click_bookmark') THEN TRUE ELSE FALSE END as is_conversion,
                    1.0 as conversion_value,
                    2.0 as engagement_score,
                    session_id,
                    anonymous_id,
                    CURRENT_TIMESTAMP() as created_at,
                    CURRENT_TIMESTAMP() as updated_at
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                WHERE date = '{date}' 
                AND event_id IS NOT NULL
                LIMIT {self.batch_size}
                """
                
                self.spark.sql(incremental_query)
                
                # í˜„ì¬ ì´ ë ˆì½”ë“œ ìˆ˜
                total_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
                added_today = min(self.batch_size, total_count - total_added - self.batch_size)  # ì¶”ì •
                total_added += added_today
                
                print(f"      âœ… {date} ì™„ë£Œ: +{added_today:,}ê°œ (ëˆ„ì : {total_count:,}ê°œ)")
                
            except Exception as e:
                print(f"      âŒ {date} ì‹¤íŒ¨: {str(e)[:50]}...")
                break
                
        return total_added
        
    def try_simple_dimension_mapping(self):
        """ê·¹ì†Œí˜• ë°°ì¹˜ë¡œ ì°¨ì› ë§¤í•‘ ì‹œë„"""
        print("\nğŸ”„ ê·¹ì†Œí˜• ì°¨ì› ë§¤í•‘ ì‹œë„...")
        
        try:
            # ê°€ì¥ ì‘ì€ ì°¨ì›ë¶€í„° (Page, Event)
            print("   ğŸ“± Page ì°¨ì› ë§¤í•‘...")
            page_mapping_query = f"""
            UPDATE {self.catalog_name}.{self.gold_database}.fact_user_events
            SET page_dim_key = (
                SELECT COALESCE(p.page_dim_key, 0)
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver s
                LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p 
                    ON s.page_name = p.page_name
                WHERE s.event_id = fact_user_events.event_id
                LIMIT 1
            )
            WHERE page_dim_key = 0
            """
            
            # UPDATEëŠ” Icebergì—ì„œ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°„ë‹¨í•œ í™•ì¸ë§Œ
            current_stats = self.spark.sql(f"""
            SELECT 
                COUNT(*) as total,
                COUNT(DISTINCT page_dim_key) as unique_pages,
                COUNT(DISTINCT event_dim_key) as unique_events
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            print(f"      í˜„ì¬ ìƒíƒœ: {current_stats.total:,}ë ˆì½”ë“œ, {current_stats.unique_pages}í˜ì´ì§€, {current_stats.unique_events}ì´ë²¤íŠ¸")
            
        except Exception as e:
            print(f"   âš ï¸ ì°¨ì› ë§¤í•‘ ì œí•œ: {str(e)[:50]}...")
            
    def validate_ultra_batch_result(self):
        """ê·¹ì†Œí˜• ë°°ì¹˜ ê²°ê³¼ ê²€ì¦"""
        print("\nğŸ” ê·¹ì†Œí˜• ë°°ì¹˜ ê²°ê³¼ ê²€ì¦...")
        
        try:
            stats = self.spark.sql(f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT time_dim_key) as unique_time_keys,
                COUNT(DISTINCT session_id) as unique_sessions,
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            print("ğŸ“Š ê·¹ì†Œí˜• ë°°ì¹˜ ê²°ê³¼:")
            print(f"   ì´ ë ˆì½”ë“œ: {stats.total_records:,}ê°œ")
            print(f"   ê³ ìœ  ì‹œê°„í‚¤: {stats.unique_time_keys:,}ê°œ")
            print(f"   ê³ ìœ  ì„¸ì…˜: {stats.unique_sessions:,}ê°œ")
            print(f"   ì „í™˜ ì´ë²¤íŠ¸: {stats.conversions:,}ê°œ")
            print(f"   í‰ê·  ì°¸ì—¬ë„: {stats.avg_engagement}ì ")
            
            # ì„±ê³µ ê¸°ì¤€
            if stats.total_records >= 10000:
                print("\nğŸ‰ ê·¹ì†Œí˜• ë°°ì¹˜ ì„±ê³µ!")
                print("   âœ… JVM í¬ë˜ì‹œ ì—†ì´ ì²˜ë¦¬")
                print("   âœ… ì•ˆì •ì ì¸ ê¸°ë°˜ êµ¬ì¶•")
                print("   âœ… ì ì§„ì  í™•ì¥ ê°€ëŠ¥")
            else:
                print("\nâš ï¸ ë¶€ë¶„ ì„±ê³µ")
                print("   ğŸ’¡ ë” ë³´ìˆ˜ì ì¸ ì ‘ê·¼ í•„ìš”")
                
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            
    def execute_ultra_batch_processing(self):
        """ê·¹ì†Œí˜• ë°°ì¹˜ ì²˜ë¦¬ ì „ì²´ ì‹¤í–‰"""
        print("ğŸ”¬ ê·¹ì†Œí˜• ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰...")
        print("=" * 60)
        
        try:
            # 1. ìµœì†Œí•œì˜ SparkSession
            self.create_minimal_spark_session()
            
            # 2. JOIN ì—†ëŠ” ë‹¨ìˆœ Fact í…Œì´ë¸”
            initial_count = self.create_simple_fact_table_no_joins()
            
            if initial_count > 0:
                # 3. ì ì§„ì  ë°°ì¹˜ í™•ì¥
                added_count = self.add_more_batches_incrementally()
                
                # 4. ì°¨ì› ë§¤í•‘ ì‹œë„ (ì„ íƒì )
                self.try_simple_dimension_mapping()
                
                # 5. ê²°ê³¼ ê²€ì¦
                self.validate_ultra_batch_result()
                
                print(f"\nğŸ‰ ê·¹ì†Œí˜• ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
                print(f"   âœ… ê¸°ë³¸: {initial_count:,}ê°œ")
                print(f"   âœ… ì¶”ê°€: {added_count:,}ê°œ")
                print(f"   âœ… ì´: {initial_count + added_count:,}ê°œ")
            else:
                print("\nâŒ ê·¹ì†Œí˜• ë°°ì¹˜ë„ ì‹¤íŒ¨")
                print("   ğŸ’¡ ì•„í‚¤í…ì²˜ ì¬ê²€í†  í•„ìš”")
                
        except Exception as e:
            print(f"âŒ ê·¹ì†Œí˜• ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    ultra_processor = UltraBatchProcessor()
    ultra_processor.execute_ultra_batch_processing()
