#!/usr/bin/env python3
"""
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ Gold Layer ìµœì†Œ êµ¬í˜„
ì™„ì „í•œ ì†”ë£¨ì…˜ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ìµœì í™” ì ‘ê·¼ë²•
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import os

class MinimalGoldLayer:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ Gold Layer êµ¬í˜„"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        
    def create_minimal_spark_session(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”ëœ SparkSession ìƒì„±"""
        print("ğŸ§Š ë©”ëª¨ë¦¬ ìµœì í™” SparkSession ìƒì„± ì¤‘...")
        
        self.spark = SparkSession.builder \
            .appName("MinimalGoldLayer") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.driver.maxResultSize", "512m") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.initialPartitionNum", "4") \
            .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "64MB") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.kryo.unsafe", "true") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
            .config("spark.sql.adaptive.localShuffleReader.enabled", "true") \
            .config("spark.sql.adaptive.skewJoin.enabled", "false") \
            .config("spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold", "32MB") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("ERROR")
        print("âœ… ë©”ëª¨ë¦¬ ìµœì í™” SparkSession ìƒì„± ì™„ë£Œ!")
        
    def create_minimal_fact_table(self):
        """ë©”ëª¨ë¦¬ ì•ˆì „í•œ ìµœì†Œ Fact í…Œì´ë¸” ìƒì„±"""
        print("\nğŸ“Š ë©”ëª¨ë¦¬ ì•ˆì „í•œ Fact í…Œì´ë¸” ìƒì„±...")
        
        # ë§¤ìš° ì‘ì€ ë°°ì¹˜ë¡œ ì‹œì‘
        minimal_query = f"""
        WITH silver_mini AS (
            SELECT 
                event_id,
                user_id,
                session_id,
                anonymous_id,
                event_name,
                page_name,
                prop_recipe_id,
                utc_timestamp,
                date
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE date = '2025-07-01'  -- í•˜ë£¨ë§Œ ì²˜ë¦¬
            LIMIT 5000  -- ë§¤ìš° ì‘ì€ ë°°ì¹˜
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
        SELECT 
            event_id,
            0 as user_dim_key,  -- ë‹¨ìˆœí™”
            CAST(DATE_FORMAT(utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(utc_timestamp) as time_dim_key,
            0 as recipe_dim_key,  -- ë‹¨ìˆœí™”
            0 as page_dim_key,  -- ë‹¨ìˆœí™”
            1 as event_dim_key,  -- ê¸°ë³¸ê°’
            
            1 as event_count,
            0 as session_duration_seconds,
            30 as page_view_duration_seconds,
            
            CASE WHEN event_name = 'auth_success' THEN TRUE ELSE FALSE END as is_conversion,
            1.0 as conversion_value,
            CASE 
                WHEN event_name = 'auth_success' THEN 10.0
                WHEN event_name = 'click_recipe' THEN 7.0
                WHEN event_name = 'search_recipe' THEN 5.0
                ELSE 2.0
            END as engagement_score,
            
            session_id,
            anonymous_id,
            
            CURRENT_TIMESTAMP() as created_at,
            CURRENT_TIMESTAMP() as updated_at
            
        FROM silver_mini
        WHERE event_id IS NOT NULL
        """
        
        try:
            self.spark.sql(minimal_query)
            
            # ê²°ê³¼ í™•ì¸
            count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
            print(f"âœ… ë©”ëª¨ë¦¬ ì•ˆì „í•œ Fact í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {count:,}ê°œ ë ˆì½”ë“œ")
            
            # ì ì§„ì  í™•ì¥
            self.expand_fact_table_gradually()
            
        except Exception as e:
            print(f"âŒ ë©”ëª¨ë¦¬ ì•ˆì „í•œ Fact í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
    def expand_fact_table_gradually(self):
        """ì ì§„ì ìœ¼ë¡œ Fact í…Œì´ë¸” í™•ì¥"""
        print("\nğŸ”„ ì ì§„ì  Fact í…Œì´ë¸” í™•ì¥...")
        
        # í•˜ë£¨ì”© ì¶”ê°€ ì²˜ë¦¬
        dates_to_process = [
            '2025-07-02', '2025-07-03', '2025-07-04', '2025-07-05'
        ]
        
        total_added = 0
        
        for date in dates_to_process:
            try:
                print(f"   ğŸ“… {date} ì²˜ë¦¬ ì¤‘...")
                
                expansion_query = f"""
                INSERT INTO {self.catalog_name}.{self.gold_database}.fact_user_events
                SELECT 
                    event_id,
                    0 as user_dim_key,
                    CAST(DATE_FORMAT(utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(utc_timestamp) as time_dim_key,
                    0 as recipe_dim_key,
                    0 as page_dim_key,
                    1 as event_dim_key,
                    
                    1 as event_count,
                    0 as session_duration_seconds,
                    30 as page_view_duration_seconds,
                    
                    CASE WHEN event_name = 'auth_success' THEN TRUE ELSE FALSE END as is_conversion,
                    1.0 as conversion_value,
                    CASE 
                        WHEN event_name = 'auth_success' THEN 10.0
                        WHEN event_name = 'click_recipe' THEN 7.0
                        WHEN event_name = 'search_recipe' THEN 5.0
                        ELSE 2.0
                    END as engagement_score,
                    
                    session_id,
                    anonymous_id,
                    
                    CURRENT_TIMESTAMP() as created_at,
                    CURRENT_TIMESTAMP() as updated_at
                    
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                WHERE date = '{date}'
                AND event_id IS NOT NULL
                LIMIT 5000
                """
                
                self.spark.sql(expansion_query)
                
                # ë°°ì¹˜ë³„ í™•ì¸
                current_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.fact_user_events").collect()[0]['cnt']
                batch_added = current_count - total_added
                total_added = current_count
                
                print(f"   âœ… {date} ì™„ë£Œ: +{batch_added:,}ê°œ ë ˆì½”ë“œ (ì´ {total_added:,}ê°œ)")
                
            except Exception as e:
                print(f"   âŒ {date} ì‹¤íŒ¨: {str(e)}")
                continue
        
        print(f"\nâœ… ì ì§„ì  í™•ì¥ ì™„ë£Œ: ì´ {total_added:,}ê°œ ë ˆì½”ë“œ")
        
    def validate_minimal_solution(self):
        """ìµœì†Œ ì†”ë£¨ì…˜ ê²€ì¦"""
        print("\nğŸ” ìµœì†Œ ì†”ë£¨ì…˜ ê²€ì¦...")
        
        try:
            stats = self.spark.sql(f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT time_dim_key) as unique_time_keys,
                COUNT(DISTINCT session_id) as unique_sessions,
                SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
                ROUND(AVG(engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            print("ğŸ“Š ìµœì†Œ ì†”ë£¨ì…˜ ê²°ê³¼:")
            print(f"   ì´ ë ˆì½”ë“œ: {stats['total_records']:,}ê°œ")
            print(f"   ê³ ìœ  ì‹œê°„í‚¤: {stats['unique_time_keys']:,}ê°œ")
            print(f"   ê³ ìœ  ì„¸ì…˜: {stats['unique_sessions']:,}ê°œ")
            print(f"   ì „í™˜ ì´ë²¤íŠ¸: {stats['conversions']:,}ê°œ")
            print(f"   í‰ê·  ì°¸ì—¬ë„: {stats['avg_engagement']}")
            
            # ê°„ë‹¨í•œ ì‹œê°„ëŒ€ë³„ ë¶„ì„
            hourly_analysis = self.spark.sql(f"""
            SELECT 
                (time_dim_key % 100) as hour,
                COUNT(*) as events,
                COUNT(DISTINCT session_id) as sessions,
                ROUND(AVG(engagement_score), 2) as avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            GROUP BY (time_dim_key % 100)
            ORDER BY hour
            LIMIT 10
            """).collect()
            
            print("\nâ° ì‹œê°„ëŒ€ë³„ ë¶„ì„ (ìƒìœ„ 10ì‹œê°„):")
            for row in hourly_analysis:
                print(f"   {row['hour']:02d}ì‹œ: {row['events']}ì´ë²¤íŠ¸, {row['sessions']}ì„¸ì…˜, {row['avg_engagement']}ì  ì°¸ì—¬ë„")
            
            print("\nâœ… ë©”ëª¨ë¦¬ ì•ˆì „í•œ ìµœì†Œ ì†”ë£¨ì…˜ ê²€ì¦ ì™„ë£Œ!")
            print("   ğŸ’¡ ì´ ê¸°ë°˜ìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ì™„ì „í•œ ì†”ë£¨ì…˜ êµ¬ì¶• ê°€ëŠ¥")
            
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            
    def execute_minimal_solution(self):
        """ìµœì†Œ ì†”ë£¨ì…˜ ì „ì²´ ì‹¤í–‰"""
        print("ğŸš€ ë©”ëª¨ë¦¬ ì•ˆì „í•œ ìµœì†Œ ì†”ë£¨ì…˜ ì‹¤í–‰...")
        print("=" * 50)
        
        try:
            # 1. SparkSession ìƒì„±
            self.create_minimal_spark_session()
            
            # 2. ìµœì†Œ Fact í…Œì´ë¸” ìƒì„±
            self.create_minimal_fact_table()
            
            # 3. ê²€ì¦
            self.validate_minimal_solution()
            
            print("\nğŸ‰ ìµœì†Œ ì†”ë£¨ì…˜ ì„±ê³µ!")
            print("   âœ… ë©”ëª¨ë¦¬ í¬ë˜ì‹œ ì—†ì´ ì•ˆì •ì  ì‹¤í–‰")
            print("   âœ… ê¸°ë³¸ ë¶„ì„ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
            print("   âœ… ì ì§„ì  í™•ì¥ ê¸°ë°˜ ë§ˆë ¨")
            
        except Exception as e:
            print(f"âŒ ìµœì†Œ ì†”ë£¨ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    minimal_gold = MinimalGoldLayer()
    minimal_gold.execute_minimal_solution()
