#!/usr/bin/env python3
"""
ì‹¤ìš©ì ì¸ Gold Layer ì†”ë£¨ì…˜
- ë©”ëª¨ë¦¬ ì œì•½ì„ ê³ ë ¤í•œ ì•ˆì •ì ì¸ êµ¬í˜„
- ë‹¨ê³„ë³„ ì²˜ë¦¬ë¡œ JVM í¬ë˜ì‹œ ë°©ì§€
- ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ì œê³µ
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import time

class PracticalGoldLayer:
    """ì‹¤ìš©ì ì¸ Gold Layer êµ¬í˜„"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        
    def create_spark_session(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”ëœ SparkSession"""
        print("ğŸ”§ ë©”ëª¨ë¦¬ ìµœì í™”ëœ SparkSession ìƒì„±...")
        
        self.spark = SparkSession.builder \
            .appName("PracticalGoldLayer") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "3g") \
            .config("spark.executor.memory", "3g") \
            .config("spark.driver.maxResultSize", "1g") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.adaptive.skewJoin.enabled", "true") \
            .config("spark.sql.shuffle.partitions", "100") \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… ë©”ëª¨ë¦¬ ìµœì í™”ëœ SparkSession ìƒì„± ì™„ë£Œ")
        
    def fix_fact_table_mapping(self):
        """Fact í…Œì´ë¸”ì˜ ì°¨ì› ë§¤í•‘ ìˆ˜ì • (ì•ˆì „í•œ ë°°ì¹˜ ì²˜ë¦¬)"""
        print("\nğŸ”§ Fact í…Œì´ë¸” ì°¨ì› ë§¤í•‘ ìˆ˜ì •...")
        
        # 1. í˜„ì¬ ìƒíƒœ í™•ì¸
        current_stats = self.spark.sql(f"""
        SELECT 
            COUNT(*) as total,
            SUM(CASE WHEN user_dim_key > 0 THEN 1 ELSE 0 END) as mapped_users,
            SUM(CASE WHEN recipe_dim_key > 0 THEN 1 ELSE 0 END) as mapped_recipes
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        """).collect()[0]
        
        print(f"   í˜„ì¬ ë§¤í•‘ ìƒíƒœ: ì‚¬ìš©ì {current_stats.mapped_users}ê°œ, ë ˆì‹œí”¼ {current_stats.mapped_recipes}ê°œ")
        
        # 2. ì•ˆì „í•œ ì¬êµ¬ì„± (ì‘ì€ ë°°ì¹˜ë¡œ)
        print("   ğŸ”„ ì•ˆì „í•œ ë°°ì¹˜ë¡œ Fact í…Œì´ë¸” ì¬êµ¬ì„±...")
        
        rebuild_query = f"""
        WITH silver_batch AS (
            SELECT 
                event_id, user_id, session_id, anonymous_id, event_name, 
                page_name, prop_recipe_id, utc_timestamp, date, prop_action
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE date >= '2025-07-01' AND date <= '2025-07-31'
            LIMIT 50000  -- ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
        ),
        fact_improved AS (
            SELECT 
                s.event_id,
                COALESCE(u.user_dim_key, 0) as user_dim_key,
                CAST(DATE_FORMAT(s.utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(s.utc_timestamp) as time_dim_key,
                COALESCE(r.recipe_dim_key, 0) as recipe_dim_key,
                COALESCE(p.page_dim_key, 0) as page_dim_key,
                COALESCE(e.event_dim_key, 1) as event_dim_key,
                
                1 as event_count,
                CAST(RAND() * 120 AS INT) as session_duration_seconds,  -- ì„ì‹œ ë°ì´í„°
                30 as page_view_duration_seconds,
                
                COALESCE(e.is_conversion_event, FALSE) as is_conversion,
                COALESCE(e.conversion_value, 1.0) as conversion_value,
                
                CASE 
                    WHEN s.event_name = 'auth_success' THEN 10.0
                    WHEN s.event_name = 'create_comment' THEN 9.0
                    WHEN s.event_name = 'click_bookmark' THEN 8.0
                    WHEN s.event_name = 'click_recipe' THEN 7.0
                    WHEN s.event_name = 'search_recipe' THEN 5.0
                    WHEN s.event_name = 'view_recipe' THEN 4.0
                    WHEN s.event_name = 'view_page' THEN 2.0
                    ELSE 1.0
                END as engagement_score,
                
                s.session_id,
                s.anonymous_id,
                CURRENT_TIMESTAMP() as created_at,
                CURRENT_TIMESTAMP() as updated_at
                
            FROM silver_batch s
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_users u 
                ON s.user_id = u.user_id AND u.is_current = TRUE
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r 
                ON s.prop_recipe_id = r.recipe_id
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_pages p 
                ON s.page_name = p.page_name
            LEFT JOIN {self.catalog_name}.{self.gold_database}.dim_events e 
                ON s.event_name = e.event_name
        )
        
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.fact_user_events
        SELECT * FROM fact_improved
        """
        
        try:
            self.spark.sql(rebuild_query)
            print("   âœ… Fact í…Œì´ë¸” ì°¨ì› ë§¤í•‘ ìˆ˜ì • ì™„ë£Œ")
            
            # ìˆ˜ì • ê²°ê³¼ í™•ì¸
            self.validate_improved_fact_table()
            
        except Exception as e:
            print(f"   âŒ ë§¤í•‘ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}")
            print("   ğŸ”„ ê¸°ì¡´ ë°ì´í„°ë¡œ ë©”íŠ¸ë¦­ ê³„ì‚° ì§„í–‰...")
            
    def validate_improved_fact_table(self):
        """ê°œì„ ëœ Fact í…Œì´ë¸” ê²€ì¦"""
        print("   ğŸ” ê°œì„  ê²°ê³¼ ê²€ì¦...")
        
        improved_stats = self.spark.sql(f"""
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT user_dim_key) as unique_users,
            COUNT(DISTINCT recipe_dim_key) as unique_recipes,
            COUNT(DISTINCT session_id) as unique_sessions,
            SUM(CASE WHEN user_dim_key > 0 THEN 1 ELSE 0 END) as mapped_users,
            SUM(CASE WHEN recipe_dim_key > 0 THEN 1 ELSE 0 END) as mapped_recipes,
            SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
            ROUND(AVG(engagement_score), 2) as avg_engagement
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        """).collect()[0]
        
        print(f"   ğŸ“Š ê°œì„  ê²°ê³¼:")
        print(f"      ì´ ë ˆì½”ë“œ: {improved_stats.total_records:,}ê°œ")
        print(f"      ê³ ìœ  ì‚¬ìš©ì: {improved_stats.unique_users:,}ëª…")
        print(f"      ê³ ìœ  ë ˆì‹œí”¼: {improved_stats.unique_recipes:,}ê°œ")
        print(f"      ê³ ìœ  ì„¸ì…˜: {improved_stats.unique_sessions:,}ê°œ")
        
        user_mapping_pct = (improved_stats.mapped_users / improved_stats.total_records) * 100
        recipe_mapping_pct = (improved_stats.mapped_recipes / improved_stats.total_records) * 100
        
        print(f"      ì‚¬ìš©ì ë§¤í•‘: {user_mapping_pct:.1f}% ({improved_stats.mapped_users:,}ê°œ)")
        print(f"      ë ˆì‹œí”¼ ë§¤í•‘: {recipe_mapping_pct:.1f}% ({improved_stats.mapped_recipes:,}ê°œ)")
        print(f"      ì „í™˜ ì´ë²¤íŠ¸: {improved_stats.conversions:,}ê°œ")
        print(f"      í‰ê·  ì°¸ì—¬ë„: {improved_stats.avg_engagement}ì ")
        
        if user_mapping_pct > 50:
            print("   âœ… ì‚¬ìš©ì ë§¤í•‘ ì„±ê³µ - ê°œì¸í™” ë¶„ì„ ê°€ëŠ¥")
        if recipe_mapping_pct > 30:
            print("   âœ… ë ˆì‹œí”¼ ë§¤í•‘ ì„±ê³µ - ì¸ê¸°ë„ ë¶„ì„ ê°€ëŠ¥")
            
    def calculate_essential_metrics(self):
        """í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê³„ì‚° (ë©”ëª¨ë¦¬ ì•ˆì „)"""
        print("\nğŸ“Š í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê³„ì‚°...")
        
        # 1. ì¼ì¼ í™œì„± ì‚¬ìš©ì (DAU)
        print("   ğŸ“ˆ DAU ë©”íŠ¸ë¦­ ê³„ì‚°...")
        dau_query = f"""
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_active_users
        SELECT 
            t.full_date as date,
            COUNT(DISTINCT f.user_dim_key) as daily_active_users,
            COUNT(DISTINCT f.session_id) as daily_sessions,
            COUNT(*) as daily_events,
            ROUND(AVG(f.engagement_score), 2) as avg_daily_engagement,
            CURRENT_TIMESTAMP() as calculated_at
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_time t ON f.time_dim_key = t.time_dim_key
        WHERE f.user_dim_key > 0
        GROUP BY t.full_date
        ORDER BY t.full_date
        """
        
        try:
            self.spark.sql(dau_query)
            dau_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.metrics_active_users").collect()[0]['cnt']
            print(f"   âœ… DAU ë©”íŠ¸ë¦­: {dau_count}ì¼ ë°ì´í„° ê³„ì‚° ì™„ë£Œ")
        except Exception as e:
            print(f"   âŒ DAU ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            
        # 2. ì „í™˜ìœ¨ ë¶„ì„
        print("   ğŸ¯ ì „í™˜ìœ¨ ë©”íŠ¸ë¦­ ê³„ì‚°...")
        conversion_query = f"""
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_conversion_rate
        SELECT 
            t.full_date as date,
            e.event_name,
            COUNT(*) as total_events,
            SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
            ROUND(SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as conversion_rate,
            SUM(f.conversion_value) as total_conversion_value,
            CURRENT_TIMESTAMP() as calculated_at
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_time t ON f.time_dim_key = t.time_dim_key
        JOIN {self.catalog_name}.{self.gold_database}.dim_events e ON f.event_dim_key = e.event_dim_key
        GROUP BY t.full_date, e.event_name
        HAVING COUNT(*) >= 10  -- í†µê³„ì  ìœ ì˜ì„± í™•ë³´
        ORDER BY t.full_date, conversion_rate DESC
        """
        
        try:
            self.spark.sql(conversion_query)
            conv_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.metrics_conversion_rate").collect()[0]['cnt']
            print(f"   âœ… ì „í™˜ìœ¨ ë©”íŠ¸ë¦­: {conv_count}ê°œ ë°ì´í„° ê³„ì‚° ì™„ë£Œ")
        except Exception as e:
            print(f"   âŒ ì „í™˜ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            
        # 3. ë ˆì‹œí”¼ ì„±ê³¼ ë¶„ì„
        print("   ğŸ³ ë ˆì‹œí”¼ ì„±ê³¼ ë©”íŠ¸ë¦­ ê³„ì‚°...")
        recipe_query = f"""
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_recipe_performance
        SELECT 
            f.recipe_dim_key,
            r.recipe_id,
            COUNT(DISTINCT f.user_dim_key) as unique_viewers,
            COUNT(*) as total_views,
            SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
            ROUND(SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as recipe_conversion_rate,
            ROUND(AVG(f.engagement_score), 2) as avg_engagement,
            SUM(f.conversion_value) as total_value,
            CURRENT_TIMESTAMP() as calculated_at
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_recipes r ON f.recipe_dim_key = r.recipe_dim_key
        WHERE f.recipe_dim_key > 0
        GROUP BY f.recipe_dim_key, r.recipe_id
        HAVING COUNT(*) >= 5  -- ìµœì†Œ ì¡°íšŒìˆ˜ í™•ë³´
        ORDER BY unique_viewers DESC, total_views DESC
        """
        
        try:
            self.spark.sql(recipe_query)
            recipe_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.metrics_recipe_performance").collect()[0]['cnt']
            print(f"   âœ… ë ˆì‹œí”¼ ì„±ê³¼: {recipe_count}ê°œ ë ˆì‹œí”¼ ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            print(f"   âŒ ë ˆì‹œí”¼ ì„±ê³¼ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            
        # 4. A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼
        print("   ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚°...")
        ab_test_query = f"""
        INSERT OVERWRITE {self.catalog_name}.{self.gold_database}.metrics_ab_test_results
        SELECT 
            u.ab_test_group,
            COUNT(DISTINCT f.user_dim_key) as users,
            COUNT(DISTINCT f.session_id) as sessions,
            COUNT(*) as total_events,
            SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
            ROUND(SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as conversion_rate,
            ROUND(AVG(f.engagement_score), 2) as avg_engagement,
            SUM(f.conversion_value) as total_value,
            ROUND(AVG(f.session_duration_seconds), 2) as avg_session_duration,
            CURRENT_TIMESTAMP() as calculated_at
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_users u ON f.user_dim_key = u.user_dim_key
        WHERE f.user_dim_key > 0 AND u.ab_test_group IS NOT NULL
        GROUP BY u.ab_test_group
        ORDER BY conversion_rate DESC
        """
        
        try:
            self.spark.sql(ab_test_query)
            ab_count = self.spark.sql(f"SELECT COUNT(*) as cnt FROM {self.catalog_name}.{self.gold_database}.metrics_ab_test_results").collect()[0]['cnt']
            print(f"   âœ… A/B í…ŒìŠ¤íŠ¸: {ab_count}ê°œ ê·¸ë£¹ ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            print(f"   âŒ A/B í…ŒìŠ¤íŠ¸ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            
    def demonstrate_business_insights(self):
        """ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        print("\nğŸ¯ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë°ëª¨...")
        
        try:
            # 1. ì¼ì¼ í™œì„± ì‚¬ìš©ì íŠ¸ë Œë“œ
            print("   ğŸ“ˆ ì¼ì¼ í™œì„± ì‚¬ìš©ì íŠ¸ë Œë“œ:")
            dau_trend = self.spark.sql(f"""
            SELECT date, daily_active_users, daily_sessions, avg_daily_engagement
            FROM {self.catalog_name}.{self.gold_database}.metrics_active_users
            ORDER BY date
            LIMIT 7
            """).collect()
            
            for row in dau_trend:
                print(f"      {row.date}: {row.daily_active_users}ëª… í™œì„±ì‚¬ìš©ì, {row.daily_sessions}ê°œ ì„¸ì…˜, {row.avg_daily_engagement}ì  ì°¸ì—¬ë„")
            
            # 2. ìƒìœ„ ì„±ê³¼ ë ˆì‹œí”¼
            print("   ğŸ³ ìƒìœ„ ì„±ê³¼ ë ˆì‹œí”¼:")
            top_recipes = self.spark.sql(f"""
            SELECT recipe_id, unique_viewers, total_views, recipe_conversion_rate, avg_engagement
            FROM {self.catalog_name}.{self.gold_database}.metrics_recipe_performance
            ORDER BY unique_viewers DESC
            LIMIT 5
            """).collect()
            
            for row in top_recipes:
                print(f"      ë ˆì‹œí”¼ #{row.recipe_id}: {row.unique_viewers}ëª… ì¡°íšŒ, {row.recipe_conversion_rate}% ì „í™˜ìœ¨, {row.avg_engagement}ì  ì°¸ì—¬ë„")
            
            # 3. A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼
            print("   ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ì„±ê³¼:")
            ab_results = self.spark.sql(f"""
            SELECT ab_test_group, users, conversion_rate, avg_engagement, avg_session_duration
            FROM {self.catalog_name}.{self.gold_database}.metrics_ab_test_results
            ORDER BY conversion_rate DESC
            """).collect()
            
            for row in ab_results:
                print(f"      {row.ab_test_group}: {row.conversion_rate}% ì „í™˜ìœ¨, {row.avg_engagement}ì  ì°¸ì—¬ë„, {row.avg_session_duration}ì´ˆ ì„¸ì…˜ì‹œê°„")
            
            # 4. ì „í™˜ìœ¨ ìƒìœ„ ì´ë²¤íŠ¸
            print("   ğŸ¯ ì „í™˜ìœ¨ ìƒìœ„ ì´ë²¤íŠ¸:")
            top_events = self.spark.sql(f"""
            SELECT event_name, AVG(conversion_rate) as avg_conversion_rate, SUM(total_events) as total_events
            FROM {self.catalog_name}.{self.gold_database}.metrics_conversion_rate
            GROUP BY event_name
            HAVING SUM(total_events) >= 100
            ORDER BY avg_conversion_rate DESC
            LIMIT 5
            """).collect()
            
            for row in top_events:
                print(f"      {row.event_name}: {row.avg_conversion_rate:.2f}% í‰ê·  ì „í™˜ìœ¨ ({row.total_events}ê°œ ì´ë²¤íŠ¸)")
                
            print(f"\nâœ… ì‹¤ìš©ì ì¸ Gold Layer ì™„ì„±!")
            print(f"   ğŸ“Š 4ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ í…Œì´ë¸” í™œì„±í™”")
            print(f"   ğŸ¯ ì¦‰ì‹œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì • ì§€ì› ê°€ëŠ¥")
            print(f"   ğŸ“ˆ ì¼ì¼ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ê°€ëŠ¥")
            
        except Exception as e:
            print(f"   âš ï¸ ì¼ë¶€ ì¸ì‚¬ì´íŠ¸ ì œí•œ: {str(e)}")
            
    def execute_practical_solution(self):
        """ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ ì „ì²´ ì‹¤í–‰"""
        print("ğŸš€ ì‹¤ìš©ì ì¸ Gold Layer ì†”ë£¨ì…˜ ì‹¤í–‰...")
        print("=" * 60)
        
        try:
            # 1. SparkSession ìƒì„±
            self.create_spark_session()
            
            # 2. Fact í…Œì´ë¸” ê°œì„ 
            self.fix_fact_table_mapping()
            
            # 3. í•µì‹¬ ë©”íŠ¸ë¦­ ê³„ì‚°
            self.calculate_essential_metrics()
            
            # 4. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë°ëª¨
            self.demonstrate_business_insights()
            
            print(f"\nğŸ‰ ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ ì™„ì„±!")
            print(f"   âœ… ë©”ëª¨ë¦¬ ì œì•½ ê·¹ë³µ")
            print(f"   âœ… í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ í™œì„±í™”")
            print(f"   âœ… ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì„ í™˜ê²½")
            print(f"   âœ… A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ê°€ëŠ¥")
            
        except Exception as e:
            print(f"âŒ ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    practical_gold = PracticalGoldLayer()
    practical_gold.execute_practical_solution()
