#!/usr/bin/env python3
"""
ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ì—ì„œ ì•ˆì „í•œ ë©”íŠ¸ë¦­ ê³„ì‚°
- JVM í¬ë˜ì‹œ ì—†ëŠ” ë‹¨ìˆœ ì¿¼ë¦¬ë§Œ ì‚¬ìš©
- ê¸°ì¡´ 25,000ê°œ ë ˆì½”ë“œë¡œ ì˜ë¯¸ìˆëŠ” ë¶„ì„ ì œê³µ
- ì¦‰ì‹œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì • ì§€ì›
"""

from pyspark.sql import SparkSession

class SafeGoldAnalytics:
    """ì•ˆì „í•œ Gold Layer ë¶„ì„"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        
    def create_minimal_spark_session(self):
        """ìµœì†Œí•œì˜ ì•ˆì „í•œ SparkSession"""
        print("ğŸ”§ ì•ˆì „í•œ SparkSession ìƒì„±...")
        
        self.spark = SparkSession.builder \
            .appName("SafeAnalytics") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.sql.shuffle.partitions", "50") \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… ì•ˆì „í•œ SparkSession ìƒì„± ì™„ë£Œ")
        
    def analyze_current_state(self):
        """í˜„ì¬ ë°ì´í„° ìƒíƒœ ë¶„ì„"""
        print("\nğŸ“Š í˜„ì¬ Gold Layer ë°ì´í„° ë¶„ì„...")
        
        # 1. ê¸°ë³¸ í†µê³„
        print("   ğŸ” ê¸°ë³¸ í†µê³„:")
        basic_stats = self.spark.sql(f"""
        SELECT 
            COUNT(*) as total_events,
            COUNT(DISTINCT session_id) as total_sessions,
            COUNT(DISTINCT time_dim_key) as time_periods,
            COUNT(DISTINCT page_dim_key) as pages_visited,
            COUNT(DISTINCT event_dim_key) as event_types,
            SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
            ROUND(AVG(engagement_score), 2) as avg_engagement,
            ROUND(AVG(session_duration_seconds), 2) as avg_session_time
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        """).collect()[0]
        
        print(f"      ì´ ì´ë²¤íŠ¸: {basic_stats.total_events:,}ê°œ")
        print(f"      ì´ ì„¸ì…˜: {basic_stats.total_sessions:,}ê°œ")
        print(f"      ì‹œê°„ êµ¬ê°„: {basic_stats.time_periods}ê°œ")
        print(f"      ë°©ë¬¸ í˜ì´ì§€: {basic_stats.pages_visited}ê°œ")
        print(f"      ì´ë²¤íŠ¸ íƒ€ì…: {basic_stats.event_types}ê°œ")
        print(f"      ì „í™˜ ì´ë²¤íŠ¸: {basic_stats.conversions:,}ê°œ")
        print(f"      í‰ê·  ì°¸ì—¬ë„: {basic_stats.avg_engagement}ì ")
        print(f"      í‰ê·  ì„¸ì…˜ ì‹œê°„: {basic_stats.avg_session_time}ì´ˆ")
        
        # ì „í™˜ìœ¨ ê³„ì‚°
        conversion_rate = (basic_stats.conversions / basic_stats.total_events) * 100
        print(f"      ì „í™˜ìœ¨: {conversion_rate:.2f}%")
        
    def calculate_safe_metrics(self):
        """JVM í¬ë˜ì‹œ ì—†ëŠ” ì•ˆì „í•œ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\nğŸ“ˆ ì•ˆì „í•œ ë©”íŠ¸ë¦­ ê³„ì‚°...")
        
        # 1. ì´ë²¤íŠ¸ íƒ€ì…ë³„ ë¶„ì„ (ê°„ë‹¨í•œ GROUP BY)
        print("   ğŸ¬ ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì„±ê³¼:")
        event_analysis = self.spark.sql(f"""
        SELECT 
            e.event_name,
            e.event_category,
            COUNT(*) as event_count,
            SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
            ROUND(SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as conversion_rate,
            ROUND(AVG(f.engagement_score), 2) as avg_engagement
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_events e ON f.event_dim_key = e.event_dim_key
        GROUP BY e.event_name, e.event_category
        ORDER BY event_count DESC
        """).collect()
        
        for row in event_analysis:
            print(f"      {row.event_name}: {row.event_count:,}ê±´ ({row.conversion_rate}% ì „í™˜ìœ¨, {row.avg_engagement}ì  ì°¸ì—¬ë„)")
            
        # 2. í˜ì´ì§€ë³„ ë¶„ì„
        print("   ğŸ“± í˜ì´ì§€ë³„ ì„±ê³¼:")
        page_analysis = self.spark.sql(f"""
        SELECT 
            p.page_name,
            p.funnel_stage,
            COUNT(*) as visits,
            COUNT(DISTINCT f.session_id) as unique_sessions,
            SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
            ROUND(AVG(f.engagement_score), 2) as avg_engagement
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_pages p ON f.page_dim_key = p.page_dim_key
        WHERE p.page_name != 'Unknown'
        GROUP BY p.page_name, p.funnel_stage
        ORDER BY visits DESC
        """).collect()
        
        for row in page_analysis:
            conversion_rate = (row.conversions / row.visits) * 100 if row.visits > 0 else 0
            print(f"      {row.page_name} ({row.funnel_stage}): {row.visits:,}ë°©ë¬¸, {row.unique_sessions:,}ì„¸ì…˜, {conversion_rate:.2f}% ì „í™˜ìœ¨")
            
        # 3. ì‹œê°„ëŒ€ë³„ ë¶„ì„
        print("   â° ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´:")
        time_analysis = self.spark.sql(f"""
        SELECT 
            t.hour,
            COUNT(*) as events,
            COUNT(DISTINCT f.session_id) as sessions,
            SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
            ROUND(AVG(f.engagement_score), 2) as avg_engagement
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_time t ON f.time_dim_key = t.time_dim_key
        GROUP BY t.hour
        ORDER BY t.hour
        """).collect()
        
        for row in time_analysis:
            conversion_rate = (row.conversions / row.events) * 100 if row.events > 0 else 0
            print(f"      {row.hour:02d}ì‹œ: {row.events:,}ì´ë²¤íŠ¸, {row.sessions:,}ì„¸ì…˜, {conversion_rate:.2f}% ì „í™˜ìœ¨, {row.avg_engagement}ì  ì°¸ì—¬ë„")
            
    def provide_business_insights(self):
        """í˜„ì¬ ë°ì´í„°ë¡œ ì œê³µ ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸"""
        print("\nğŸ¯ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­...")
        
        # 1. ìµœê³  ì„±ê³¼ ì´ë²¤íŠ¸ ì‹ë³„
        top_event = self.spark.sql(f"""
        SELECT 
            e.event_name,
            COUNT(*) as event_count,
            ROUND(SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as conversion_rate
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_events e ON f.event_dim_key = e.event_dim_key
        GROUP BY e.event_name
        HAVING COUNT(*) >= 100
        ORDER BY conversion_rate DESC
        LIMIT 1
        """).collect()[0]
        
        print(f"   ğŸ† ìµœê³  ì „í™˜ ì´ë²¤íŠ¸: {top_event.event_name} ({top_event.conversion_rate}% ì „í™˜ìœ¨)")
        
        # 2. ìµœì  ì‚¬ìš© ì‹œê°„ëŒ€
        peak_hour = self.spark.sql(f"""
        SELECT 
            t.hour,
            COUNT(*) as events,
            ROUND(AVG(f.engagement_score), 2) as avg_engagement
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_time t ON f.time_dim_key = t.time_dim_key
        GROUP BY t.hour
        ORDER BY avg_engagement DESC
        LIMIT 1
        """).collect()[0]
        
        print(f"   ğŸ“ˆ ìµœì  í™œë™ ì‹œê°„: {peak_hour.hour}ì‹œ (í‰ê·  {peak_hour.avg_engagement}ì  ì°¸ì—¬ë„)")
        
        # 3. ì „í™˜ í¼ë„ ë¶„ì„
        funnel_analysis = self.spark.sql(f"""
        SELECT 
            p.funnel_stage,
            COUNT(*) as stage_events,
            SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) as conversions,
            ROUND(SUM(CASE WHEN f.is_conversion = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as stage_conversion_rate
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_pages p ON f.page_dim_key = p.page_dim_key
        WHERE p.funnel_stage != 'Unknown'
        GROUP BY p.funnel_stage
        ORDER BY stage_events DESC
        """).collect()
        
        print(f"   ğŸ”„ ì „í™˜ í¼ë„ ì„±ê³¼:")
        for row in funnel_analysis:
            print(f"      {row.funnel_stage}: {row.stage_events:,}ì´ë²¤íŠ¸ â†’ {row.conversions:,}ì „í™˜ ({row.stage_conversion_rate}%)")
            
        # 4. ì „ì²´ ì„±ê³¼ ìš”ì•½
        total_stats = self.spark.sql(f"""
        SELECT 
            COUNT(DISTINCT session_id) as total_sessions,
            SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) as total_conversions,
            ROUND(SUM(CASE WHEN is_conversion = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(DISTINCT session_id), 2) as session_conversion_rate,
            ROUND(AVG(engagement_score), 2) as overall_engagement
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
        """).collect()[0]
        
        print(f"\nğŸ“Š ì „ì²´ ì„±ê³¼ ìš”ì•½:")
        print(f"   ì„¸ì…˜ ìˆ˜: {total_stats.total_sessions:,}ê°œ")
        print(f"   ì „í™˜ ìˆ˜: {total_stats.total_conversions:,}ê°œ")
        print(f"   ì„¸ì…˜ ì „í™˜ìœ¨: {total_stats.session_conversion_rate}%")
        print(f"   ì „ì²´ ì°¸ì—¬ë„: {total_stats.overall_engagement}ì ")
        
    def suggest_improvements(self):
        """í˜„ì¬ ë°ì´í„° ê¸°ë°˜ ê°œì„  ì œì•ˆ"""
        print("\nğŸ’¡ ë°ì´í„° ê¸°ë°˜ ê°œì„  ì œì•ˆ...")
        
        # 1. ë‚®ì€ ì„±ê³¼ í˜ì´ì§€ ì‹ë³„
        low_performance_pages = self.spark.sql(f"""
        SELECT 
            p.page_name,
            COUNT(*) as visits,
            ROUND(AVG(f.engagement_score), 2) as avg_engagement
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_pages p ON f.page_dim_key = p.page_dim_key
        WHERE p.page_name != 'Unknown'
        GROUP BY p.page_name
        HAVING COUNT(*) >= 100
        ORDER BY avg_engagement ASC
        LIMIT 2
        """).collect()
        
        print(f"   ğŸ“‰ ê°œì„  í•„ìš” í˜ì´ì§€:")
        for row in low_performance_pages:
            print(f"      {row.page_name}: {row.avg_engagement}ì  ì°¸ì—¬ë„ (í‰ê·  ì´í•˜)")
            
        # 2. ë¹„í™œì„± ì‹œê°„ëŒ€ ì‹ë³„
        low_activity_hours = self.spark.sql(f"""
        SELECT 
            t.hour,
            COUNT(*) as events
        FROM {self.catalog_name}.{self.gold_database}.fact_user_events f
        JOIN {self.catalog_name}.{self.gold_database}.dim_time t ON f.time_dim_key = t.time_dim_key
        GROUP BY t.hour
        ORDER BY events ASC
        LIMIT 3
        """).collect()
        
        print(f"   â° í™œì„±í™” í•„ìš” ì‹œê°„ëŒ€:")
        for row in low_activity_hours:
            print(f"      {row.hour:02d}ì‹œ: {row.events}ì´ë²¤íŠ¸ (ë§ˆì¼€íŒ… ê¸°íšŒ)")
            
        print(f"\nğŸ¯ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜:")
        print(f"   1. ê³ ì „í™˜ ì´ë²¤íŠ¸ í”„ë¡œëª¨ì…˜ ê°•í™”")
        print(f"   2. í”¼í¬ ì‹œê°„ëŒ€ ì»¨í…ì¸  ìµœì í™”")  
        print(f"   3. ì €ì„±ê³¼ í˜ì´ì§€ UX ê°œì„ ")
        print(f"   4. ë¹„í™œì„± ì‹œê°„ëŒ€ íƒ€ê²Ÿ ë§ˆì¼€íŒ…")
        
    def execute_safe_analysis(self):
        """ì•ˆì „í•œ ë¶„ì„ ì „ì²´ ì‹¤í–‰"""
        print("ğŸ›¡ï¸ ì•ˆì „í•œ Gold Layer ë¶„ì„ ì‹¤í–‰...")
        print("=" * 60)
        
        try:
            # 1. SparkSession ìƒì„±
            self.create_minimal_spark_session()
            
            # 2. í˜„ì¬ ìƒíƒœ ë¶„ì„
            self.analyze_current_state()
            
            # 3. ì•ˆì „í•œ ë©”íŠ¸ë¦­ ê³„ì‚°
            self.calculate_safe_metrics()
            
            # 4. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ì œê³µ
            self.provide_business_insights()
            
            # 5. ê°œì„  ì œì•ˆ
            self.suggest_improvements()
            
            print(f"\nğŸ‰ ì•ˆì „í•œ ë¶„ì„ ì™„ë£Œ!")
            print(f"   âœ… JVM í¬ë˜ì‹œ ì—†ìŒ")
            print(f"   âœ… ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ")
            print(f"   âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì • ì§€ì› ë°ì´í„° í™•ë³´")
            print(f"   âœ… ê°œì„  ë°©í–¥ ëª…í™•í™”")
            
        except Exception as e:
            print(f"âŒ ì•ˆì „í•œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    safe_analytics = SafeGoldAnalytics()
    safe_analytics.execute_safe_analysis()
