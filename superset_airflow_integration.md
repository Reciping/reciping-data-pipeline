# ğŸš€ Apache Superset + Airflow ëŒ€ì‹œë³´ë“œ ì•„í‚¤í…ì²˜

## ğŸ“‹ **ì „ì²´ êµ¬ì¡°**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Apache        â”‚    â”‚   SQL Files     â”‚    â”‚   Apache        â”‚
â”‚   Airflow       â”‚ â”€â”€ â”‚   Repository    â”‚ â”€â”€ â”‚   Superset      â”‚
â”‚  (ìŠ¤ì¼€ì¤„ë§)      â”‚    â”‚  (ì¿¼ë¦¬ ê´€ë¦¬)     â”‚    â”‚  (ì‹œê°í™”)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Gold Layer    â”‚
                    â”‚ fact_user_eventsâ”‚
                    â”‚ (Star Schema)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ **1. Airflow DAG êµ¬ì„±**

### ğŸ“… **Daily ETL + Dashboard Refresh DAG**
```python
# dags/reciping_dashboard_pipeline.py
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from airflow.providers.http.operators.http import SimpleHttpOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'reciping-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 8, 23),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'reciping_dashboard_pipeline',
    default_args=default_args,
    description='Reciping ëŒ€ì‹œë³´ë“œ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 1 * * *',  # ë§¤ì¼ ìƒˆë²½ 1ì‹œ
    catchup=False
)

# Bronze â†’ Silver ë³€í™˜
bronze_to_silver = BashOperator(
    task_id='bronze_to_silver',
    bash_command='cd /opt/reciping && python bronze_to_silver_iceberg.py',
    dag=dag
)

# Silver â†’ Gold ë³€í™˜ (Star Schema)
silver_to_gold = BashOperator(
    task_id='silver_to_gold',
    bash_command='cd /opt/reciping && python silver_to_gold_processor.py',
    dag=dag
)

# ëŒ€ì‹œë³´ë“œ ìƒˆë¡œê³ ì¹¨ í•¨ìˆ˜
def refresh_superset_dashboard():
    """Superset ëŒ€ì‹œë³´ë“œ ìºì‹œ ìƒˆë¡œê³ ì¹¨"""
    import requests
    
    # Superset API ì¸ì¦
    login_data = {
        'username': 'admin',
        'password': 'admin',
        'provider': 'db'
    }
    
    # ë¡œê·¸ì¸
    session = requests.Session()
    login_response = session.post(
        'http://superset:8088/api/v1/security/login',
        json=login_data
    )
    
    if login_response.status_code == 200:
        # ëŒ€ì‹œë³´ë“œ ìƒˆë¡œê³ ì¹¨
        refresh_response = session.post(
            'http://superset:8088/api/v1/dashboard/1/refresh'
        )
        print(f"ëŒ€ì‹œë³´ë“œ ìƒˆë¡œê³ ì¹¨ ê²°ê³¼: {refresh_response.status_code}")
    
# Superset ëŒ€ì‹œë³´ë“œ ìƒˆë¡œê³ ì¹¨
refresh_dashboard = PythonOperator(
    task_id='refresh_superset_dashboard',
    python_callable=refresh_superset_dashboard,
    dag=dag
)

# ì˜ì¡´ì„± ì„¤ì •
bronze_to_silver >> silver_to_gold >> refresh_dashboard
```

## ğŸ—‚ï¸ **2. SQL íŒŒì¼ ê´€ë¦¬ êµ¬ì¡°**

### ğŸ“ **SQL Repository êµ¬ì¡°**
```
sql/
â”œâ”€â”€ base_queries/                # ê¸°ë³¸ ì¿¼ë¦¬
â”‚   â”œâ”€â”€ kpi_metrics.sql         # í•µì‹¬ KPI
â”‚   â”œâ”€â”€ hourly_activity.sql     # ì‹œê°„ëŒ€ë³„ í™œë™
â”‚   â””â”€â”€ conversion_funnel.sql   # ì „í™˜ í¼ë„
â”œâ”€â”€ dashboard_queries/          # ëŒ€ì‹œë³´ë“œë³„ ì¿¼ë¦¬
â”‚   â”œâ”€â”€ executive_dashboard.sql # ê²½ì˜ì§„ ëŒ€ì‹œë³´ë“œ
â”‚   â”œâ”€â”€ product_analytics.sql   # ìƒí’ˆ ë¶„ì„
â”‚   â””â”€â”€ user_behavior.sql       # ì‚¬ìš©ì í–‰ë™
â””â”€â”€ materialized_views/         # ì§‘ê³„ ë·°
    â”œâ”€â”€ daily_summary.sql       # ì¼ê°„ ìš”ì•½
    â””â”€â”€ weekly_trends.sql       # ì£¼ê°„ íŠ¸ë Œë“œ
```

### ğŸ” **í•µì‹¬ KPI SQL ì˜ˆì‹œ**
```sql
-- sql/base_queries/kpi_metrics.sql
-- ì¼ê°„ í•µì‹¬ ì§€í‘œ ì¿¼ë¦¬
SELECT 
    CURRENT_DATE() as report_date,
    
    -- DAU (Daily Active Users)
    COUNT(DISTINCT session_id) as dau,
    
    -- ì´ë²¤íŠ¸ ì´ ê°œìˆ˜
    COUNT(*) as total_events,
    
    -- ì „í™˜ìœ¨
    ROUND(
        SUM(CASE WHEN is_conversion THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 
        2
    ) as conversion_rate,
    
    -- í‰ê·  ì°¸ì—¬ë„
    ROUND(AVG(engagement_score), 2) as avg_engagement,
    
    -- ì„¸ì…˜ë‹¹ í‰ê·  ì´ë²¤íŠ¸
    ROUND(COUNT(*) * 1.0 / COUNT(DISTINCT session_id), 2) as events_per_session

FROM iceberg_catalog.gold_analytics.fact_user_events
WHERE time_dim_key >= CAST(DATE_FORMAT(CURRENT_DATE(), 'yyyyMMdd00') AS BIGINT)
  AND time_dim_key <= CAST(DATE_FORMAT(CURRENT_DATE(), 'yyyyMMdd23') AS BIGINT);
```

### â° **ì‹œê°„ëŒ€ë³„ í™œë™ ë¶„ì„**
```sql
-- sql/base_queries/hourly_activity.sql
-- KST ì‹œê°„ëŒ€ë³„ í™œë™ íˆíŠ¸ë§µ ë°ì´í„°
SELECT 
    (time_dim_key % 100) as hour,
    COUNT(*) as event_count,
    COUNT(DISTINCT session_id) as unique_sessions,
    ROUND(AVG(engagement_score), 2) as avg_engagement,
    
    -- ì´ë²¤íŠ¸ íƒ€ì…ë³„ ë¶„í¬
    SUM(CASE WHEN event_dim_key = 1 THEN 1 ELSE 0 END) as auth_events,
    SUM(CASE WHEN event_dim_key = 2 THEN 1 ELSE 0 END) as comment_events,
    SUM(CASE WHEN event_dim_key = 3 THEN 1 ELSE 0 END) as bookmark_events,
    SUM(CASE WHEN event_dim_key = 4 THEN 1 ELSE 0 END) as recipe_click_events,
    SUM(CASE WHEN event_dim_key = 5 THEN 1 ELSE 0 END) as search_events,
    SUM(CASE WHEN event_dim_key = 6 THEN 1 ELSE 0 END) as view_events

FROM iceberg_catalog.gold_analytics.fact_user_events
WHERE time_dim_key >= CAST(DATE_FORMAT(CURRENT_DATE() - INTERVAL 7 DAYS, 'yyyyMMdd00') AS BIGINT)
GROUP BY (time_dim_key % 100)
ORDER BY hour;
```

## ğŸ¨ **3. Superset ëŒ€ì‹œë³´ë“œ êµ¬ì„±**

### ğŸ—ï¸ **Docker Composeë¡œ Superset ì„¤ì •**
```yaml
# docker-compose.superset.yml
version: '3.8'
services:
  superset:
    image: apache/superset:latest
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_CONFIG_PATH=/app/superset_config.py
    volumes:
      - ./superset_config.py:/app/superset_config.py
      - ./sql:/app/sql  # SQL íŒŒì¼ ë§ˆìš´íŠ¸
    depends_on:
      - postgres
      
  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

### âš™ï¸ **Superset ì„¤ì •**
```python
# superset_config.py
import os

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (Spark/Hive)
SQLALCHEMY_DATABASE_URI = 'hive://localhost:9083/default'

# Spark SQL ì—°ê²° ì„¤ì •
SQLALCHEMY_BINDS = {
    'spark': 'spark://spark-master:7077',
    'hive': 'hive://localhost:9083/iceberg_catalog'
}

# ìºì‹œ ì„¤ì • (Redis)
CACHE_CONFIG = {
    'CACHE_TYPE': 'redis',
    'CACHE_DEFAULT_TIMEOUT': 300,
    'CACHE_KEY_PREFIX': 'superset_',
    'CACHE_REDIS_HOST': 'redis',
    'CACHE_REDIS_PORT': 6379,
    'CACHE_REDIS_DB': 1,
}

# ìŠ¤ì¼€ì¤„ëœ ì¿¼ë¦¬ ì„¤ì •
ENABLE_SCHEDULED_EMAIL_REPORTS = True
SCHEDULED_EMAIL_DEBUG_MODE = True
```

## ğŸ”„ **4. ìë™í™” ì›Œí¬í”Œë¡œ**

### ğŸ“ˆ **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸**
```python
# airflow/dags/superset_sync.py
def sync_sql_to_superset():
    """SQL íŒŒì¼ì„ Superset ì°¨íŠ¸ë¡œ ë™ê¸°í™”"""
    import os
    import requests
    
    sql_directory = "/opt/reciping/sql"
    
    for root, dirs, files in os.walk(sql_directory):
        for file in files:
            if file.endswith('.sql'):
                sql_path = os.path.join(root, file)
                
                with open(sql_path, 'r') as f:
                    sql_content = f.read()
                
                # Superset APIë¡œ ì°¨íŠ¸ ìƒì„±/ì—…ë°ì´íŠ¸
                chart_data = {
                    'slice_name': file.replace('.sql', ''),
                    'query': sql_content,
                    'database_id': 1,  # Hive/Spark ë°ì´í„°ë² ì´ìŠ¤ ID
                    'viz_type': 'table',  # ê¸°ë³¸ í…Œì´ë¸”
                }
                
                # API í˜¸ì¶œë¡œ ì°¨íŠ¸ ìƒì„±
                response = requests.post(
                    'http://superset:8088/api/v1/chart/',
                    json=chart_data,
                    headers={'Authorization': f'Bearer {access_token}'}
                )
                
                print(f"ì°¨íŠ¸ ìƒì„± ê²°ê³¼: {file} - {response.status_code}")

# SQL ë™ê¸°í™” íƒœìŠ¤í¬
sync_sql_task = PythonOperator(
    task_id='sync_sql_to_superset',
    python_callable=sync_sql_to_superset,
    dag=dag
)
```

## ğŸ¯ **5. ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿**

### ğŸ“Š **ê²½ì˜ì§„ ëŒ€ì‹œë³´ë“œ**
```sql
-- sql/dashboard_queries/executive_dashboard.sql
-- ê²½ì˜ì§„ìš© í•µì‹¬ ì§€í‘œ ëŒ€ì‹œë³´ë“œ
WITH daily_metrics AS (
  SELECT 
    FLOOR(time_dim_key / 100) as date,
    COUNT(DISTINCT session_id) as dau,
    COUNT(*) as total_events,
    SUM(CASE WHEN is_conversion THEN 1 ELSE 0 END) as conversions
  FROM iceberg_catalog.gold_analytics.fact_user_events
  WHERE time_dim_key >= CAST(DATE_FORMAT(CURRENT_DATE() - INTERVAL 30 DAYS, 'yyyyMMdd00') AS BIGINT)
  GROUP BY FLOOR(time_dim_key / 100)
),
growth_metrics AS (
  SELECT 
    date,
    dau,
    LAG(dau, 1) OVER (ORDER BY date) as prev_dau,
    ROUND((dau - LAG(dau, 1) OVER (ORDER BY date)) * 100.0 / LAG(dau, 1) OVER (ORDER BY date), 2) as dau_growth,
    total_events,
    conversions,
    ROUND(conversions * 100.0 / total_events, 2) as conversion_rate
  FROM daily_metrics
)
SELECT * FROM growth_metrics
ORDER BY date DESC;
```

## ğŸš€ **6. ë°°í¬ ë° ìš´ì˜**

### ğŸ³ **í†µí•© Docker Compose**
```yaml
# docker-compose.full.yml
version: '3.8'
services:
  # Spark í™˜ê²½ (ê¸°ì¡´)
  spark-dev:
    # ... ê¸°ì¡´ ì„¤ì •
  
  # Airflow
  airflow-webserver:
    image: apache/airflow:2.7.0
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./sql:/opt/airflow/sql
  
  # Superset
  superset:
    image: apache/superset:latest
    ports:
      - "8088:8088"
    volumes:
      - ./sql:/app/sql
  
  # Redis (ìºì‹œ)
  redis:
    image: redis:6.2
    ports:
      - "6379:6379"
```

### ğŸ“‹ **ì‹¤í–‰ ìˆœì„œ**
1. **ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰**: `silver_to_gold_processor.py`
2. **Airflow DAG í™œì„±í™”**: ìŠ¤ì¼€ì¤„ëœ ETL ì‹¤í–‰  
3. **Superset ëŒ€ì‹œë³´ë“œ êµ¬ì„±**: SQL íŒŒì¼ ê¸°ë°˜ ì°¨íŠ¸ ìƒì„±
4. **ìë™ ìƒˆë¡œê³ ì¹¨**: Airflow â†’ ETL â†’ Superset ì—°ê³„

ì´ êµ¬ì¡°ë¡œ **SQL íŒŒì¼ ì¤‘ì‹¬ì˜ ìœ ì§€ë³´ìˆ˜ê°€ ì‰¬ìš´ ëŒ€ì‹œë³´ë“œ**ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ¯
