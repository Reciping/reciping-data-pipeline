# iceberg_table_maintenance.py
from pyspark.sql import SparkSession
import os

def main():
    """
    Iceberg í…Œì´ë¸”ì˜ ê³ ê¸‰ ê´€ë¦¬ ë° ìœ ì§€ë³´ìˆ˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.
    - í…Œì´ë¸” ìµœì í™” (Compaction)
    - ìŠ¤ëƒ…ìƒ· ê´€ë¦¬ ë° ì •ë¦¬
    - í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì§„í™”
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['HADOOP_USER_NAME'] = 'root'
        os.environ['USER'] = 'root'
        os.environ['HOME'] = '/tmp'
        os.environ['JAVA_OPTS'] = '-Duser.name=root'
        os.environ['IVY_HOME'] = '/tmp/.ivy2'
        os.makedirs('/tmp/.ivy2', exist_ok=True)

        # -----------------------------------------------------------------------------
        # 1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„±
        # -----------------------------------------------------------------------------
        print("ğŸ”§ SparkSession for Iceberg Maintenance ìƒì„±...")
        
        spark = SparkSession.builder \
            .appName("Iceberg_Table_Maintenance") \
            .master("local[*]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.jars.ivy", "/tmp/.ivy2") \
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.3") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
            .config("spark.sql.catalog.spark_catalog.type", "hive") \
            .config("spark.sql.catalog.spark_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.spark_catalog.warehouse", "s3a://reciping-user-event-logs/warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "")) \
            .config("spark.hadoop.fs.s3a.region", "ap-northeast-2") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.path.style.access", "false") \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSession ìƒì„± ì™„ë£Œ!")

        # -----------------------------------------------------------------------------
        # 2. ğŸ“Š í…Œì´ë¸” ìƒíƒœ ë¶„ì„
        # -----------------------------------------------------------------------------
        print("\nğŸ“Š Iceberg í…Œì´ë¸” ìƒíƒœ ë¶„ì„...")
        
        # ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ì™€ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
        print("\nğŸ—ƒï¸ ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ëª©ë¡:")
        spark.sql("SHOW DATABASES").show()
        
        databases = ["bronze_db", "silver_db", "gold_db"]
        for db in databases:
            try:
                print(f"\nğŸ“ {db} í…Œì´ë¸” ëª©ë¡:")
                spark.sql(f"SHOW TABLES IN {db}").show()
            except Exception as e:
                print(f"âš ï¸ {db} ë°ì´í„°ë² ì´ìŠ¤ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

        # -----------------------------------------------------------------------------
        # 3. ğŸ”§ í…Œì´ë¸” ìµœì í™” (Compaction)
        # -----------------------------------------------------------------------------
        print("\nğŸ”§ í…Œì´ë¸” ìµœì í™” (Compaction) ìˆ˜í–‰...")
        
        # Silver í…Œì´ë¸” ìµœì í™”
        try:
            print("\nğŸ¥ˆ Silver í…Œì´ë¸” ìµœì í™”...")
            
            # í˜„ì¬ íŒŒì¼ ìƒíƒœ í™•ì¸
            print("ğŸ“‹ ìµœì í™” ì „ Silver í…Œì´ë¸” íŒŒì¼ ìƒíƒœ:")
            spark.sql("SELECT * FROM silver_db.cleaned_events.files LIMIT 10").show(truncate=False)
            
            # Compaction ìˆ˜í–‰ (ì‘ì€ íŒŒì¼ë“¤ì„ í° íŒŒì¼ë¡œ í•©ì¹˜ê¸°)
            spark.sql("CALL spark_catalog.system.rewrite_data_files('silver_db.cleaned_events')")
            print("âœ… Silver í…Œì´ë¸” Compaction ì™„ë£Œ")
            
            # ìµœì í™” í›„ ìƒíƒœ í™•ì¸
            print("ğŸ“‹ ìµœì í™” í›„ Silver í…Œì´ë¸” íŒŒì¼ ìƒíƒœ:")
            spark.sql("SELECT * FROM silver_db.cleaned_events.files LIMIT 10").show(truncate=False)
            
        except Exception as e:
            print(f"âš ï¸ Silver í…Œì´ë¸” ìµœì í™” ì‹¤íŒ¨: {e}")

        # Gold í…Œì´ë¸”ë“¤ ìµœì í™”
        gold_tables = [
            "daily_events_summary",
            "user_behavior_profiles", 
            "recipe_popularity_analysis",
            "search_trends_analysis",
            "ab_test_performance"
        ]
        
        for table in gold_tables:
            try:
                print(f"\nğŸ¥‡ {table} í…Œì´ë¸” ìµœì í™”...")
                spark.sql(f"CALL spark_catalog.system.rewrite_data_files('gold_db.{table}')")
                print(f"âœ… {table} í…Œì´ë¸” Compaction ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ {table} í…Œì´ë¸” ìµœì í™” ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 4. ğŸ—‚ï¸ ìŠ¤ëƒ…ìƒ· ê´€ë¦¬ ë° ì •ë¦¬
        # -----------------------------------------------------------------------------
        print("\nğŸ—‚ï¸ ìŠ¤ëƒ…ìƒ· ê´€ë¦¬ ë° ì •ë¦¬...")
        
        # Silver í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì •ë³´ ì¡°íšŒ
        try:
            print("\nğŸ“¸ Silver í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì´ë ¥:")
            snapshots_df = spark.sql("SELECT * FROM silver_db.cleaned_events.snapshots ORDER BY committed_at DESC")
            snapshots_df.show(10, truncate=False)
            
            # ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì •ë¦¬ (7ì¼ ì´ì „ ìŠ¤ëƒ…ìƒ· ì‚­ì œ)
            print("\nğŸ§¹ ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì •ë¦¬ (7ì¼ ì´ì „)...")
            spark.sql("CALL spark_catalog.system.expire_snapshots('silver_db.cleaned_events', INTERVAL 7 DAYS)")
            print("âœ… ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ìŠ¤ëƒ…ìƒ· ê´€ë¦¬ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 5. ğŸ“ˆ í…Œì´ë¸” ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        # -----------------------------------------------------------------------------
        print("\nğŸ“ˆ í…Œì´ë¸” ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§...")
        
        # ê° í…Œì´ë¸”ì˜ í¬ê¸° ë° íŒŒì¼ ìˆ˜ í™•ì¸
        try:
            print("\nğŸ“Š Silver í…Œì´ë¸” ì„±ëŠ¥ ì§€í‘œ:")
            
            # íŒŒì¼ ìˆ˜ ë° í¬ê¸° ì •ë³´
            files_info = spark.sql("""
                SELECT 
                    COUNT(*) as file_count,
                    SUM(file_size_in_bytes) as total_size_bytes,
                    AVG(file_size_in_bytes) as avg_file_size_bytes,
                    MIN(file_size_in_bytes) as min_file_size_bytes,
                    MAX(file_size_in_bytes) as max_file_size_bytes
                FROM silver_db.cleaned_events.files
            """)
            files_info.show()
            
            # íŒŒí‹°ì…˜ë³„ ì •ë³´
            print("\nğŸ“ Silver í…Œì´ë¸” íŒŒí‹°ì…˜ ì •ë³´:")
            partition_info = spark.sql("""
                SELECT 
                    partition,
                    COUNT(*) as file_count,
                    SUM(file_size_in_bytes) as partition_size_bytes
                FROM silver_db.cleaned_events.files
                GROUP BY partition
                ORDER BY partition_size_bytes DESC
            """)
            partition_info.show(20, truncate=False)
            
        except Exception as e:
            print(f"âš ï¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 6. ğŸ”„ ìŠ¤í‚¤ë§ˆ ì§„í™” ì˜ˆì œ
        # -----------------------------------------------------------------------------
        print("\nğŸ”„ ìŠ¤í‚¤ë§ˆ ì§„í™” ê¸°ëŠ¥ ë°ëª¨...")
        
        try:
            # í˜„ì¬ Silver í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸
            print("\nğŸ“‹ í˜„ì¬ Silver í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ:")
            spark.sql("DESCRIBE silver_db.cleaned_events").show(50, truncate=False)
            
            # ìŠ¤í‚¤ë§ˆ ì§„í™” ì˜ˆì œ: ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€ (ë°ëª¨ìš©)
            print("\nğŸ†• ìŠ¤í‚¤ë§ˆ ì§„í™” ì˜ˆì œ: ìƒˆ ì»¬ëŸ¼ ì¶”ê°€ ì‹œë®¬ë ˆì´ì…˜")
            print("ğŸ’¡ ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆë¥¼ ì•ˆì „í•˜ê²Œ ì§„í™”ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            print("   - ALTER TABLE ADD COLUMN")
            print("   - ìƒˆë¡œìš´ ì»¬ëŸ¼ì€ ê¸°ì¡´ ë°ì´í„°ì— ëŒ€í•´ NULL ê°’ì„ ê°€ì§")
            print("   - í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥")
            
            # ì˜ˆì œ ìŠ¤í‚¤ë§ˆ ì§„í™” ì¿¼ë¦¬ (ì‹¤í–‰í•˜ì§€ ì•Šê³  ì¶œë ¥ë§Œ)
            schema_evolution_example = """
            -- ì˜ˆì œ: ìƒˆë¡œìš´ ì¶”ì  ì»¬ëŸ¼ ì¶”ê°€
            ALTER TABLE silver_db.cleaned_events 
            ADD COLUMN data_quality_score DOUBLE COMMENT 'Data quality score (0.0-1.0)';
            
            -- ì˜ˆì œ: ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€
            ALTER TABLE silver_db.cleaned_events 
            ADD COLUMN data_lineage_id STRING COMMENT 'Data lineage tracking ID';
            """
            print(f"ì˜ˆì œ ì¿¼ë¦¬:\n{schema_evolution_example}")
            
        except Exception as e:
            print(f"âš ï¸ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 7. ğŸš€ ê³ ê¸‰ Iceberg ê¸°ëŠ¥ í™œìš©
        # -----------------------------------------------------------------------------
        print("\nğŸš€ ê³ ê¸‰ Iceberg ê¸°ëŠ¥ í™œìš©...")
        
        try:
            # Time Travel ì¿¼ë¦¬ ì˜ˆì œ
            print("\nâ° Time Travel ê¸°ëŠ¥ ë°ëª¨:")
            
            # ìµœì‹  ìŠ¤ëƒ…ìƒ· ID ì¡°íšŒ
            latest_snapshot = spark.sql("SELECT snapshot_id FROM silver_db.cleaned_events.snapshots ORDER BY committed_at DESC LIMIT 1").collect()
            if latest_snapshot:
                snapshot_id = latest_snapshot[0]['snapshot_id']
                print(f"ğŸ“¸ ìµœì‹  ìŠ¤ëƒ…ìƒ· ID: {snapshot_id}")
                
                # Time Travel ì¿¼ë¦¬ ì˜ˆì œ (íŠ¹ì • ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ)
                time_travel_query = f"""
                -- íŠ¹ì • ìŠ¤ëƒ…ìƒ·ì˜ ë°ì´í„° ì¡°íšŒ
                SELECT COUNT(*) as record_count 
                FROM silver_db.cleaned_events 
                VERSION AS OF {snapshot_id}
                """
                print(f"Time Travel ì¿¼ë¦¬ ì˜ˆì œ:\n{time_travel_query}")
                
                # ì‹¤ì œ ì‹¤í–‰
                result = spark.sql(time_travel_query)
                result.show()
            
        except Exception as e:
            print(f"âš ï¸ Time Travel ê¸°ëŠ¥ ë°ëª¨ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 8. ğŸ“‹ ìœ ì§€ë³´ìˆ˜ ë¦¬í¬íŠ¸ ìƒì„±
        # -----------------------------------------------------------------------------
        print("\nğŸ“‹ ìœ ì§€ë³´ìˆ˜ ë¦¬í¬íŠ¸ ìƒì„±...")
        
        maintenance_report = {
            "maintenance_timestamp": spark.sql("SELECT current_timestamp()").collect()[0][0],
            "operations_performed": [
                "í…Œì´ë¸” ìµœì í™” (Compaction)",
                "ìŠ¤ëƒ…ìƒ· ì •ë¦¬",
                "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§",
                "ìŠ¤í‚¤ë§ˆ ì§„í™” ê²€í† "
            ],
            "recommendations": [
                "ì •ê¸°ì ì¸ Compaction ìˆ˜í–‰ (ì£¼ 1íšŒ)",
                "ìŠ¤ëƒ…ìƒ· ì •ë¦¬ ìë™í™” (ì¼ 1íšŒ)",
                "ì„±ëŠ¥ ì§€í‘œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•",
                "ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì‹œ ì˜í–¥ë„ ë¶„ì„ ìˆ˜í–‰"
            ]
        }
        
        print(f"\nğŸ“Š ìœ ì§€ë³´ìˆ˜ ì™„ë£Œ ì‹œì : {maintenance_report['maintenance_timestamp']}")
        print(f"ğŸ”§ ìˆ˜í–‰ëœ ì‘ì—…:")
        for op in maintenance_report['operations_performed']:
            print(f"   âœ… {op}")
        
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in maintenance_report['recommendations']:
            print(f"   ğŸ“ {rec}")

        # -----------------------------------------------------------------------------
        # 9. ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        spark.stop()
        print("\nâœ… Iceberg í…Œì´ë¸” ìœ ì§€ë³´ìˆ˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"âŒ ìœ ì§€ë³´ìˆ˜ ì‘ì—… ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        try:
            spark.stop()
        except:
            pass

if __name__ == "__main__":
    main()
