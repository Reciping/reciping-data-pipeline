#!/usr/bin/env python3
"""
ğŸ§Š Staging to Bronze Iceberg ETL Pipeline (Airflow-triggered, Flexible Time Parsing)
====================================================================================
S3 Staging Areaì˜ Raw JSONL íŒŒì¼ë“¤ì„ Bronze Iceberg í…Œì´ë¸”ë¡œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
Airflowë¡œë¶€í„° ì‹¤í–‰ ì‹œê°„(execution_ts)ì„ ì¸ìë¡œ ë°›ì•„ í•´ë‹¹ ì‹œê°„ëŒ€ì˜ íŒŒì¼ë§Œ ì²˜ë¦¬í•˜ë©°,
ìë™ ìŠ¤ì¼€ì¤„(ISO í˜•ì‹)ê³¼ ìˆ˜ë™ í…ŒìŠ¤íŠ¸(ê°„í¸ í˜•ì‹) ì‹œê°„ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
"""
import logging
import argparse
from datetime import datetime
import pytz
from dateutil.parser import isoparse  # ìƒë‹¨ìœ¼ë¡œ ì´ë™

from pyspark.sql import SparkSession
from pyspark.sql.functions import input_file_name, current_timestamp, to_date, lit

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StagingToBronzeETL:
    def __init__(self, test_mode: bool = True):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        self.hive_metastore_uri = "thrift://10.0.11.86:9083" # ìì‹ ì˜ Hive Metastore URIë¡œ ë³€ê²½

        if test_mode:
            self.database_name = "recipe_analytics_test"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/test_warehouse/"
            self.s3_staging_area = "s3a://reciping-user-event-logs/bronze/landing-zone/events/staging-area/"
            self.table_suffix = "_test"
        else:
            self.database_name = "recipe_analytics"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/warehouse/"
            self.s3_staging_area = "s3a://reciping-user-event-logs/bronze/landing-zone/events/staging-area/"
            self.table_suffix = ""

        # --- ë³€ê²½ì  1: í…Œì´ë¸” ì „ì²´ ì´ë¦„ì„ ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ì§€ ì•Šê³ , ê° ë¶€ë¶„ë§Œ ê´€ë¦¬ ---
        self.bronze_table_simple_name = f"bronze_events_iceberg{self.table_suffix}"
        
        print(f"ì¹´íƒˆë¡œê·¸: {self.catalog_name}")
        print(f"ë°ì´í„°ë² ì´ìŠ¤: {self.database_name}")
        print(f"Staging ê²½ë¡œ: {self.s3_staging_area}")
        print(f"Bronze í…Œì´ë¸”: {self.bronze_table_simple_name}")

    def create_spark_session(self):
        print("SparkSession ìƒì„± ì¤‘...")
        self.spark = SparkSession.builder \
            .appName("StagingToBronzeETL") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", self.hive_metastore_uri) \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", self.s3_warehouse_path) \
            .getOrCreate()
        self.spark.sparkContext.setLogLevel("WARN")

        # --- ë³€ê²½ì  2: ì‚¬ìš©í•  ì¹´íƒˆë¡œê·¸ì™€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ Spark ì„¸ì…˜ì— ëª…ì‹œì ìœ¼ë¡œ ì§€ì • ---
        print(f"í˜„ì¬ ì¹´íƒˆë¡œê·¸ë¥¼ '{self.catalog_name}'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"USE {self.catalog_name}")
        print(f"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ '{self.database_name}'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.database_name}")
        self.spark.sql(f"USE {self.database_name}")
        
        print("SparkSession ìƒì„± ë° ì„¤ì • ì™„ë£Œ")

    def create_bronze_table_if_not_exists(self):
        # --- ë³€ê²½ì  3: í…Œì´ë¸” ì´ë¦„ì— ë” ì´ìƒ ì¹´íƒˆë¡œê·¸ì™€ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ì„ í¬í•¨í•˜ì§€ ì•ŠìŒ ---
        print(f"Bronze Iceberg í…Œì´ë¸” ìƒì„± í™•ì¸: {self.bronze_table_simple_name}")
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.bronze_table_simple_name} (
            raw_event_string STRING,
            source_file STRING,
            ingestion_timestamp TIMESTAMP,
            ingestion_date DATE
        ) USING ICEBERG PARTITIONED BY (ingestion_date)
        """
        self.spark.sql(create_table_sql)
        print("Bronze Iceberg í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ")

    def run_pipeline(self, execution_ts: str):
        try:
            print(f"Staging to Bronze ETL íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ì…ë ¥ëœ ì‹œê°„: {execution_ts})")
            
            self.create_spark_session()
            self.create_bronze_table_if_not_exists()

            # --- ì´ ë¶€ë¶„ì´ ìµœì¢… ìˆ˜ì •ëœ ì‹œê°„ íŒŒì‹± ë¡œì§ì…ë‹ˆë‹¤ ---
            kst_tz = pytz.timezone('Asia/Seoul')
            kst_dt = None
            
            # 1. ìˆ˜ë™ ì‹¤í–‰ì„ ìœ„í•œ ê°„í¸ í˜•ì‹ ('YYYY-MM-DD HH:MM')ìœ¼ë¡œ ë¨¼ì € íŒŒì‹± ì‹œë„
            try:
                dt_obj = datetime.strptime(execution_ts, '%Y-%m-%d %H:%M')
                kst_dt = kst_tz.localize(dt_obj)
                print(f"ì…ë ¥ê°’ì„ ê°„í¸ í˜•ì‹(KST)ìœ¼ë¡œ ì¸ì‹: {kst_dt}")
            except ValueError:
                # 2. ê°„í¸ í˜•ì‹ ì‹¤íŒ¨ ì‹œ, ìë™ ìŠ¤ì¼€ì¤„ì„ ìœ„í•œ ISO í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„
                print("ê°„í¸ í˜•ì‹ íŒŒì‹± ì‹¤íŒ¨. ISO í˜•ì‹(UTC)ìœ¼ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                utc_dt = isoparse(execution_ts)
                kst_dt = utc_dt.astimezone(kst_tz)
                print(f"ì…ë ¥ê°’ì„ ISO í˜•ì‹ìœ¼ë¡œ ì¸ì‹ í›„ KSTë¡œ ë³€í™˜: {kst_dt}")

            if kst_dt is None:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹œê°„ í˜•ì‹ì…ë‹ˆë‹¤: {execution_ts}")
            # --- ë¡œì§ ìˆ˜ì • ë ---

            # KST ê¸°ì¤€ ì‹œê°„ì— ë§ëŠ” íŒŒì¼ëª… ë™ì  ìƒì„±
            target_filename = f"events_{kst_dt.strftime('%Y%m%d%H%M')}.jsonl"
            specific_file_path = f"{self.s3_staging_area}{target_filename}"
            
            print(f"ì²˜ë¦¬í•  ëŒ€ìƒ íŒŒì¼ ê²½ë¡œ: {specific_file_path}")
            try:
                raw_df = self.spark.read.text(specific_file_path)
                if raw_df.rdd.isEmpty():
                    print(f"íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {specific_file_path}. ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    return
            except Exception:
                print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {specific_file_path}. ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return

            # Bronze ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜
            # --- ë³€ê²½ì  ì‹œì‘ ---
            # íŒŒí‹°ì…˜ ë‚ ì§œë¥¼ í˜„ì¬ ì‹œê°„ì´ ì•„ë‹Œ, ì²˜ë¦¬ ëŒ€ìƒ ì‹œê°„(kst_dt) ê¸°ì¤€ìœ¼ë¡œ ìƒì„±
            target_date_str = kst_dt.strftime('%Y-%m-%d')
            
            bronze_df = raw_df.withColumnRenamed("value", "raw_event_string") \
                .withColumn("source_file", input_file_name()) \
                .withColumn("ingestion_timestamp", current_timestamp()) \
                .withColumn("ingestion_date", to_date(lit(target_date_str))) # <-- ì´ ë¶€ë¶„ì´ í•µì‹¬ì…ë‹ˆë‹¤.
            # --- ë³€ê²½ì  ë ---
            
            # Bronze Iceberg í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€
            print(f"Bronze Iceberg í…Œì´ë¸”ì˜ '{target_date_str}' íŒŒí‹°ì…˜ì— ë°ì´í„° ì €ì¥...")
            bronze_df.writeTo(self.bronze_table_simple_name).append()
            
            print("ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨", exc_info=True)
            raise
        finally:
            if self.spark:
                print("SparkSession ì¢…ë£Œ")
                self.spark.stop()

def main():
    # ... (ì´ì „ê³¼ ë™ì¼í•œ main í•¨ìˆ˜) ...
    parser = argparse.ArgumentParser()
    parser.add_argument("--execution-ts", required=True, help="Airflow execution timestamp (ISO 8601 or 'YYYY-MM-DD HH:MM')")
    parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True, help="Run in test mode (true/false)")
    args = parser.parse_args()

    pipeline = StagingToBronzeETL(test_mode=args.test_mode)
    pipeline.run_pipeline(execution_ts=args.execution_ts)


if __name__ == "__main__":
    main()