# streaming_to_iceberg.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import os

def main():
    """
    ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ Iceberg í…Œì´ë¸”ë¡œ ì²˜ë¦¬í•˜ëŠ” 
    ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ (Structured Streaming + Iceberg).
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['HADOOP_USER_NAME'] = 'root'
        os.environ['USER'] = 'root'
        os.environ['HOME'] = '/tmp'
        os.environ['JAVA_OPTS'] = '-Duser.name=root'
        os.environ['IVY_HOME'] = '/tmp/.ivy2'
        os.makedirs('/tmp/.ivy2', exist_ok=True)

        # -----------------------------------------------------------------------------
        # 1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° + Iceberg ì„¤ì •)
        # -----------------------------------------------------------------------------
        print("ğŸ”§ SparkSession for Streaming + Iceberg ìƒì„±...")
        
        spark = SparkSession.builder \
            .appName("Streaming_to_Iceberg_Pipeline") \
            .master("local[*]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.streaming.checkpointLocation", "/tmp/spark-streaming-checkpoints") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.jars.ivy", "/tmp/.ivy2") \
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.3") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
            .config("spark.sql.catalog.spark_catalog.type", "hive") \
            .config("spark.sql.catalog.spark_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.spark_catalog.warehouse", "s3a://reciping-user-event-logs/warehouse") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "")) \
            .config("spark.hadoop.fs.s3a.region", "ap-northeast-2") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.path.style.access", "false") \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSession for Streaming ìƒì„± ì™„ë£Œ!")

        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("/tmp/spark-streaming-checkpoints", exist_ok=True)

        # -----------------------------------------------------------------------------
        # 2. ğŸ—ï¸ ì‹¤ì‹œê°„ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¤€ë¹„
        # -----------------------------------------------------------------------------
        print("\nğŸ—ï¸ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¤€ë¹„...")
        
        # ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ì •ì˜
        streaming_event_schema = StructType([
            StructField("event_id", StringType(), False),
            StructField("event_name", StringType(), False),
            StructField("user_id", StringType(), True),
            StructField("anonymous_id", StringType(), True),
            StructField("session_id", StringType(), True),
            StructField("timestamp", StringType(), False),
            StructField("date", StringType(), False),
            StructField("context", StringType(), True),
            StructField("event_properties", StringType(), True)
        ])
        
        # ì‹¤ì‹œê°„ ì§‘ê³„ í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
        try:
            # ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ í…Œì´ë¸”
            spark.sql("""
                CREATE TABLE IF NOT EXISTS bronze_db.streaming_events (
                    event_id STRING NOT NULL,
                    event_name STRING NOT NULL,
                    user_id STRING,
                    anonymous_id STRING,
                    session_id STRING,
                    event_timestamp TIMESTAMP NOT NULL,
                    event_date DATE NOT NULL,
                    context STRING,
                    event_properties STRING,
                    ingestion_timestamp TIMESTAMP NOT NULL,
                    year INT NOT NULL,
                    month INT NOT NULL,
                    day INT NOT NULL,
                    hour INT NOT NULL
                ) USING ICEBERG
                PARTITIONED BY (year, month, day, hour)
                TBLPROPERTIES (
                    'format-version' = '2',
                    'write.target-file-size-bytes' = '134217728'
                )
            """)
            print("âœ… ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ í…Œì´ë¸” 'bronze_db.streaming_events' ì¤€ë¹„ ì™„ë£Œ")
            
            # ì‹¤ì‹œê°„ ì§‘ê³„ í…Œì´ë¸”
            spark.sql("""
                CREATE TABLE IF NOT EXISTS gold_db.realtime_metrics (
                    window_start TIMESTAMP NOT NULL,
                    window_end TIMESTAMP NOT NULL,
                    event_name STRING NOT NULL,
                    event_count BIGINT NOT NULL,
                    unique_users BIGINT NOT NULL,
                    unique_sessions BIGINT NOT NULL,
                    processing_timestamp TIMESTAMP NOT NULL,
                    year INT NOT NULL,
                    month INT NOT NULL,
                    day INT NOT NULL,
                    hour INT NOT NULL
                ) USING ICEBERG
                PARTITIONED BY (year, month, day)
                TBLPROPERTIES (
                    'format-version' = '2',
                    'write.target-file-size-bytes' = '67108864'
                )
            """)
            print("âœ… ì‹¤ì‹œê°„ ì§‘ê³„ í…Œì´ë¸” 'gold_db.realtime_metrics' ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}")

        # -----------------------------------------------------------------------------
        # 3. ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì†ŒìŠ¤ ì‹œë®¬ë ˆì´ì…˜
        # -----------------------------------------------------------------------------
        print("\nğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •...")
        
        # íŒŒì¼ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì†ŒìŠ¤ (S3 ëœë”© ì¡´ ëª¨ë‹ˆí„°ë§)
        streaming_path = "s3a://reciping-user-event-logs/bronze/streaming-zone/events/"
        
        # ìŠ¤íŠ¸ë¦¬ë° DataFrame ìƒì„±
        streaming_df = spark.readStream \
            .format("json") \
            .schema(streaming_event_schema) \
            .option("path", streaming_path) \
            .option("maxFilesPerTrigger", 1) \
            .load()
        
        print(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì†ŒìŠ¤ ì„¤ì • ì™„ë£Œ: {streaming_path}")

        # -----------------------------------------------------------------------------
        # 4. ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ë³€í™˜
        # -----------------------------------------------------------------------------
        print("\nğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ë³€í™˜ ë¡œì§ ì„¤ì •...")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ë° íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì¶”ê°€
        streaming_transformed = streaming_df \
            .withColumn("event_timestamp", to_timestamp(col("timestamp"))) \
            .withColumn("event_date", col("date").cast(DateType())) \
            .withColumn("ingestion_timestamp", current_timestamp()) \
            .withColumn("year", year(col("event_timestamp"))) \
            .withColumn("month", month(col("event_timestamp"))) \
            .withColumn("day", dayofmonth(col("event_timestamp"))) \
            .withColumn("hour", hour(col("event_timestamp"))) \
            .filter(col("event_timestamp").isNotNull()) \
            .drop("timestamp", "date")
        
        print("âœ… ìŠ¤íŠ¸ë¦¬ë° ë³€í™˜ ë¡œì§ ì„¤ì • ì™„ë£Œ")

        # -----------------------------------------------------------------------------
        # 5. ğŸ’¾ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ Iceberg í…Œì´ë¸”ë¡œ ì €ì¥
        # -----------------------------------------------------------------------------
        print("\nğŸ’¾ ì‹¤ì‹œê°„ Iceberg ì €ì¥ ìŠ¤íŠ¸ë¦¼ ì„¤ì •...")
        
        # Iceberg í…Œì´ë¸”ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì €ì¥
        streaming_to_iceberg = streaming_transformed.writeStream \
            .format("iceberg") \
            .outputMode("append") \
            .option("table", "bronze_db.streaming_events") \
            .option("checkpointLocation", "/tmp/spark-streaming-checkpoints/streaming_events") \
            .trigger(processingTime="30 seconds") \
            .start()
        
        print("âœ… ì‹¤ì‹œê°„ Iceberg ì €ì¥ ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨")

        # -----------------------------------------------------------------------------
        # 6. ğŸ“Š ì‹¤ì‹œê°„ ì§‘ê³„ ì²˜ë¦¬
        # -----------------------------------------------------------------------------
        print("\nğŸ“Š ì‹¤ì‹œê°„ ì§‘ê³„ ìŠ¤íŠ¸ë¦¼ ì„¤ì •...")
        
        # 5ë¶„ ìœˆë„ìš° ì§‘ê³„
        windowed_aggregation = streaming_transformed \
            .withWatermark("event_timestamp", "5 minutes") \
            .groupBy(
                window(col("event_timestamp"), "5 minutes"),
                "event_name"
            ).agg(
                count("*").alias("event_count"),
                countDistinct("user_id").alias("unique_users"),
                countDistinct("session_id").alias("unique_sessions")
            ).select(
                col("window.start").alias("window_start"),
                col("window.end").alias("window_end"),
                "event_name",
                "event_count",
                "unique_users", 
                "unique_sessions",
                current_timestamp().alias("processing_timestamp"),
                year(col("window.start")).alias("year"),
                month(col("window.start")).alias("month"),
                dayofmonth(col("window.start")).alias("day"),
                hour(col("window.start")).alias("hour")
            )
        
        # ì‹¤ì‹œê°„ ì§‘ê³„ë¥¼ Iceberg í…Œì´ë¸”ë¡œ ì €ì¥
        aggregation_to_iceberg = windowed_aggregation.writeStream \
            .format("iceberg") \
            .outputMode("append") \
            .option("table", "gold_db.realtime_metrics") \
            .option("checkpointLocation", "/tmp/spark-streaming-checkpoints/realtime_metrics") \
            .trigger(processingTime="60 seconds") \
            .start()
        
        print("âœ… ì‹¤ì‹œê°„ ì§‘ê³„ ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨")

        # -----------------------------------------------------------------------------
        # 7. ğŸ–¥ï¸ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë‹ˆí„°ë§ ë° ì½˜ì†” ì¶œë ¥
        # -----------------------------------------------------------------------------
        print("\nğŸ–¥ï¸ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë‹ˆí„°ë§ ì„¤ì •...")
        
        # ì½˜ì†”ë¡œ ì‹¤ì‹œê°„ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)
        console_output = streaming_transformed.writeStream \
            .format("console") \
            .outputMode("append") \
            .option("truncate", "false") \
            .option("numRows", 5) \
            .trigger(processingTime="30 seconds") \
            .start()
        
        print("âœ… ì½˜ì†” ëª¨ë‹ˆí„°ë§ ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨")

        # -----------------------------------------------------------------------------
        # 8. ğŸ“ˆ ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§
        # -----------------------------------------------------------------------------
        print("\nğŸ“ˆ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
        print("ğŸ’¡ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ“¡ ëª¨ë‹ˆí„°ë§ ì¤‘ì¸ ê²½ë¡œ:", streaming_path)
        print("ğŸ”„ ì²˜ë¦¬ ì£¼ê¸°: 30ì´ˆ (ë°ì´í„° ì €ì¥), 60ì´ˆ (ì§‘ê³„)")
        print("â¹ï¸  ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        
        try:
            # ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì²´í¬ ë£¨í”„
            import time
            runtime_minutes = 0
            max_runtime_minutes = 10  # ìµœëŒ€ 10ë¶„ ì‹¤í–‰
            
            while runtime_minutes < max_runtime_minutes:
                time.sleep(60)  # 1ë¶„ ëŒ€ê¸°
                runtime_minutes += 1
                
                print(f"\nâ° ì‹¤í–‰ ì‹œê°„: {runtime_minutes}ë¶„")
                
                # ìŠ¤íŠ¸ë¦¼ ìƒíƒœ í™•ì¸
                if streaming_to_iceberg.isActive:
                    progress = streaming_to_iceberg.lastProgress
                    if progress:
                        print(f"ğŸ“Š Iceberg ì €ì¥ ìŠ¤íŠ¸ë¦¼ - ì²˜ë¦¬ëœ í–‰: {progress.get('inputRowsPerSecond', 0)}/ì´ˆ")
                
                if aggregation_to_iceberg.isActive:
                    agg_progress = aggregation_to_iceberg.lastProgress
                    if agg_progress:
                        print(f"ğŸ”¢ ì§‘ê³„ ìŠ¤íŠ¸ë¦¼ - ì²˜ë¦¬ëœ í–‰: {agg_progress.get('inputRowsPerSecond', 0)}/ì´ˆ")
                
                # ì‹¤ì œ ë°ì´í„°ê°€ ë“¤ì–´ì™”ëŠ”ì§€ í™•ì¸
                try:
                    current_count = spark.sql("SELECT COUNT(*) as cnt FROM bronze_db.streaming_events").collect()[0]['cnt']
                    print(f"ğŸ’¾ í˜„ì¬ ì €ì¥ëœ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ìˆ˜: {current_count:,}")
                    
                    metrics_count = spark.sql("SELECT COUNT(*) as cnt FROM gold_db.realtime_metrics").collect()[0]['cnt']
                    print(f"ğŸ“Š í˜„ì¬ ì €ì¥ëœ ì§‘ê³„ ë©”íŠ¸ë¦­ ìˆ˜: {metrics_count:,}")
                except:
                    print("ğŸ“Š í…Œì´ë¸” ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (í…Œì´ë¸”ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ)")
            
            print(f"\nâœ… {max_runtime_minutes}ë¶„ ì‹¤í–‰ ì™„ë£Œ. ìŠ¤íŠ¸ë¦¼ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # -----------------------------------------------------------------------------
        # 9. ìŠ¤íŠ¸ë¦¼ ì •ë¦¬ ë° ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        print("\nğŸ›‘ ìŠ¤íŠ¸ë¦¬ë° ì‘ì—… ì¢…ë£Œ ì¤‘...")
        
        # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
        streaming_to_iceberg.stop()
        aggregation_to_iceberg.stop()
        console_output.stop()
        
        print("âœ… ëª¨ë“  ìŠ¤íŠ¸ë¦¼ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìµœì¢… ê²°ê³¼ í™•ì¸
        try:
            print("\nğŸ“Š ìµœì¢… ì²˜ë¦¬ ê²°ê³¼:")
            
            final_events = spark.sql("SELECT COUNT(*) as total_events FROM bronze_db.streaming_events").collect()[0]['total_events']
            print(f"ğŸ’¾ ì´ ì²˜ë¦¬ëœ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸: {final_events:,}")
            
            final_metrics = spark.sql("SELECT COUNT(*) as total_metrics FROM gold_db.realtime_metrics").collect()[0]['total_metrics']
            print(f"ğŸ“Š ì´ ìƒì„±ëœ ì§‘ê³„ ë©”íŠ¸ë¦­: {final_metrics:,}")
            
            if final_metrics > 0:
                print("\nğŸ“ˆ ìµœê·¼ ì§‘ê³„ ê²°ê³¼ (ìƒìœ„ 5ê°œ):")
                spark.sql("""
                    SELECT window_start, window_end, event_name, event_count, unique_users
                    FROM gold_db.realtime_metrics 
                    ORDER BY window_start DESC 
                    LIMIT 5
                """).show()
            
        except Exception as e:
            print(f"âš ï¸ ìµœì¢… ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 10. ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        spark.stop()
        print("âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        try:
            spark.stop()
        except:
            pass

if __name__ == "__main__":
    main()
