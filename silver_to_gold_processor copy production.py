#!/usr/bin/env python3
"""
ğŸ§Š Silver to Gold ETL Processor (Incremental, Airflow-triggered)
================================================================
Silver Iceberg í…Œì´ë¸”ì—ì„œ íŠ¹ì • íŒŒí‹°ì…˜ì˜ ë°ì´í„°ë¥¼ ì½ì–´ Gold Fact í…Œì´ë¸”ë¡œ ë³€í™˜/ì§‘ê³„í•©ë‹ˆë‹¤.
Airflowë¡œë¶€í„° ì‹¤í–‰ ì‹œê°„ì„ ë°›ì•„ ì ì§„ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import logging
import argparse
from datetime import datetime
import pytz
from dateutil.parser import isoparse

from pyspark.sql import SparkSession
from pyspark.sql.functions import when, size, split, coalesce, cast, col, lit, date_format

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SilverToGoldProcessor:
    def __init__(self, test_mode: bool = True):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        
        if test_mode:
            print("=== í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
            self.silver_database = "recipe_analytics_test"
            self.gold_database = "recipe_analytics_test" # Goldë„ í…ŒìŠ¤íŠ¸ DB ì‚¬ìš©
            self.table_suffix = "_test"
        else:
            print("=== ìš´ì˜ ëª¨ë“œë¡œ ì‹¤í–‰ ===")
            self.silver_database = "recipe_analytics"
            self.gold_database = "gold_analytics"
            self.table_suffix = ""
            
        # self.silver_table_name = f"{self.catalog_name}.{self.silver_database}.user_events_silver{self.table_suffix}"
        # self.gold_table_name = f"{self.catalog_name}.{self.gold_database}.fact_user_events{self.table_suffix}"
        self.silver_table_name = f"user_events_silver{self.table_suffix}" # <--- ë‹¨ìˆœ ì´ë¦„ìœ¼ë¡œ ë³€ê²½
        self.gold_table_name = f"fact_user_events{self.table_suffix}" # <--- ë‹¨ìˆœ ì´ë¦„ìœ¼ë¡œ ë³€ê²½

    def create_spark_session(self):
        print("SparkSession ìƒì„± ì¤‘...")
        self.spark = SparkSession.builder \
            .appName("SilverToGold_ETL") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://10.0.11.86:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .getOrCreate()
        self.spark.sparkContext.setLogLevel("WARN")

        # --- ì´ ë¶€ë¶„ì´ í•µì‹¬ì ì¸ ìˆ˜ì •ì…ë‹ˆë‹¤ ---
        print(f"í˜„ì¬ ì¹´íƒˆë¡œê·¸ë¥¼ '{self.catalog_name}'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"USE {self.catalog_name}")
        
        # Gold í…Œì´ë¸”ì€ gold_databaseì—, Silver í…Œì´ë¸”ì€ silver_databaseì— ìˆìŠµë‹ˆë‹¤.
        # ë”°ë¼ì„œ Gold í…Œì´ë¸”ì„ ë§Œë“¤ê¸° ì „ì— target databaseë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
        print(f"ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ '{self.gold_database}' ìƒì„± ë° ì‚¬ìš© ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.gold_database}")
        self.spark.sql(f"USE {self.gold_database}")
        # --- ìˆ˜ì • ë ---

        print("SparkSession ìƒì„± ì™„ë£Œ")

    # --- [ì‹ ê·œ ì¶”ê°€] Gold í…Œì´ë¸” ìƒì„± í•¨ìˆ˜ ---
    def create_gold_table_if_not_exists(self):
        """Gold Fact í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"Gold Fact í…Œì´ë¸” ìƒì„± í™•ì¸: {self.gold_table_name}")
        # Silver í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ Gold í…Œì´ë¸” DDL ì‘ì„±
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.gold_table_name} (
            event_id STRING,
            user_dim_key BIGINT,
            time_dim_key BIGINT,
            recipe_dim_key BIGINT,
            page_dim_key BIGINT,
            event_dim_key INT,
            event_count INT,
            session_duration_seconds BIGINT,
            page_view_duration_seconds BIGINT,
            is_conversion BOOLEAN,
            conversion_value DOUBLE,
            engagement_score DOUBLE,
            session_id STRING,
            anonymous_id STRING,
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        )
        USING ICEBERG
        PARTITIONED BY (days(created_at)) -- Gold í…Œì´ë¸”ì€ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ íŒŒí‹°ì…”ë‹
        """
        self.spark.sql(create_table_sql)
        print("Gold Fact í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ")

    def transform_and_load_gold_data(self, target_date: str):
        """Silver ë° Dimension í…Œì´ë¸”ë“¤ì„ ì¡°ì¸í•˜ì—¬ Gold Fact í…Œì´ë¸”ë¡œ ë³€í™˜ ë° ì ì¬í•©ë‹ˆë‹¤."""
        
        print(f"Silver to Gold ì²˜ë¦¬ ì‹œì‘ (ëŒ€ìƒ ë‚ ì§œ: {target_date})")
        
        try:
            # --- [ë³€ê²½ì ] ì½ì–´ì˜¬ í…Œì´ë¸”ë“¤ì˜ ì „ì²´ ì´ë¦„(DB.TABLE)ì„ ëª…ì‹œì ìœ¼ë¡œ ìƒì„± ---
            full_silver_table = f"{self.silver_database}.{self.silver_table_name}"
            dim_user_table = f"{self.gold_database}.dim_user{self.table_suffix}"
            dim_recipe_table = f"{self.gold_database}.dim_recipe{self.table_suffix}"
            dim_event_table = f"{self.gold_database}.dim_event{self.table_suffix}"
            dim_page_table = f"{self.gold_database}.dim_page{self.table_suffix}"

            # 1. Silver í…Œì´ë¸”ì—ì„œ í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„° ì½ê¸° (ì „ì²´ ì´ë¦„ ì‚¬ìš©)
            silver_df = self.spark.read.table(full_silver_table).where(f"date = '{target_date}'")
            
            silver_count = silver_df.count()
            if silver_count == 0:
                print(f"{target_date} ë‚ ì§œì˜ Silver ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return

            print(f"{target_date} ë‚ ì§œì˜ Silver ë°ì´í„° {silver_count}ê±´ì„ Gold í…Œì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")

            # 2. í•„ìš”í•œ ëª¨ë“  Dimension í…Œì´ë¸” ì½ê¸° (ì „ì²´ ì´ë¦„ ì‚¬ìš©)
            dim_user = self.spark.read.table(dim_user_table)
            dim_recipe = self.spark.read.table(dim_recipe_table)
            dim_event = self.spark.read.table(dim_event_table)
            dim_page = self.spark.read.table(dim_page_table)

            # 3. Silver ë°ì´í„°ì™€ ëª¨ë“  Dimensionì„ ìˆœì°¨ì ìœ¼ë¡œ ì¡°ì¸
            joined_df = silver_df \
                .join(dim_user, ["user_id", "anonymous_id", "user_segment", "cooking_style"], "left") \
                .join(dim_recipe, silver_df.prop_recipe_id == dim_recipe.recipe_id, "left") \
                .join(dim_event, "event_name", "left") \
                .join(dim_page, ["page_name", "page_url"], "left")

            # 4. ìµœì¢… Fact í…Œì´ë¸” í˜•íƒœ ìƒì„± (DataFrame API ì‚¬ìš©)
            # from pyspark.sql.functions import when, size, split, coalesce, lit, date_format
            
            fact_df = joined_df.select(
                col("event_id"),
                coalesce(col("user_sk"), lit(0)).alias("user_dim_key"),
                date_format(col("kst_timestamp"), "yyyyMMddHH").cast("bigint").alias("time_dim_key"),
                coalesce(col("recipe_sk"), lit(0)).alias("recipe_dim_key"),
                coalesce(col("page_sk"), lit(0)).alias("page_dim_key"),
                coalesce(col("event_sk"), lit(0)).alias("event_dim_key"),
                lit(1).alias("event_count"),
                when(col("prop_action").isNotNull() & (size(split(col("prop_action"), ":")) >= 2), 
                     coalesce(split(col("prop_action"), ":")[1].cast("bigint"), lit(60)))
                .otherwise(60).alias("session_duration_seconds"),
                lit(30).cast("bigint").alias("page_view_duration_seconds"),
                when(col("event_name").isin('auth_success', 'click_bookmark', 'create_comment'), True).otherwise(False).alias("is_conversion"),
                lit(1.0).alias("conversion_value"),
                when(col("event_name") == 'auth_success', 10.0).when(col("event_name") == 'create_comment', 9.0)
                .when(col("event_name") == 'click_bookmark', 8.0).when(col("event_name") == 'click_recipe', 7.0)
                .when(col("event_name") == 'search_recipe', 5.0).when(col("event_name") == 'view_recipe', 4.0)
                .when(col("event_name") == 'view_page', 2.0).otherwise(1.0).alias("engagement_score"),
                col("session_id"),
                col("anonymous_id"),
                col("kst_timestamp").alias("created_at"),
                col("kst_timestamp").alias("updated_at")
            )

            # 5. Gold í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€ (Append) (ì „ì²´ ì´ë¦„ ì‚¬ìš©)
            print("Gold í…Œì´ë¸” ì ì¬ ì¤‘...")
            fact_df.writeTo(f"{self.gold_database}.{self.gold_table_name}").append()
            print("Gold í…Œì´ë¸” ì ì¬ ì™„ë£Œ.")

        except Exception as e:
            logger.error("Gold ë°ì´í„° ë³€í™˜/ì ì¬ ì‹¤íŒ¨", exc_info=True)
            raise

    
    def run_pipeline(self, execution_ts: str):
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            self.create_spark_session()
            
            # Airflowê°€ ë„˜ê²¨ì¤€ ì‹¤í–‰ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬í•  ë‚ ì§œ(íŒŒí‹°ì…˜) ê²°ì •
            kst_tz = pytz.timezone('Asia/Seoul')
            try:
                dt_obj = datetime.strptime(execution_ts, '%Y-%m-%d %H:%M')
                kst_dt = kst_tz.localize(dt_obj)
            except ValueError:
                utc_dt = isoparse(execution_ts)
                kst_dt = utc_dt.astimezone(kst_tz)
            
            target_date = kst_dt.strftime('%Y-%m-%d')
            
            # --- [ì‹ ê·œ ì¶”ê°€] Gold í…Œì´ë¸” ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ ---
            self.create_gold_table_if_not_exists()
            
            # --- [ì‹ ê·œ ì¶”ê°€] Silver í…Œì´ë¸”ì˜ ìµœì‹  ë©”íƒ€ë°ì´í„° ê°•ì œ ìƒˆë¡œê³ ì¹¨ ---
            print(f"Silver í…Œì´ë¸”ì˜ ìµœì‹  ì •ë³´ ìƒˆë¡œê³ ì¹¨: {self.silver_table_name}")
            self.spark.catalog.refreshTable(self.silver_table_name)
            # -----------------------------------------------------------
            
            # ì´ì œ ìµœì‹  ìƒíƒœê°€ ë³´ì¥ëœ Silver í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì½ìŠµë‹ˆë‹¤.
            self.transform_and_load_gold_data(target_date)
            
            print("Silver to Gold ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨", exc_info=True)
            raise
        finally:
            if self.spark:
                self.spark.stop()

def main():
    parser = argparse.ArgumentParser(description="Silver to Gold Iceberg ETL Job (Incremental)")
    parser.add_argument("--execution-ts", required=True, help="Airflow execution timestamp")
    parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True, help="Run in test mode")
    args = parser.parse_args()

    processor = SilverToGoldProcessor(test_mode=args.test_mode)
    processor.run_pipeline(execution_ts=args.execution_ts)

if __name__ == "__main__":
    main()