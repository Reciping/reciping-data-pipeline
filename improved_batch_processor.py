#!/usr/bin/env python3
"""
ê°œì„ ëœ ë°°ì¹˜ ì²˜ë¦¬ê¸° - ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬
ë°©ë²• 1: ë‚ ì§œë³„ ì™„ì „ ì²˜ë¦¬ (ê¶Œì¥)
ë°©ë²• 2: ì „ì²´ ì‹œê°„ìˆœ ì²˜ë¦¬
"""

from pyspark.sql import SparkSession
import time

class ImprovedBatchProcessor:
    """ê°œì„ ëœ ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.catalog_name = "iceberg_catalog"
        self.silver_database = "recipe_analytics"
        self.gold_database = "gold_analytics"
        self.spark = None
        self.batch_size = 5000
        
    def create_spark_session(self):
        """ìµœì í™”ëœ SparkSession"""
        print("ğŸ”§ ê°œì„ ëœ ë°°ì¹˜ìš© SparkSession...")
        
        self.spark = SparkSession.builder \
            .appName("ImprovedBatch") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://metastore:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/iceberg/warehouse/") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.sql.shuffle.partitions", "50") \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("WARN")
        print("âœ… ê°œì„ ëœ SparkSession ìƒì„± ì™„ë£Œ")
        
    def method1_date_by_date_complete(self):
        """ë°©ë²• 1: ë‚ ì§œë³„ ì™„ì „ ì²˜ë¦¬ (ê¶Œì¥)"""
        print("\nğŸ—“ï¸ ë°©ë²• 1: ë‚ ì§œë³„ ì™„ì „ ì²˜ë¦¬ ì‹œì‘...")
        
        # ì²˜ë¦¬í•  ë‚ ì§œ ëª©ë¡
        dates = [
            '2025-07-01', '2025-07-02', '2025-07-03', '2025-07-04', '2025-07-05',
            '2025-07-06', '2025-07-07', '2025-07-08', '2025-07-09', '2025-07-10'
        ]
        
        total_processed = 0
        is_first_date = True
        
        for date in dates:
            print(f"\nğŸ“… {date} ì²˜ë¦¬ ì‹œì‘...")
            
            # í•´ë‹¹ ë‚ ì§œì˜ ì´ ì´ë²¤íŠ¸ ìˆ˜ í™•ì¸
            date_count = self.spark.sql(f"""
                SELECT COUNT(*) as cnt 
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                WHERE date = '{date}'
            """).collect()[0]['cnt']
            
            needed_batches = (date_count + self.batch_size - 1) // self.batch_size
            print(f"   ì´ {date_count:,}ê°œ ì´ë²¤íŠ¸ â†’ {needed_batches}ê°œ ë°°ì¹˜ í•„ìš”")
            
            # í•´ë‹¹ ë‚ ì§œë¥¼ ë°°ì¹˜ë³„ë¡œ ì™„ì „ ì²˜ë¦¬
            for batch_num in range(needed_batches):
                offset = batch_num * self.batch_size
                
                try:
                    print(f"      ë°°ì¹˜ {batch_num + 1}/{needed_batches} ì²˜ë¦¬ ì¤‘...")
                    
                    if is_first_date and batch_num == 0:
                        # ì²« ë²ˆì§¸ ë°°ì¹˜ëŠ” OVERWRITE
                        insert_mode = "INSERT OVERWRITE"
                    else:
                        # ì´í›„ ë°°ì¹˜ë“¤ì€ APPEND
                        insert_mode = "INSERT INTO"
                    
                    batch_query = f"""
                    {insert_mode} {self.catalog_name}.{self.gold_database}.fact_user_events
                    SELECT 
                        event_id,
                        0 as user_dim_key,
                        CAST(DATE_FORMAT(utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(utc_timestamp) as time_dim_key,
                        0 as recipe_dim_key,
                        0 as page_dim_key,
                        1 as event_dim_key,
                        1 as event_count,
                        60 as session_duration_seconds,
                        30 as page_view_duration_seconds,
                        CASE WHEN event_name IN ('auth_success', 'click_bookmark') THEN TRUE ELSE FALSE END as is_conversion,
                        1.0 as conversion_value,
                        2.0 as engagement_score,
                        session_id,
                        anonymous_id,
                        CURRENT_TIMESTAMP() as created_at,
                        CURRENT_TIMESTAMP() as updated_at
                    FROM (
                        SELECT 
                            event_id, session_id, anonymous_id, event_name, utc_timestamp,
                            ROW_NUMBER() OVER (ORDER BY utc_timestamp, event_id) as row_num
                        FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                        WHERE date = '{date}' AND event_id IS NOT NULL
                    ) ranked
                    WHERE row_num > {offset} AND row_num <= {offset + self.batch_size}
                    """
                    
                    self.spark.sql(batch_query)
                    
                    # í˜„ì¬ ì§„í–‰ ìƒí™©
                    current_total = self.spark.sql(f"""
                        SELECT COUNT(*) as cnt 
                        FROM {self.catalog_name}.{self.gold_database}.fact_user_events
                    """).collect()[0]['cnt']
                    
                    batch_processed = min(self.batch_size, date_count - offset)
                    total_processed += batch_processed
                    
                    print(f"         âœ… +{batch_processed:,}ê°œ (ëˆ„ì : {current_total:,}ê°œ)")
                    
                    is_first_date = False
                    time.sleep(1)  # ë©”ëª¨ë¦¬ ì•ˆì •ì„±
                    
                except Exception as e:
                    print(f"         âŒ ë°°ì¹˜ {batch_num + 1} ì‹¤íŒ¨: {str(e)[:50]}...")
                    break
            
            print(f"   âœ… {date} ì™„ë£Œ: {date_count:,}ê°œ ì²˜ë¦¬")
            
        return total_processed
        
    def method2_chronological_complete(self):
        """ë°©ë²• 2: ì „ì²´ ì‹œê°„ìˆœ ì™„ì „ ì²˜ë¦¬"""
        print("\nâ° ë°©ë²• 2: ì „ì²´ ì‹œê°„ìˆœ ì™„ì „ ì²˜ë¦¬ ì‹œì‘...")
        
        # ì „ì²´ ì´ë²¤íŠ¸ ìˆ˜ í™•ì¸
        total_events = self.spark.sql(f"""
            SELECT COUNT(*) as cnt 
            FROM {self.catalog_name}.{self.silver_database}.user_events_silver
            WHERE date >= '2025-07-01' AND date <= '2025-07-10'
        """).collect()[0]['cnt']
        
        needed_batches = (total_events + self.batch_size - 1) // self.batch_size
        print(f"   ì´ {total_events:,}ê°œ ì´ë²¤íŠ¸ â†’ {needed_batches}ê°œ ë°°ì¹˜ í•„ìš”")
        
        total_processed = 0
        
        for batch_num in range(needed_batches):
            offset = batch_num * self.batch_size
            
            try:
                print(f"   ë°°ì¹˜ {batch_num + 1}/{needed_batches} ì²˜ë¦¬ ì¤‘...")
                
                insert_mode = "INSERT OVERWRITE" if batch_num == 0 else "INSERT INTO"
                
                batch_query = f"""
                {insert_mode} {self.catalog_name}.{self.gold_database}.fact_user_events
                SELECT 
                    event_id,
                    0 as user_dim_key,
                    CAST(DATE_FORMAT(utc_timestamp, 'yyyyMMdd') AS BIGINT) * 100 + HOUR(utc_timestamp) as time_dim_key,
                    0 as recipe_dim_key,
                    0 as page_dim_key,
                    1 as event_dim_key,
                    1 as event_count,
                    60 as session_duration_seconds,
                    30 as page_view_duration_seconds,
                    CASE WHEN event_name IN ('auth_success', 'click_bookmark') THEN TRUE ELSE FALSE END as is_conversion,
                    1.0 as conversion_value,
                    2.0 as engagement_score,
                    session_id,
                    anonymous_id,
                    CURRENT_TIMESTAMP() as created_at,
                    CURRENT_TIMESTAMP() as updated_at
                FROM (
                    SELECT 
                        event_id, session_id, anonymous_id, event_name, utc_timestamp,
                        ROW_NUMBER() OVER (ORDER BY utc_timestamp, event_id) as row_num
                    FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                    WHERE date >= '2025-07-01' AND date <= '2025-07-10' 
                    AND event_id IS NOT NULL
                ) ranked
                WHERE row_num > {offset} AND row_num <= {offset + self.batch_size}
                """
                
                self.spark.sql(batch_query)
                
                batch_processed = min(self.batch_size, total_events - offset)
                total_processed += batch_processed
                
                print(f"      âœ… +{batch_processed:,}ê°œ (ëˆ„ì : {total_processed:,}ê°œ)")
                
                time.sleep(1)  # ë©”ëª¨ë¦¬ ì•ˆì •ì„±
                
            except Exception as e:
                print(f"      âŒ ë°°ì¹˜ {batch_num + 1} ì‹¤íŒ¨: {str(e)[:50]}...")
                break
                
        return total_processed
        
    def validate_complete_processing(self):
        """ì™„ì „ ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦"""
        print("\nğŸ” ì™„ì „ ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦...")
        
        try:
            # Fact í…Œì´ë¸” í†µê³„
            fact_stats = self.spark.sql(f"""
                SELECT 
                    COUNT(*) as fact_total,
                    COUNT(DISTINCT time_dim_key) as unique_time_keys,
                    COUNT(DISTINCT session_id) as unique_sessions
                FROM {self.catalog_name}.{self.gold_database}.fact_user_events
            """).collect()[0]
            
            # Silver í…Œì´ë¸” í†µê³„ (10ì¼ê°„)
            silver_stats = self.spark.sql(f"""
                SELECT 
                    COUNT(*) as silver_total
                FROM {self.catalog_name}.{self.silver_database}.user_events_silver
                WHERE date >= '2025-07-01' AND date <= '2025-07-10'
            """).collect()[0]
            
            print("ğŸ“Š ì™„ì „ ì²˜ë¦¬ ê²€ì¦ ê²°ê³¼:")
            print(f"   Silver ì›ë³¸: {silver_stats.silver_total:,}ê°œ")
            print(f"   Fact ì²˜ë¦¬: {fact_stats.fact_total:,}ê°œ")
            print(f"   ì²˜ë¦¬ìœ¨: {(fact_stats.fact_total / silver_stats.silver_total) * 100:.1f}%")
            print(f"   ê³ ìœ  ì‹œê°„í‚¤: {fact_stats.unique_time_keys:,}ê°œ")
            print(f"   ê³ ìœ  ì„¸ì…˜: {fact_stats.unique_sessions:,}ê°œ")
            
            if fact_stats.fact_total >= silver_stats.silver_total * 0.95:
                print("\nğŸ‰ ì™„ì „ ì²˜ë¦¬ ì„±ê³µ!")
                print("   âœ… 95% ì´ìƒ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
                print("   âœ… ëˆ„ë½ ì—†ëŠ” ì™„ì „í•œ ë°°ì¹˜ ì²˜ë¦¬")
                return True
            else:
                print("\nâš ï¸ ë¶€ë¶„ ì²˜ë¦¬ë¨")
                print(f"   ğŸ’¡ {silver_stats.silver_total - fact_stats.fact_total:,}ê°œ ëˆ„ë½")
                return False
                
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return False
            
    def calculate_realistic_time(self, method="date_by_date"):
        """ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°"""
        print(f"\nâ±ï¸ {method} ë°©ì‹ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°...")
        
        if method == "date_by_date":
            # 10ì¼ Ã— í‰ê·  32,000ê°œ Ã· 5,000 = 64ë°°ì¹˜
            estimated_batches = 64
            batch_time = 50  # ì´ˆ (ì‹¤ì¸¡ ê¸°ë°˜)
        else:
            # 320,000ê°œ Ã· 5,000 = 64ë°°ì¹˜
            estimated_batches = 64
            batch_time = 45  # ì´ˆ (ì‹œê°„ìˆœ ì •ë ¬ì´ ì•½ê°„ ë¹ ë¦„)
            
        total_seconds = estimated_batches * batch_time
        total_hours = total_seconds / 3600
        
        print(f"   ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {estimated_batches}ê°œ")
        print(f"   ë°°ì¹˜ë‹¹ ì‹œê°„: {batch_time}ì´ˆ")
        print(f"   ì´ ì˜ˆìƒ ì‹œê°„: {total_hours:.1f}ì‹œê°„")
        
        return total_hours
        
    def execute_improved_processing(self, method="date_by_date"):
        """ê°œì„ ëœ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        print("ğŸš€ ê°œì„ ëœ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰...")
        print("=" * 60)
        
        try:
            # SparkSession ìƒì„±
            self.create_spark_session()
            
            # ì‹œê°„ ê³„ì‚°
            estimated_time = self.calculate_realistic_time(method)
            
            if method == "date_by_date":
                print(f"\nğŸ—“ï¸ ë‚ ì§œë³„ ì™„ì „ ì²˜ë¦¬ ì‹¤í–‰...")
                processed = self.method1_date_by_date_complete()
            else:
                print(f"\nâ° ì‹œê°„ìˆœ ì™„ì „ ì²˜ë¦¬ ì‹¤í–‰...")
                processed = self.method2_chronological_complete()
            
            # ê²°ê³¼ ê²€ì¦
            success = self.validate_complete_processing()
            
            if success:
                print(f"\nğŸ‰ ê°œì„ ëœ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
                print(f"   âœ… ì´ ì²˜ë¦¬: {processed:,}ê°œ")
                print(f"   âœ… ì˜ˆìƒ ì‹œê°„: {estimated_time:.1f}ì‹œê°„")
                print(f"   âœ… ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬")
            else:
                print(f"\nâš ï¸ ë¶€ë¶„ì  ì„±ê³µ")
                print(f"   ğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„°: {processed:,}ê°œ")
                
        except Exception as e:
            print(f"âŒ ê°œì„ ëœ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        finally:
            if self.spark:
                self.spark.stop()

if __name__ == "__main__":
    processor = ImprovedBatchProcessor()
    
    # ë°©ë²• 1: ë‚ ì§œë³„ ì™„ì „ ì²˜ë¦¬ (ê¶Œì¥)
    processor.execute_improved_processing(method="date_by_date")
    
    # ë°©ë²• 2: ì‹œê°„ìˆœ ì™„ì „ ì²˜ë¦¬ (ëŒ€ì•ˆ)
    # processor.execute_improved_processing(method="chronological")
