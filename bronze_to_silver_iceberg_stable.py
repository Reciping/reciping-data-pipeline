# bronze_to_silver_iceberg_stable.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, year, month, dayofmonth, hour, date_format, to_timestamp, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType, DateType, TimestampType

def main():
    """
    ì•ˆì •ì ì¸ Iceberg ë²„ì „ìœ¼ë¡œ Bronze, Silver ë ˆì´ì–´ë¥¼ êµ¬ì¶•í•˜ëŠ”
    ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ ETL íŒŒì´í”„ë¼ì¸ (Iceberg 1.4.3 + Hadoop Catalog).
    """
    try:
        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì • (ì„±ê³µí•œ ì„¤ì • ì ìš©)
        import os
        import subprocess
        
        # ì„±ê³µí•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['HADOOP_USER_NAME'] = 'root'
        os.environ['USER'] = 'root'
        os.environ['HOME'] = '/tmp'
        os.environ['JAVA_OPTS'] = '-Duser.name=root -XX:+UseG1GC -XX:G1HeapRegionSize=32m'
        # Ivy ì„¤ì •
        os.environ['IVY_HOME'] = '/tmp/.ivy2'
        os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.jars.ivy=/tmp/.ivy2 pyspark-shell'
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('/tmp/.ivy2', exist_ok=True)

        # -----------------------------------------------------------------------------
        # 1. ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„± (ì•ˆì •ì ì¸ Iceberg + Hadoop Catalog)
        # -----------------------------------------------------------------------------
        print("ğŸ”§ SparkSession with Stable Iceberg (Hadoop Catalog) ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        
        spark = SparkSession.builder \
            .appName("Bronze_to_Silver_Iceberg_Stable_Pipeline") \
            .master("local[2]") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.jars.ivy", "/tmp/.ivy2") \
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,org.apache.hadoop:hadoop-aws:3.3.4") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hadoop") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", "s3a://reciping-user-event-logs/warehouse/iceberg") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "")) \
            .config("spark.hadoop.fs.s3a.region", "ap-northeast-2") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.path.style.access", "false") \
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true") \
            .config("spark.hadoop.fs.s3a.fast.upload", "true") \
            .config("spark.hadoop.fs.s3a.block.size", "134217728") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.driver.maxResultSize", "1g") \
            .getOrCreate()

        spark.sparkContext.setLogLevel("WARN")
        print("âœ… SparkSession with Stable Icebergê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # -----------------------------------------------------------------------------
        # 2. ğŸ—ï¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ë° ê´€ë¦¬
        # -----------------------------------------------------------------------------
        print("\nğŸ—ï¸ Iceberg ë„¤ì„ìŠ¤í˜ì´ìŠ¤ êµ¬ì„±...")
        
        # Bronze, Silver, Gold ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
        try:
            spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.bronze")
            spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.silver") 
            spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.gold")
            print("âœ… Iceberg ë„¤ì„ìŠ¤í˜ì´ìŠ¤ êµ¬ì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}")
        
        # í˜„ì¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª©ë¡ í™•ì¸
        print("\nğŸ“‹ í˜„ì¬ Iceberg ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª©ë¡:")
        spark.sql("SHOW NAMESPACES IN iceberg_catalog").show()

        # -----------------------------------------------------------------------------
        # 3. ğŸ¥‰ Bronze Layer - Iceberg í…Œì´ë¸” êµ¬ì¶•
        # -----------------------------------------------------------------------------
        print("\nğŸ¥‰ Bronze Layer (Iceberg) êµ¬ì¶• ì‹œì‘...")
        
        # S3 ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°
        landing_zone_path = "s3a://reciping-user-event-logs/bronze/landing-zone/events/"
        print(f"ğŸ“‚ ëœë”© ì¡´ì—ì„œ ë°ì´í„° ì½ê¸°: {landing_zone_path}")
        
        try:
            df_raw = spark.read.json(landing_zone_path)
            row_count = df_raw.count()
            print(f"âœ… ëœë”© ì¡´ ë°ì´í„° ë¡œë“œ ì„±ê³µ! í–‰ ìˆ˜: {row_count:,}")
            
            # ì²˜ë¦¬ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            df_raw_with_metadata = df_raw.withColumn("ingestion_timestamp", current_timestamp())
            
            # Bronze Iceberg í…Œì´ë¸” ìƒì„± (ë‹¨ìˆœí™”)
            try:
                # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
                spark.sql("DROP TABLE IF EXISTS iceberg_catalog.bronze.raw_events")
                
                # ì‘ì€ ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ë¶€ë‹´ ê°ì†Œ)
                df_sample = df_raw_with_metadata.limit(10000)
                
                df_sample.writeTo("iceberg_catalog.bronze.raw_events") \
                    .tableProperty("format-version", "2") \
                    .tableProperty("write.target-file-size-bytes", "67108864") \
                    .create()
                
                sample_count = df_sample.count()
                print(f"âœ… Bronze Iceberg í…Œì´ë¸” 'iceberg_catalog.bronze.raw_events' ìƒì„± ì™„ë£Œ (ìƒ˜í”Œ {sample_count:,}í–‰)")
                
            except Exception as e:
                print(f"âŒ Bronze í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
                return
            
        except Exception as e:
            print(f"âŒ ëœë”© ì¡´ì—ì„œ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("ğŸ’¡ upload_to_landing_zone.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            spark.stop()
            return

        # -----------------------------------------------------------------------------
        # 4. ğŸ“Š Bronze í…Œì´ë¸” ê²€ì¦
        # -----------------------------------------------------------------------------
        print("\nğŸ“Š Bronze Iceberg í…Œì´ë¸” ê²€ì¦...")
        
        # Bronze í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì½ê¸° í…ŒìŠ¤íŠ¸
        try:
            df_bronze = spark.table("iceberg_catalog.bronze.raw_events")
            bronze_count = df_bronze.count()
            print(f"âœ… Bronze í…Œì´ë¸” ê²€ì¦ ì„±ê³µ! í–‰ ìˆ˜: {bronze_count:,}")
            
            # ìŠ¤í‚¤ë§ˆ í™•ì¸
            print("\nğŸ“‹ Bronze í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ:")
            df_bronze.printSchema()
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ì•ˆì „í•˜ê²Œ)
            print("\nğŸ“Š Bronze ìƒ˜í”Œ ë°ì´í„° (ìƒìœ„ 2í–‰):")
            df_bronze.select("event_id", "event_name", "user_id", "timestamp").show(2, truncate=True)
            
        except Exception as e:
            print(f"âŒ Bronze í…Œì´ë¸” ê²€ì¦ ì‹¤íŒ¨: {e}")
            return

        # -----------------------------------------------------------------------------
        # 5. ğŸ§ª Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (ì•ˆì „í•˜ê²Œ)
        # -----------------------------------------------------------------------------
        print("\nğŸ§ª Iceberg ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        try:
            # í…Œì´ë¸” ì •ë³´ ì¡°íšŒ
            print("\nğŸ—ƒï¸ Bronze í…Œì´ë¸” ì •ë³´:")
            spark.sql("DESCRIBE TABLE iceberg_catalog.bronze.raw_events").show(5, truncate=False)
            
        except Exception as e:
            print(f"âš ï¸ í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        try:
            # ìŠ¤ëƒ…ìƒ· ì´ë ¥ ì¡°íšŒ (ì•ˆì „í•˜ê²Œ)
            print("\nğŸ“¸ Bronze í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì´ë ¥:")
            snapshots = spark.sql("SELECT snapshot_id, committed_at FROM iceberg_catalog.bronze.raw_events.snapshots ORDER BY committed_at DESC LIMIT 2")
            snapshots.show(truncate=False)
            
        except Exception as e:
            print(f"âš ï¸ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # -----------------------------------------------------------------------------
        # 6. ê²€ì¦ ë° ìš”ì•½
        # -----------------------------------------------------------------------------
        print("\nğŸ“ˆ Stable Iceberg ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ìš”ì•½:")
        print(f"ğŸ¥‰ Bronze í–‰ ìˆ˜: {bronze_count:,}")
        print(f"ğŸ“Š ìƒì„±ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤: iceberg_catalog.bronze, iceberg_catalog.silver, iceberg_catalog.gold")
        print(f"ğŸ—ƒï¸ Bronze í…Œì´ë¸”: iceberg_catalog.bronze.raw_events (Iceberg v2)")
        print(f"ğŸ—ï¸ ì¹´íƒˆë¡œê·¸: Hadoop Catalog (ì•ˆì •ì  ë²„ì „)")
        print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ì†Œ: S3 (s3a://reciping-user-event-logs/warehouse/iceberg)")
        
        # ì§€ì›ë˜ëŠ” ê¸°ëŠ¥ë“¤
        print(f"\nğŸ’ ê²€ì¦ëœ Iceberg ê¸°ëŠ¥:")
        print(f"   âœ… í…Œì´ë¸” ìƒì„± ë° ê´€ë¦¬")
        print(f"   âœ… ìŠ¤ëƒ…ìƒ· ì´ë ¥ ì¶”ì ")
        print(f"   âœ… ë©”íƒ€ë°ì´í„° ì¡°íšŒ")
        print(f"   âœ… S3 ìŠ¤í† ë¦¬ì§€ í†µí•©")
        print(f"   ğŸ“ ë‹¤ìŒ ë‹¨ê³„: Silver Layer êµ¬ì¶•, Time Travel, Schema Evolution")

        # -----------------------------------------------------------------------------
        # 7. ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        # -----------------------------------------------------------------------------
        spark.stop()
        print("âœ… Stable Iceberg ETL íŒŒì´í”„ë¼ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # ìŠ¤íŒŒí¬ ì„¸ì…˜ì´ ìˆë‹¤ë©´ ì¢…ë£Œ
        try:
            spark.stop()
        except:
            pass

# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
if __name__ == "__main__":
    main()
