#!/usr/bin/env python3
"""
ğŸ§Š Silver to Gold ETL Processor (Stateless & Idempotent)
==========================================================
Silver Iceberg í…Œì´ë¸”ê³¼ Dimension í…Œì´ë¸”ë“¤ì„ ì¡°ì¸í•˜ì—¬ Gold Fact í…Œì´ë¸”ë¡œ ë³€í™˜/ì§‘ê³„í•©ë‹ˆë‹¤.
Airflowë¡œë¶€í„° ì‹¤í–‰ ì‹œê°„ì„ ë°›ì•„ ì ì§„ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import logging
import argparse
from datetime import datetime
import pytz
from dateutil.parser import isoparse
from typing import Optional

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, from_json, current_timestamp, lit,
    year, month, dayofmonth, hour, date_format, expr, when, size, split, coalesce
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType,
    DoubleType, BooleanType, TimestampType, DateType, LongType, ArrayType
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SilverToGoldProcessor:
    def __init__(self, test_mode: bool = True):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        self.hive_metastore_uri = "thrift://10.0.11.86:9083" # ìì‹ ì˜ Hive Metastore URI

        if test_mode:
            self.silver_database = "recipe_analytics_test"
            self.gold_database = "recipe_analytics_test"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/test_warehouse/"
            self.table_suffix = "_test"
        else:
            self.silver_database = "recipe_analytics"
            self.gold_database = "gold_analytics"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/warehouse/"
            self.table_suffix = ""
            
        # ë³€ìˆ˜ì—ëŠ” ì¹´íƒˆë¡œê·¸/DBì´ë¦„ ì—†ì´ ìˆœìˆ˜ í…Œì´ë¸” ì´ë¦„ë§Œ ì €ì¥
        self.silver_table_name = f"user_events_silver{self.table_suffix}"
        self.gold_table_name = f"fact_user_events{self.table_suffix}"

    def create_spark_session(self):
        """SparkSession ìƒì„± ë° ì¹´íƒˆë¡œê·¸/DB ì„¤ì •"""
        print("SparkSession ìƒì„± ì¤‘...")
        self.spark = SparkSession.builder \
            .appName("SilverToGold_ETL") \
            .config("spark.local.dir", "/home/ec2-user/spark-tmp") \
            .config("spark.sql.session.timeZone", "Asia/Seoul") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", self.hive_metastore_uri) \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", self.s3_warehouse_path) \
            .getOrCreate()
        self.spark.sparkContext.setLogLevel("WARN")

        # ì„¸ì…˜ì˜ ê¸°ë³¸ ì¹´íƒˆë¡œê·¸ì™€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ Gold ìš©ìœ¼ë¡œ ì„¤ì •
        print(f"í˜„ì¬ ì¹´íƒˆë¡œê·¸ë¥¼ '{self.catalog_name}'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"USE {self.catalog_name}")
        
        print(f"ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ '{self.gold_database}' ìƒì„± ë° ì‚¬ìš© ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.gold_database}")
        self.spark.sql(f"USE {self.gold_database}")
        
        print("SparkSession ìƒì„± ë° ì„¤ì • ì™„ë£Œ")

    def create_gold_table_if_not_exists(self):
        """Gold Fact í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"Gold Fact í…Œì´ë¸” ìƒì„± í™•ì¸: {self.gold_table_name}")
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.gold_table_name} (
            event_id STRING, user_dim_key BIGINT, time_dim_key BIGINT, recipe_dim_key BIGINT,
            page_dim_key BIGINT, event_dim_key BIGINT, event_count INT,
            session_duration_seconds BIGINT, page_view_duration_seconds BIGINT,
            is_conversion BOOLEAN, conversion_value DOUBLE, engagement_score DOUBLE,
            session_id STRING, anonymous_id STRING, created_at TIMESTAMP, updated_at TIMESTAMP
        )
        USING ICEBERG
        PARTITIONED BY (days(created_at))
        """
        self.spark.sql(create_table_sql)
        print("Gold Fact í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ")

    
    # def transform_and_load_gold_data(self, target_date: Optional[str] = None):
    #     """
    #     ê°œì„ ëœ ë²„ì „: JOIN ì¡°ê±´ì„ ë” ìœ ì—°í•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ ë°ì´í„° ì†ì‹¤ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.
    #     """
    #     if target_date:
    #         print(f"Silver to Gold ì¦ë¶„ ì²˜ë¦¬ ì‹œì‘ (ëŒ€ìƒ ë‚ ì§œ: {target_date})")
    #     else:
    #         print("Silver to Gold ë²Œí¬ ì²˜ë¦¬ ì‹œì‘ (ì „ì²´ ë°ì´í„°)")

    #     full_silver_table = f"{self.silver_database}.{self.silver_table_name}"
    #     dim_user_table = f"dim_user{self.table_suffix}"
    #     dim_recipe_table = f"dim_recipe{self.table_suffix}"
    #     dim_event_table = f"dim_event{self.table_suffix}"
    #     dim_page_table = f"dim_page{self.table_suffix}"
        
    #     try:
    #         # Silver ë°ì´í„° ì½ê¸°
    #         silver_df_reader = self.spark.read.table(full_silver_table)
    #         if target_date:
    #             silver_df = silver_df_reader.where(f"date = '{target_date}'")
    #         else:
    #             silver_df = silver_df_reader

    #         silver_count = silver_df.count()
    #         if silver_count == 0:
    #             print(f"ì²˜ë¦¬í•  Silver ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    #             return

    #         print(f"Silver ë°ì´í„° {silver_count:,}ê±´ì„ Gold í…Œì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")

    #         # Dimension í…Œì´ë¸”ë“¤ ì½ê¸°
    #         dim_user = self.spark.read.table(dim_user_table)
    #         dim_recipe = self.spark.read.table(dim_recipe_table)
    #         dim_event = self.spark.read.table(dim_event_table)
    #         dim_page = self.spark.read.table(dim_page_table)

    #         # ===== í•µì‹¬ ìˆ˜ì • ë¶€ë¶„: ë” ìœ ì—°í•œ JOIN ì¡°ê±´ =====
            
    #         # 1. user_idì™€ anonymous_idë§Œìœ¼ë¡œ JOIN (í•„ìˆ˜ í‚¤ë§Œ ì‚¬ìš©)
    #         joined_df = silver_df.alias("s") \
    #             .join(
    #                 dim_user.alias("du"), 
    #                 (col("s.user_id") == col("du.user_id")) & 
    #                 (col("s.anonymous_id") == col("du.anonymous_id")),
    #                 "left"
    #             ) \
    #             .join(
    #                 dim_recipe.alias("dr"), 
    #                 col("s.prop_recipe_id") == col("dr.recipe_id"), 
    #                 "left"
    #             ) \
    #             .join(
    #                 dim_event.alias("de"), 
    #                 col("s.event_name") == col("de.event_name"), 
    #                 "left"
    #             ) \
    #             .join(
    #                 dim_page.alias("dp"), 
    #                 (col("s.page_name") == col("dp.page_name")) & 
    #                 (col("s.page_url") == col("dp.page_url")), 
    #                 "left"
    #             )

    #         # 2. ìµœì¢… Fact í…Œì´ë¸” ìƒì„± (ì»¬ëŸ¼ëª… ëª…ì‹œì  ì§€ì •)
    #         fact_df = joined_df.select(
    #             col("s.event_id"),
    #             coalesce(col("du.user_sk"), lit(0)).alias("user_dim_key"),
    #             date_format(col("s.kst_timestamp"), "yyyyMMddHH").cast("bigint").alias("time_dim_key"),
    #             coalesce(col("dr.recipe_sk"), lit(0)).alias("recipe_dim_key"),
    #             coalesce(col("dp.page_sk"), lit(0)).alias("page_dim_key"),
    #             coalesce(col("de.event_sk"), lit(0)).alias("event_dim_key"),
    #             lit(1).alias("event_count"),
    #             when(col("s.prop_action").isNotNull() & (size(split(col("s.prop_action"), ":")) >= 2), 
    #                  coalesce(split(col("s.prop_action"), ":")[1].cast("bigint"), lit(60)))
    #             .otherwise(60).alias("session_duration_seconds"),
    #             lit(30).cast("bigint").alias("page_view_duration_seconds"),
    #             when(col("s.event_name").isin('auth_success', 'click_bookmark', 'create_comment'), True)
    #             .otherwise(False).alias("is_conversion"),
    #             lit(1.0).alias("conversion_value"),
    #             when(col("s.event_name") == 'auth_success', 10.0)
    #             .when(col("s.event_name") == 'create_comment', 9.0)
    #             .when(col("s.event_name") == 'click_bookmark', 8.0)
    #             .when(col("s.event_name") == 'click_recipe', 7.0)
    #             .when(col("s.event_name") == 'search_recipe', 5.0)
    #             .when(col("s.event_name") == 'view_recipe', 4.0)
    #             .when(col("s.event_name") == 'view_page', 2.0)
    #             .otherwise(1.0).alias("engagement_score"),
    #             col("s.session_id"),
    #             col("s.anonymous_id"),
    #             col("s.kst_timestamp").alias("created_at"),
    #             col("s.kst_timestamp").alias("updated_at")
    #         )

    #         # ë°ì´í„° ì ì¬
    #         if target_date:
    #             print("Gold í…Œì´ë¸”ì— ì¦ë¶„ ë°ì´í„° ì¶”ê°€(Append)...")
    #             fact_df.writeTo(self.gold_table_name).append()
    #         else:
    #             print("Gold í…Œì´ë¸” ì „ì²´ ë°ì´í„° ë®ì–´ì“°ê¸°(Overwrite)...")
    #             fact_df.write.format("iceberg").mode("overwrite").saveAsTable(self.gold_table_name)

    #         print("Gold í…Œì´ë¸” ì ì¬ ì™„ë£Œ.")

    #     except Exception as e:
    #         logger.error("Gold ë°ì´í„° ë³€í™˜/ì ì¬ ì‹¤íŒ¨", exc_info=True)
    #         raise


    def update_dim_user_if_needed(self, silver_df):
        """
        Silverì˜ ì‹ ê·œ ì‚¬ìš©ìë¥¼ dim_userì— ìë™ ì¶”ê°€
        
        Args:
            silver_df: Silver DataFrame
        """
        try:
            logger.info("ì‹ ê·œ ì‚¬ìš©ì í™•ì¸ ì¤‘...")
            
            dim_user_table = f"dim_user{self.table_suffix}"
            
            # ê¸°ì¡´ dim_user ì½ê¸°
            dim_user = self.spark.read.table(dim_user_table)
            existing_user_ids = dim_user.select("user_id", "anonymous_id").where(col("user_id").isNotNull())
            
            # Silverì˜ ê³ ìœ  ì‚¬ìš©ì ì¶”ì¶œ
            silver_users = silver_df.select(
                "user_id", 
                "anonymous_id", 
                "user_segment", 
                "cooking_style", 
                "ab_test_group"
            ).where(col("user_id").isNotNull()).distinct()
            
            # ì‹ ê·œ ì‚¬ìš©ì í•„í„°ë§ (user_id + anonymous_id ì¡°í•©ìœ¼ë¡œ í™•ì¸)
            new_users = silver_users.alias("su").join(
                existing_user_ids.alias("eu"),
                (col("su.user_id") == col("eu.user_id")) & 
                (col("su.anonymous_id") == col("eu.anonymous_id")),
                "left_anti"
            )
            
            new_count = new_users.count()
            
            if new_count > 0:
                logger.info(f"ì‹ ê·œ ì‚¬ìš©ì {new_count}ëª… ë°œê²¬ - dim_user ì—…ë°ì´íŠ¸ ì¤‘...")
                
                # í˜„ì¬ ìµœëŒ€ user_sk ì¡°íšŒ
                max_sk_result = self.spark.sql(f"""
                    SELECT COALESCE(MAX(user_sk), 0) as max_sk 
                    FROM {dim_user_table}
                """).collect()
                current_max_sk = max_sk_result[0]['max_sk']
                
                logger.info(f"í˜„ì¬ ìµœëŒ€ user_sk: {current_max_sk}")
                
                # user_sk ìƒì„±
                from pyspark.sql.window import Window
                from pyspark.sql.functions import row_number
                
                window_spec = Window.orderBy("user_id")
                new_users_with_sk = new_users.withColumn(
                    "user_sk", 
                    row_number().over(window_spec) + lit(current_max_sk)
                ).select(
                    "user_sk", 
                    "user_id", 
                    "anonymous_id",
                    "user_segment", 
                    "cooking_style", 
                    "ab_test_group"
                )
                
                # dim_userì— APPEND
                new_users_with_sk.writeTo(dim_user_table).append()
                
                logger.info(f"âœ… dim_userì— {new_count}ëª… ì¶”ê°€ ì™„ë£Œ")
                
                # ê²€ì¦
                new_max_sk = self.spark.sql(f"""
                    SELECT MAX(user_sk) as max_sk 
                    FROM {dim_user_table}
                """).collect()[0]['max_sk']
                
                logger.info(f"ì—…ë°ì´íŠ¸ í›„ ìµœëŒ€ user_sk: {new_max_sk}")
            else:
                logger.info("ì‹ ê·œ ì‚¬ìš©ì ì—†ìŒ - dim_user ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
                
        except Exception as e:
            logger.warning(f"dim_user ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}", exc_info=True)


    def transform_and_load_gold_data(self, target_date: Optional[str] = None):
        """ê°œì„ ëœ Silver to Gold ë³€í™˜ - ê²¬ê³ í•œ JOIN ì²˜ë¦¬"""
        if target_date:
            print(f"Silver to Gold ì¦ë¶„ ì²˜ë¦¬ ì‹œì‘ (ëŒ€ìƒ ë‚ ì§œ: {target_date})")
        else:
            print("Silver to Gold ë²Œí¬ ì²˜ë¦¬ ì‹œì‘ (ì „ì²´ ë°ì´í„°)")

        full_silver_table = f"{self.silver_database}.{self.silver_table_name}"
        dim_user_table = f"dim_user{self.table_suffix}"
        dim_recipe_table = f"dim_recipe{self.table_suffix}"
        dim_event_table = f"dim_event{self.table_suffix}"
        dim_page_table = f"dim_page{self.table_suffix}"
        
        try:
            # Silver ë°ì´í„° ì½ê¸°
            silver_df_reader = self.spark.read.table(full_silver_table)
            if target_date:
                silver_df = silver_df_reader.where(f"date = '{target_date}'")
            else:
                silver_df = silver_df_reader

            silver_count = silver_df.count()
            if silver_count == 0:
                print(f"ì²˜ë¦¬í•  Silver ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            print(f"Silver ë°ì´í„° {silver_count:,}ê±´ì„ Gold í…Œì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")

            # âœ… ì‹ ê·œ ì‚¬ìš©ì ìë™ ì—…ë°ì´íŠ¸ 
            self.update_dim_user_if_needed(silver_df)

            # Dimension í…Œì´ë¸”ë“¤ ì½ê¸° (dim_user ë‹¤ì‹œ ì½ê¸° - ë°©ê¸ˆ ì—…ë°ì´íŠ¸ ë°˜ì˜)
            dim_user = self.spark.read.table(dim_user_table)
            dim_recipe = self.spark.read.table(dim_recipe_table)
            dim_event = self.spark.read.table(dim_event_table)
            dim_page = self.spark.read.table(dim_page_table)

            # === ê°œì„ ëœ JOIN ë¡œì§ ===
            
            # 1. User JOIN (user_id + anonymous_id ê¸°ì¤€)
            joined_df = silver_df.alias("s") \
                .join(
                    dim_user.alias("du"), 
                    (col("s.user_id") == col("du.user_id")) & 
                    (col("s.anonymous_id") == col("du.anonymous_id")),
                    "left"
                )
            
            # 2. Recipe JOIN (ë°ì´í„° íƒ€ì… ì•ˆì „ì„± ë³´ì¥)
            joined_df = joined_df.join(
                dim_recipe.alias("dr"), 
                col("s.prop_recipe_id").cast("string") == col("dr.recipe_id").cast("string"), 
                "left"
            )
            
            # 3. Event JOIN (NULL ì•ˆì „ ì²˜ë¦¬)
            joined_df = joined_df.join(
                dim_event.alias("de"), 
                (col("s.event_name") == col("de.event_name")) & 
                col("s.event_name").isNotNull(), 
                "left"
            )
            
            # 4. Page JOIN (NULL ê°’ ì²˜ë¦¬ ê°œì„ )
            joined_df = joined_df.join(
                dim_page.alias("dp"), 
                (coalesce(col("s.page_name"), lit("")) == coalesce(col("dp.page_name"), lit(""))) & 
                (coalesce(col("s.page_url"), lit("")) == coalesce(col("dp.page_url"), lit(""))) &
                (col("s.page_name").isNotNull() | col("s.page_url").isNotNull()), 
                "left"
            )

            # 5. ìµœì¢… Fact í…Œì´ë¸” ìƒì„±
            fact_df = joined_df.select(
                col("s.event_id"),
                coalesce(col("du.user_sk"), lit(0)).alias("user_dim_key"),
                date_format(col("s.kst_timestamp"), "yyyyMMddHH").cast("bigint").alias("time_dim_key"),
                coalesce(col("dr.recipe_sk"), lit(0)).alias("recipe_dim_key"),
                coalesce(col("dp.page_sk"), lit(0)).alias("page_dim_key"),
                coalesce(col("de.event_sk"), lit(0)).alias("event_dim_key"),
                lit(1).alias("event_count"),
                when(col("s.prop_action").isNotNull() & (size(split(col("s.prop_action"), ":")) >= 2), 
                    coalesce(split(col("s.prop_action"), ":")[1].cast("bigint"), lit(60)))
                .otherwise(60).alias("session_duration_seconds"),
                lit(30).cast("bigint").alias("page_view_duration_seconds"),
                when(col("s.event_name").isin('auth_success', 'click_bookmark', 'create_comment'), True)
                .otherwise(False).alias("is_conversion"),
                lit(1.0).alias("conversion_value"),
                when(col("s.event_name") == 'auth_success', 10.0)
                .when(col("s.event_name") == 'create_comment', 9.0)
                .when(col("s.event_name") == 'click_bookmark', 8.0)
                .when(col("s.event_name") == 'click_recipe', 7.0)
                .when(col("s.event_name") == 'search_recipe', 5.0)
                .when(col("s.event_name") == 'view_recipe', 4.0)
                .when(col("s.event_name") == 'view_page', 2.0)
                .otherwise(1.0).alias("engagement_score"),
                col("s.session_id"),
                col("s.anonymous_id"),
                col("s.kst_timestamp").alias("created_at"),
                col("s.kst_timestamp").alias("updated_at")
            )

            # 6. JOIN ì„±ê³µë¥  ë¡œê¹…
            total_count = fact_df.count()
            user_join_success = fact_df.where(col("user_dim_key") != 0).count()
            recipe_join_success = fact_df.where(col("recipe_dim_key") != 0).count()
            page_join_success = fact_df.where(col("page_dim_key") != 0).count()
            event_join_success = fact_df.where(col("event_dim_key") != 0).count()
            
            print(f"JOIN ì„±ê³µë¥ :")
            print(f"  User: {user_join_success:,}/{total_count:,} ({user_join_success/total_count*100:.1f}%)")
            print(f"  Recipe: {recipe_join_success:,}/{total_count:,} ({recipe_join_success/total_count*100:.1f}%)")
            print(f"  Page: {page_join_success:,}/{total_count:,} ({page_join_success/total_count*100:.1f}%)")
            print(f"  Event: {event_join_success:,}/{total_count:,} ({event_join_success/total_count*100:.1f}%)")

            # 7. ë°ì´í„° ì ì¬
            if target_date:
                print("Gold í…Œì´ë¸”ì— ì¦ë¶„ ë°ì´í„° ì¶”ê°€(Append)...")
                fact_df.writeTo(self.gold_table_name).append()
            else:
                print("Gold í…Œì´ë¸” ì „ì²´ ë°ì´í„° ë®ì–´ì“°ê¸°(Overwrite)...")
                fact_df.write.format("iceberg").mode("overwrite").saveAsTable(self.gold_table_name)

            print("Gold í…Œì´ë¸” ì ì¬ ì™„ë£Œ.")

        except Exception as e:
            logger.error("Gold ë°ì´í„° ë³€í™˜/ì ì¬ ì‹¤íŒ¨", exc_info=True)
            raise

#     def run_pipeline(self, target_date: Optional[str] = None):
#         """
#         ë©”ì¸ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
#         target_dateê°€ ìˆìœ¼ë©´ ì¦ë¶„ ëª¨ë“œ, ì—†ìœ¼ë©´ ë²Œí¬ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.
#         """
#         try:
#             self.create_spark_session()
#             self.create_gold_table_if_not_exists()
            
#             # Silver í…Œì´ë¸”ì˜ ìµœì‹  ë©”íƒ€ë°ì´í„° ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
#             # Airflow ë“± ì™¸ë¶€ì—ì„œ Silver í…Œì´ë¸”ì´ ì—…ë°ì´íŠ¸ëœ ì§í›„ ì´ ì¡ì„ ì‹¤í–‰í•  ë•Œ í•„ìš”í•©ë‹ˆë‹¤.
#             full_silver_table_name = f"{self.silver_database}.{self.silver_table_name}"
#             print(f"Silver í…Œì´ë¸”ì˜ ìµœì‹  ì •ë³´ ìƒˆë¡œê³ ì¹¨: {full_silver_table_name}")
#             self.spark.catalog.refreshTable(full_silver_table_name)
            
#             # target_date ì¸ìë¥¼ transform í•¨ìˆ˜ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
#             self.transform_and_load_gold_data(target_date)
            
#             print("Silver to Gold ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
            
#         except Exception as e:
#             logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨", exc_info=True)
#             raise
#         finally:
#             if self.spark:
#                 self.spark.stop()

# def main():
#     parser = argparse.ArgumentParser(description="Silver to Gold Iceberg ETL Job")
#     parser.add_argument("--target-date", required=False, help="Target date for incremental processing (YYYY-MM-DD)")
#     parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True)
#     args = parser.parse_args()

#     processor = SilverToGoldProcessor(test_mode=args.test_mode)
#     processor.run_pipeline(target_date=args.target_date)

    def run_pipeline(self, data_interval_start: Optional[str] = None, data_interval_end: Optional[str] = None):
        """
        [ìˆ˜ì •ë¨] data_interval_startë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•  Silver ë°ì´í„°ì˜ ë‚ ì§œë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        """
        try:
            self.create_spark_session()
            self.create_gold_table_if_not_exists()
            
            if not data_interval_start:
                raise ValueError("ì¦ë¶„ ì²˜ë¦¬ë¥¼ ìœ„í•´ --data-interval-start ì¸ìê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.")

            full_silver_table_name = f"{self.silver_database}.{self.silver_table_name}"
            self.spark.catalog.refreshTable(full_silver_table_name)

            # === ì¦ë¶„ ëª¨ë“œ ===
            print(f"ì¦ë¶„ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰: {data_interval_start} ~ {data_interval_end}")
            
            # 1. data_interval_start(UTC)ë¥¼ KST ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ 'YYYY-MM-DD' ë‚ ì§œ íšë“
            start_time_utc = isoparse(data_interval_start)
            kst_tz = pytz.timezone('Asia/Seoul')
            start_time_kst = start_time_utc.astimezone(kst_tz)
            target_date_str = start_time_kst.strftime('%Y-%m-%d')
            
            print(f"Silver í…Œì´ë¸”ì˜ ì²˜ë¦¬ ëŒ€ìƒ íŒŒí‹°ì…˜ ë‚ ì§œ: {target_date_str}")
            
            # 2. ê¸°ì¡´ ë³€í™˜ í•¨ìˆ˜ì— target_date ì¸ìë¥¼ ì „ë‹¬í•˜ì—¬ ì‹¤í–‰
            self.transform_and_load_gold_data(target_date=target_date_str)
            
            print("Silver to Gold ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
                
        except Exception as e:
            logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨", exc_info=True)
            raise
        finally:
            if self.spark:
                self.spark.stop()

def main():
    parser = argparse.ArgumentParser(description="Stateless Silver to Gold ETL Job")
    parser.add_argument("--data-interval-start", required=True)
    parser.add_argument("--data-interval-end", required=True)
    parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True)
    args = parser.parse_args()

    processor = SilverToGoldProcessor(test_mode=args.test_mode)
    processor.run_pipeline(
        data_interval_start=args.data_interval_start,
        data_interval_end=args.data_interval_end
    )

if __name__ == "__main__":
    main()