"""
ğŸ§Š Dimension Tables Creation Script
====================================
Silver í…Œì´ë¸” ë° S3 ë§ˆìŠ¤í„° íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ Gold Layerì˜ ëª¨ë“  Dimension í…Œì´ë¸”ì„ ìƒì„±/ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""
import logging
import argparse
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, monotonically_increasing_id, to_date, year, month, dayofmonth, hour, 
    date_format, lit, when, expr, row_number, desc
)
from pyspark.sql.window import Window

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DimensionBuilder:
    def __init__(self, test_mode: bool = True):
        self.spark = None
        self.catalog_name = "iceberg_catalog"
        self.s3_master_path = "s3a://reciping-user-event-logs/meta-data/"

        if test_mode:
            self.source_database = "recipe_analytics_test"
            self.target_database = "recipe_analytics_test"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/test_warehouse/"
            self.table_suffix = "_test"
        else:
            self.source_database = "recipe_analytics"
            self.target_database = "gold_analytics"
            self.s3_warehouse_path = "s3a://reciping-user-event-logs/iceberg/warehouse/"
            self.table_suffix = ""
            
        self.silver_table_name = f"{self.catalog_name}.{self.source_database}.user_events_silver{self.table_suffix}"
        
    def create_spark_session(self):
        """SparkSession ìƒì„± ë° ì¹´íƒˆë¡œê·¸/DB ì„¤ì •"""
        print("SparkSession ìƒì„± ì¤‘...")
        self.spark = SparkSession.builder \
            .appName("DimensionBuilder") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.iceberg_catalog.type", "hive") \
            .config("spark.sql.catalog.iceberg_catalog.uri", "thrift://10.0.11.86:9083") \
            .config("spark.sql.catalog.iceberg_catalog.warehouse", self.s3_warehouse_path) \
            .getOrCreate()
        self.spark.sparkContext.setLogLevel("WARN")

        print(f"í˜„ì¬ ì¹´íƒˆë¡œê·¸ë¥¼ '{self.catalog_name}'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"USE {self.catalog_name}")
        
        print(f"ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ '{self.target_database}' ìƒì„± ë° ì‚¬ìš© ì„¤ì •í•©ë‹ˆë‹¤.")
        self.spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.target_database}")
        self.spark.sql(f"USE {self.target_database}")
        
        print("SparkSession ìƒì„± ë° ì„¤ì • ì™„ë£Œ")

    def _create_user_dimension(self, silver_df: DataFrame, table_name: str):
        """
        User Dimension ì „ìš© ìƒì„± í•¨ìˆ˜
        user_idë³„ë¡œ í•˜ë‚˜ì˜ surrogate keyë§Œ í• ë‹¹í•˜ì—¬ DAU ê³„ì‚° ì •í™•ì„± ë³´ì¥
        """
        print(f"User Dimension í…Œì´ë¸” ìƒì„±/ì—…ë°ì´íŠ¸ ì¤‘: {table_name}")
        
        # 1. user_idë³„ë¡œ ê°€ì¥ ìµœê·¼ ì •ë³´ë§Œ ìœ ì§€ (SCD Type 1 ë°©ì‹)
        window = Window.partitionBy("user_id").orderBy(desc("processed_at"))
        
        user_dim = silver_df.select(
            "user_id", "anonymous_id", "user_segment", "cooking_style", "ab_test_group", "processed_at"
        ).where(col("user_id").isNotNull()) \
        .withColumn("rn", row_number().over(window)) \
        .where(col("rn") == 1) \
        .drop("rn", "processed_at") \
        .distinct()  # í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ì œê±°
        
        # 2. Surrogate Key í• ë‹¹
        user_dim_with_sk = user_dim.withColumn("user_sk", monotonically_increasing_id())
        
        # 3. ìµœì¢… ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        final_user_dim = user_dim_with_sk.select(
            "user_sk", "user_id", "anonymous_id", "user_segment", "cooking_style", "ab_test_group"
        )
        
        # 4. í…Œì´ë¸” ì €ì¥
        final_user_dim.write.format("iceberg").mode("overwrite").saveAsTable(table_name)
        
        user_count = final_user_dim.count()
        print(f"{table_name} ì²˜ë¦¬ ì™„ë£Œ. ì´ {user_count:,} ê±´")
        
        # 5. ê²€ì¦ ë¡œê·¸
        unique_user_ids = final_user_dim.select("user_id").distinct().count()
        print(f"ê²€ì¦: unique user_id = {unique_user_ids:,}, total records = {user_count:,}")
        if unique_user_ids != user_count:
            print("âš ï¸  ê²½ê³ : user_id ì¤‘ë³µì´ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤!")
        else:
            print("âœ… ê²€ì¦ ì„±ê³µ: user_idë‹¹ í•˜ë‚˜ì˜ ë ˆì½”ë“œë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.")

    def _create_dimension(self, table_name: str, source_df: DataFrame, id_cols: list, surrogate_key: str):
        """ì¼ë°˜ Dimension ìƒì„± í—¬í¼ í•¨ìˆ˜ (user ì œì™¸)"""
        print(f"Dimension í…Œì´ë¸” ìƒì„±/ì—…ë°ì´íŠ¸ ì¤‘: {table_name}")
        dim_df = source_df.select(*id_cols).where(col(id_cols[0]).isNotNull()).distinct()
        dim_df_with_sk = dim_df.withColumn(surrogate_key, monotonically_increasing_id())
        dim_df_with_sk.write.format("iceberg").mode("overwrite").saveAsTable(table_name)
        print(f"{table_name} ì²˜ë¦¬ ì™„ë£Œ. ì´ {dim_df_with_sk.count():,} ê±´")

    def _create_dim_recipe_from_master(self):
        """S3 ë§ˆìŠ¤í„° íŒŒì¼ì—ì„œ ë ˆì‹œí”¼ Dimension ìƒì„±"""
        table_name = f"dim_recipe{self.table_suffix}"
        master_file_path = f"{self.s3_master_path}total_recipes.parquet"
        
        print(f"Dimension í…Œì´ë¸” ìƒì„±/ì—…ë°ì´íŠ¸ ì¤‘: {table_name} (Source: {master_file_path})")

        recipe_master_df = self.spark.read.parquet(master_file_path)

        dim_recipe_df = recipe_master_df.select(
            col("id").alias("recipe_id"),
            col("name").alias("recipe_name"),
            col("dish_type"),
            col("ingredient_type"),
            col("method_type"),
            col("situation_type"),
            col("difficulty"),
            col("cooking_time")
        )

        dim_recipe_with_sk = dim_recipe_df.withColumn("recipe_sk", monotonically_increasing_id())

        final_dim_df = dim_recipe_with_sk.select(
            "recipe_sk", "recipe_id", "recipe_name", "dish_type", 
            "ingredient_type", "method_type", "situation_type", "difficulty", "cooking_time"
        )
        
        final_dim_df.write.format("iceberg").mode("overwrite").saveAsTable(table_name)
        print(f"{table_name} ì²˜ë¦¬ ì™„ë£Œ. ì´ {final_dim_df.count():,} ê±´")

    def run(self):
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            self.create_spark_session()
            
            print(f"Silver í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì½ê¸°: {self.silver_table_name}")
            silver_df = self.spark.read.table(self.silver_table_name)

            print(f"Silver í…Œì´ë¸”ì˜ ìµœì‹  ì •ë³´ ìƒˆë¡œê³ ì¹¨: {self.silver_table_name}")
            self.spark.catalog.refreshTable(self.silver_table_name)
            silver_df.cache()

            # === í•µì‹¬ ìˆ˜ì •: User Dimensionì„ ì „ìš© í•¨ìˆ˜ë¡œ ìƒì„± ===
            self._create_user_dimension(silver_df, f"dim_user{self.table_suffix}")
            
            # ë‚˜ë¨¸ì§€ Dimensionë“¤ì€ ê¸°ì¡´ ë°©ì‹ ìœ ì§€
            self._create_dimension(f"dim_event{self.table_suffix}", silver_df, ["event_name"], "event_sk")
            self._create_dimension(f"dim_page{self.table_suffix}", silver_df, ["page_name", "page_url"], "page_sk")

            # S3 ë§ˆìŠ¤í„° íŒŒì¼ ê¸°ë°˜ dim_recipe ìƒì„±
            self._create_dim_recipe_from_master()
            
            # dim_time í…Œì´ë¸” ìƒì„±
            print(f"Dimension í…Œì´ë¸” ìƒì„±/ì—…ë°ì´íŠ¸ ì¤‘: dim_time{self.table_suffix}")
            time_df = self.spark.sql("""
                SELECT explode(sequence(to_timestamp('2025-01-01 00:00:00'), 
                                       to_timestamp('2026-12-31 23:00:00'), 
                                       interval 1 hour)) as ts
            """)
            
            dim_time = time_df.select(
                date_format(col("ts"), "yyyyMMddHH").cast("bigint").alias("time_dim_key"),
                col("ts").alias("datetime_kst"),
                to_date(col("ts")).alias("date"),
                year(col("ts")).alias("year"),
                month(col("ts")).alias("month"),
                dayofmonth(col("ts")).alias("day"),
                hour(col("ts")).alias("hour"),
                date_format(col("ts"), "E").alias("day_of_week"),
                when(date_format(col("ts"), "E").isin("Sat", "Sun"), True).otherwise(False).alias("is_weekend")
            )

            dim_time.write.format("iceberg").mode("overwrite").saveAsTable(f"dim_time{self.table_suffix}")
            print(f"dim_time{self.table_suffix} ì²˜ë¦¬ ì™„ë£Œ.")

            silver_df.unpersist()

        except Exception as e:
            logger.error("Dimension í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨", exc_info=True)
            raise
        finally:
            if self.spark:
                self.spark.stop()

def main():
    parser = argparse.ArgumentParser(description="Create All Dimension Tables for Gold Layer")
    parser.add_argument("--test-mode", type=lambda x: (str(x).lower() == 'true'), default=True)
    args = parser.parse_args()
    builder = DimensionBuilder(test_mode=args.test_mode)
    builder.run()

if __name__ == "__main__":
    main()